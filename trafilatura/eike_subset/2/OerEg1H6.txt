Globale Erwärmung um 1,5°C – ein Sonderbericht des IPCC zu den Auswirkungen einer globalen Erwärmung um 1,5°C über das vorindustrielle Niveau und damit zusammenhängende Treibhausgas-Emissionen im Zusammenhang mit der Intensivierung der globalen Bemühungen gegen die Bedrohung des Klimawandels, nachhaltige Entwicklung und Bemühungen zur Bekämpfung der Armut.
Das hat man nun von Konsens-Wissenschaft – groupthink ohne Qualitätskontrolle.
HadCRUT4 ist der primäre, vom IPCC herangezogene globale Temperatur-Datensatz für dessen dramatische Behauptungen über eine „vom Menschen verursachte globale Erwärmung“. Es ist auch der Datensatz im Mittelpunkt von „Klimagate 2009“, gemanagt von der Climate Research Unit (CRU) an der East Anglia University.
Die Überprüfung ergab über 70 bedenkliche Bereiche – bedenklich hinsichtlich der Qualität und der Präzision der Daten.
Einer Analyse des australischen Forschers John McLean zufolge ist der Datensatz viel zu schlampig, um selbst von Klimawissenschaftlern ernst genommen werden kann, geschweige denn von einer so einflussreichen Institution wie dem IPCC oder von den Regierungen der Welt.
…
Wesentliche Punkte:
● Die Hadley-Daten sind unter den von den wichtigsten Datensätzen am meisten angeführten Daten zur Klima-Modellierung und damit für politische Maßnahmen, bei denen Milliarden von Dollars involviert sind.
● Mclean fand sonderbar unglaubwürdige Daten sowie systematische Adjustierungs-Fehler, große Lücken ohne irgendwelche Daten, Fehler bei der Zuordnung von Standorten, Grad Fahrenheit, die als Grad Celsius angegeben worden sind, und Rechtschreibfehler.
● Es wurden keinerlei Qualitätskontrollen durchgeführt: Ausreißer, welche offensichtliche Fehler sind, wurden nicht korrigiert – eine Stadt in Kolumbien soll im Jahre 1978 drei Monate lang eine mittlere tägliche Temperatur von über 80°C aufgewiesen haben. Eine Stadt in Rumänien sprang direkt aus dem Sommer des Jahres 1953 in einem Frühlingsmonat mit minus 46°C. Dies sind vermeintlich „mittlere“ Temperaturen für einen ganzen Monat insgesamt. Aus St. Kitts, einer Insel in der Karibik, wurde einen ganzen Monat lang eine Temperatur von 0°C aufgezeichnet – und das zwei mal!
● Die Temperaturen der gesamten Südhemisphäre im Jahre 1850 und der folgenden drei Jahre beruhen auf einer einigen Station in Indonesien und einigen zufälligen Schiffsbeobachtungen.
● Wassertemperaturen der Ozeane repräsentieren 70% der Erdoberfläche, aber einige Messungen stammen von Schiffen, welche von Standorten 100 km landeinwärts geortet wurden. Andere Meldungen stammen von Schiffen in Häfen, wo die Wassertemperatur kaum repräsentativ für den offenen Ozean sein dürfte.
● Wenn ein Thermometer an einer anderen Stelle aufgestellt wird, lässt die Adjustierung vermuten, dass die alte Messstelle immer zugebaut war und „aufgeheizt“ wurde durch Beton und Gebäude. In Wirklichkeit kam die künstliche Erwärmung schleichend daher. Mittels einer Korrektur aufgrund von Gebäuden, die im Jahre 1880 noch gar nicht existierten, wurden alte Aufzeichnungen künstlich kälter gemacht. Adjustierungen von nur wenigen Änderungen der Messorte können ein ganzes Jahrhundert lang künstliche Erwärmungstrends erzeugen.
Details der schlimmsten Ausreißer
● Im April, Juni und Juli 1978 meldete die Station Apto Uto (Kolumbien, ID:800890) eine mittlere monatliche Temperatur von jeweils 81,5°C; 83,4°C und 83,4°C.
● Die Monats-Mitteltemperatur vom September 1953 in der rumänischen Stadt Paltinis wird mit -46,4°C angegeben (in anderen Jahren betrug das September-Mittel etwa 11,5°C).
● Am Golden Rock Airport auf der Karibikinsel St. Kitts werden für Dezember 1981 und 1984 mittlere monatliche Temperaturen von 0,0°C gemeldet. Aber von 1971 bis 1990 betrug das Mittel in allen anderen Jahren 26,0°C.
Der Report
Unglücklicherweise liegt der Report hinter einer Zahlschranke. Aber die beträgt nur 8 Dollar.
Der Forscher John McLean führte seine Forschungen ausschließlich auf eigene Kosten durch, so dass ihm die Zahlschranke ein wenig Ausgleich bietet für die Zeit und Mühe, die er aufgebracht hat. Er schreibt:
Dieser Report basiert auf einer Aufgabenstellung für meine PhD-Arbeit. Ich erhielt meinen PhD im Dezember 2017 von der James Cook University in Townsville, Australien. Die Aufgabenstellung beruhte auf dem HadCRUT4-Datensatz und damit zusammenhängenden Datensätzen mit Stand von Ende Januar 2016. Dabei wurden 27 bedenkliche Umstände hinsichtlich des Datensatzes erkannt.
Die Versionen vom Januar 2018 der Datensätze enthielten nicht nur Aktualisierungen für die inzwischen vergangenen 24 Monate, sondern auch zusätzliche Messpunkte und daraus resultierende Änderungen der monatlichen globalen mittleren Temperatur-Anomalie bis zurück zum Beginn des Datensatzes im Jahre 1850. Der Report zieht die Daten vom Januar 2018 heran und überarbeitet die Analyse der ursprünglichen These bzw. erweitert diese, manchmal mit dem Weglassen unbedeutender Dinge, manchmal mit der Aufteilung wichtiger Themen und manchmal mit der Analyse neuer Bereiche. Die Ergebnisse werden in diesem Report beschrieben.
Die These wurde von Experten außerhalb der Universität unter die Lupe genommen, mit Bezug auf die Kommentare der Inspektoren überarbeitet und dann von der Universität akzeptiert. Der Prozess war zumindest äquivalent zu einem „peer review“, wie es von wissenschaftlichen Journalen durchgeführt wird.
Ich habe eine Ausgabe gekauft und daraus die folgende executive summary erstellt. Ich empfehle jedem dringend, ebenfalls eine Ausgabe zu kaufen und seine Arbeit zu unterstützen.
EXECUTIVE SUMMARY
Soweit ich weiß, ist dies das erste Audit des HadCRUT4-Datensatzes, also des Haupt-Datensatzes, der die Grundlage bildet für Klimazustands-Berichte des IPCC. Regierungen und das UNFCCC stützen sich schwer auf die IPCC-Berichte, so dass unabdingbar ist, dass die Temperaturdaten präzise und zuverlässig sind.
Diese Überprüfung zeigt, dass nichts dergleichen der Fall ist.
Über 70 Bereiche wurden untersucht, welche den gesamten Prozess abdecken von der Messung der Temperaturen bis zur Erstellung des Datensatzes, den daraus abgeleiteten Daten (wie etwa Mittelwerte) und deren eventueller Veröffentlichung. Die Ergebnisse (in kompakter Form in Anhang 6 gezeigt) enthalten sogar einfache Dinge wie offensichtlich falsche Daten, beschönigte Mängel von Daten, bedeutende, aber fragwürdige Hypothesen und Temperaturdaten, die unrichtig auf eine Art und Weise adjustiert worden sind, dass sie Erwärmung übertreiben.
So wurden beispielsweise Stationen gefunden, aus denen mittlere monatliche Temperaturen über 80°C vorlagen, zwei Beispiele einer Station in der Karibik mit gemeldeten mittleren Dezember-Temperaturwerten um 0°C und eine Station in Rumänien mit einer Meldung über ein September-Temperaturmittel um -45°C, wo doch das typische Septembermittel etwa 10°C beträgt. Als Clou des Ganzen gibt es auch Meldungen von Schiffen, die Standorte angaben, welche über 80 km landeinwärts liegen.
Es scheint, dass die Lieferanten der Temperaturdaten von Land und See keinerlei Checks auf grundlegende Fehler durchgeführt haben, und die Personen, welche den HadCRUT4-Datensatz erzeugten, haben diese Fehler nicht gefunden und auch keinerlei Fragen gestellt.
Das Verfahren zur Erstellung des Datensatzes hat einige Fehler entfernt, aber es wurde ein Satz von zwei Werten verwendet, berechnet aus einem Teil der Daten, aus denen jedoch vor der Berechnung die Fehler nicht korrigiert worden waren.
Datenmangel ist ein reales Problem. Der Datensatz beginnt im Jahre 1850, aber über zwei Jahre lang zu Beginn der Aufzeichnungen stammten die Festlands-Daten der Südhemisphäre von einer einzigen Station in Indonesien. Nach fünf Jahren gab es in jener Hemisphäre ganze drei Stationen. Globale Mittelwerte werden aus den Mittelwerten von jeder der beiden Hemisphären berechnet, so dass diese wenigen Stationen einen enormen Einfluss auf das haben, was vermeintlich „global“ genannt wird. Bezogen auf die Anzahl der Daten ist der Prozentsatz der Welt (oder der Hemisphäre) welche die Daten abdecken. Dem Verfahren der Berechnung der Abdeckung mit Daten in dem Datensatz zufolge wurden 50% der globalen Abdeckung erst im Jahre 1906 erreicht und 50% der Südhemisphäre erst im Jahre 1950.
Im Mai 1861 betrug die globale Abdeckung gerade mal 12% – weniger als ein Achtel. In vielen der Jahre von 1860 bis 1880 bestand die vermeintlich globale Abdeckung nur aus Europa sowie den Haupt-Schiffsrouten und Häfen, also nur etwa 13% der Erdoberfläche. Um aus diesen Daten Mittelwerte zu berechnen und sich auf diese als „globale Mittelwerte“ zu berufen muss man schon sehr leichtgläubig sein.
Ein weiteres bedeutendes Ergebnis dieses Audits ist, dass viele Temperaturwerte unsachgemäß adjustiert worden sind. Die Adjustierung von Daten zielt darauf ab, eine Temperaturaufzeichnung zu erzeugen, welche sich ergeben hätte, falls man an den gegenwärtigen Messpunkten und mit der Ausrüstung derselben die Temperatur vor Ort gemessen hätte. Adjustierungen werden typischerweise vorgenommen, wenn eine Station oder deren Instrumentierung verändert wurde.
Ein typisches Verfahren der Daten-Adjustierung ist es, alle zuvor gewonnen Messdaten um den gleichen Betrag zu verändern. Die Anwendung dieses Verfahrens Fälle mit graduellen Änderungen (wie etwa eine wachsende Großstadt die wahre Temperatur zunehmend beeinflusst), ist sehr falsch und führt dazu, dass die früheren Daten stärker adjustiert werden als man es hätte tun sollen. Messpunkte können häufig versetzt worden sein, und wenn man alle Daten zuvor jedes Mal adjustiert, können die frühesten Daten weit unter ihrem korrekten Wert liegen, womit der ganze Datensatz einen übertriebenen Erwärmungstrend zeigt.
Unter dem Strich lautet die Schlussfolgerung, dass die Daten in keiner Weise für globale Studien geeignet sind. Daten vor 1950 leiden unter geringer Abdeckung und sehr wahrscheinlichen falschen Adjustierungen der Stationsdaten. Daten nach jenem Jahr wiesen zwar eine bessere Abdeckung auf, haben aber immer noch das Problem der Daten-Adjustierung sowie die ganze Vielfalt der anderen, in dem Audit untersuchten Dinge.
Eine Berechnung der korrekten Temperaturen würde eine riesige Menge detaillierter Daten, Zeit und Mühe erfordern, was über diese Untersuchung hinausgehen würde und vermutlich unmöglich ist. Die primäre Schlussfolgerung des Audits lautet jedoch, dass der Datensatz eine übertriebene Erwärmung zeigt und dass globale Mittelwerte viel weniger sicher sind als behauptet.
Eine Implikation der Untersuchung ist, dass Klimamodelle frisiert wurden, damit deren Ergebnisse zu falschen Daten passen. Das verfälscht ihre Prophezeiungen zukünftiger Temperaturen und Schätzungen des menschlichen Einflusses auf die Temperatur.
Eine weitere Implikation ist, dass die Prämisse des Pariser Klima-Abkommens, nämlich die Mittelwerte der Jahre 1850 bis 1899 als „indikativ“ für die Temperaturen in vorindustrieller Zeit anzusehen, grundfalsch ist. Während jenes Zeitraumes war die globale Abdeckung mit Daten gering – nicht mehr als 30% über jenen Zeitraum – und viele Temperaturwerte vom Festland sind sehr wahrscheinlich exzessiv adjustiert worden und daher falsch.
Eine dritte Implikation ist, dass selbst falls die Behauptung des IPCC korrekt wäre, dass also die Menschheit für den größten Teil der Erwärmung seit 1950 verantwortlich ist, dann wäre die Größe einer solchen Erwärmung über die vergangenen 70 Jahre immer noch vernachlässigbar. Dann erhebt sich die Frage, ob die Mühen und Kosten, dieser zu begegnen, überhaupt in irgendeiner Weise Sinn machen.
Die Meinung des Autors ist ultimativ, dass der HadCRUT4-Datensatz und jedwede darauf beruhende Berichte oder Behauptungen in keiner Weise eine glaubwürdige Grundlage sind für Klimapolitik oder internationale Abkommen hinsichtlich der Gründe des Klimawandels.
Der ganze Report steht hier
Link: https://wattsupwiththat.com/2018/10/07/bombshell-audit-of-global-warming-data-finds-it-riddled-with-errors/
————————————
Auch der Karikaturist Josh, dessen Karikaturen verwendet werden dürfen, hat sich schon gedanken zum jüngsten IPCC-Bericht gemacht:
Josh: Der neue IPCC-Bericht ist ein Kunstwerk
Der Karikaturist Josh konnte schon vor der Veröffentlichung des IPCC SR 15-Berichtes darin Einblick nehmen und meint, dass ein technisches Wert hoch moderner Kunst ist:
Ich frage mich, was ist dieser Report wert?
Link: https://wattsupwiththat.com/2018/10/07/josh-the-new-ipcc-report-is-a-work-of-art/
Übersetzt von Chris Frey EIKE
Im Übrigen würden erfahrene Auditoren einiges über das Softwaremanagement der Klimamodelle wissen wollen: Jede Änderung der Software seit 20 Jahren muss eine eindeutige Revision haben, eine Änderungsbeschreibung, und Simulationsergebnisse, die in öffentliche Publikationen fließen müssen sich exakt auf entsprechende SW-Revisionen zurückführen lassen. Eine Publikation ist nichts weiter als ein Produkt: Wird später ein Fehler in der Software festgestellt muss der ‚Kunde‘ (Regirung, IPCC, o..) informiert werden, eine Korrektur nachgereicht.
Wetten, dass das PIK dafür, was in der Industrie teils gesetzlich gefordert ist, überhaupt keine dieser Voraussetzungen erfüllt, dass deren Qualitätsmanagement in keiner Weise den Milliarden-schweren Folgen gerecht wird. Wetten, dass unsere Regierung keinen blassen Schimmer davon hat, wie wichtig ein entsprechendes Qualitätsmanagement ist.
Es wird doch immer behauptet, 97% der Wissenschaft haben ein klares Bild der Situation. Wie ist es dann möglich, daß Inkonsistenzen im wichtigsten Material, auf dem alles aufsetzt, offenbar seit Beginn an unentdeckt enthalten sind und mitausgewertet werden? Was machen all diese „Experten“ denn eigentlich die ganze Zeit?
Schauen Sie mal hier. Das ist unserem Herrn Limburg schon 2010 aufgefallen. Stochern im Nebel, heißt das in der realen Welt.
Herr Strasser, Haben Sie noch nie was von „Climate-Gate“ gehört. Dann haben Sie immer die falsche Literatur gelesen. Bleiben Sie hier bei EIKE, dann bekommen Sie Informationen und Fakten von der anderen Seite. Und wir sind unabhängig, weil wir damit kein Geld verdienen. Und wie wurde der Hohepeißenberg Datensatz „homogenisiert“? Die Anfangswerte wurden von einem P.Winkler nach unten (kälter) korrigiert. Da die Messstation bis 1936 an der Nordseite der unbeheizten Klosterkirche angebracht war, hätten die Werte nach oben korrigiert werden müssen. Seitdem steht die Station nämlich im Freien, rundum sonnenbeschienen, wenn die Sonne scheint.
… frage ich mich, wie kann es sein, daß das erst 2018 jemandem auffällt?…
Auf solchen schlechten Eingangsdaten verlässliche Aussagen über die Zukunft, über unzählige Iterationen der Klimamodelle, die im Bereich von deterministischem Chaos angewendet werden, zu machen ist mathematisch völliger Unfug. Traurig, dass sich die meisten Mathematiker und Physiker so eine schlampige Wissenschaft von den Klimaforschern bieten lassen, ohne endlich auf die methodischen Schwächen hinzuweisen. Nun, mein Physik Professor am Lehrstuhl hat diesen Unfug schon vor 20 Jahren nicht akzeptiert, und ich – immerhin mit einer Diplomarbeit über Nichtlineare Systeme (nicht bzgl. Klima) – kann auch nur den Kopf schütteln über soviel Schlamperei, Unvermögen und Hochmut.
Geht es jetzt nicht abwärts mit den Sonnenfleckenschwankungsmaxima? Man schaue sich mal die Prognosen der Sonnenforscher an. Demgemäß wird es mit den Temperaturen abwärts gehen.
Es handelt sich weder um Unvermögen und Hochmut noch um Schlamperei, sondern um vom Berliner Politgesindel gesteuerte Propaganda. Jeder der aktiv Beteiligten weiß, daß er lügt!
Ich habe die Daten der Stationen der USA östlich des 100 Längengrades aller Stationen korrigiert. Gemessen haben Freiwillige TMIN und TMAX, allerdings nicht nur am Morgen, sondern auch Am Abend.
Ich habe nur die Fehler korrigiert und zwar anhand der Daten der nächsten Stationen. Wenn also die Fahrenheittemperaturen gravierende Abweichungen zeigten wurde korrigiert. Eines wurde aber nicht gemacht: Die Temperaturdifferenzen im üblichen Bereich wurden nicht angepaßt.Geht ja auch nicht, denn eine Station mitten in der Stadt hat eine höhere „Mitteltemperatur“ als die auf dem Land, ein paar Kilometer weg.
Letztendlich ging es nur um Schreib- und Übertragungsfehler. Die Originalzettelbilder gibt es übrigens im Netz zum Runterladen. Da kann man dann erkennen wer die Fehler gemacht hat.
Mein US-Datensatz ist m.E. fast fehlerfrei.
Blödland weiß das aber nicht, denn die Bevölkerung wird dumm gehalten.
Und wenn man die deutschen Datensätze anschaut, so liegt Hamburg Fühlsbüttel nunmehr schon lange mitten in der Stadt, Lübeck Flughafen 50 bis 100m von der Landebahn und Abstellfläche entfernt und ca. 20m von einer stark befahrenen Straße.
Bremerhaven Doppelschleuse zwischen Weser, Fischereihafen und Stadt und 30 Jahre nördlich eines AKW an der Weser.
Cuxhaven liegt ebenfalls direkt am Hafen und der Elbe und südlich ist die Stadt.
Und wer sich die US-Stationen anschaut findet die Wetterhütten über Parkplätzen, neben Klimaanlagenauslässen usw. … .
Alles unbrauchbare Temperaturmessungen.
Das ist aber eine durchgängige Methode der Klima-Politik und ihrer willfährigen GEZ Medien. Es werden keine Grundsatzdiskussionen geführt, nur über das wie darf gesprochen werden, niemals über das warum. Analog die Diskussion beim Dieselmotor (die Diesellüge). Dass ein Grenzwert von 40 µg/m³ NO2 eigentlich eine Lachnummer ist wenn der MAK Wert bei 950 µg/m³ liegt wird einfach nicht thematisiert. Man suggeriert an Hand von willkürlichen Grenzwerten inexistente Gefahren um sich dann als Retter aufzuspielen. Beim Klima erfindet man eine „Gegenstrahlung“, das CO2 wird zum „Umweltgift“, beim Auto ist es das NO2 oder der ominöse „Feinstaub“ (der massenhaft jedes Jahr von Bäumen produziert wird = Pollen). Die Fledermäuse im Hambacher Forst werden höchstrichterlich geschützt, die 250,000 die an den Windrädern sterben hingegen sind den Lückenmedien keine Meldung wert. Wie gesagt, auf der Farm der Tiere sind die Öko-Schweine dabei die Macht zu übernehmen. Wir schaffen das….
Diese sind im höchsten Maß grün versifft.
Gestern auf WDR wurden die schlechten Umfragewerte der CDU auf mangelnde Aktionsbereitschaft in Sachen Umwelt und Klima zurückgeführt. Diese Aussage wurde untermauert durch die „Passantenbefragung“ von zwei „Blondieleins“…
Oder die wollen das Publikum veräppeln.
Noch gravierender sind die Spiegelfechtereien der sachlichen Grundlagen all dieser Berichte und Rechenprogramme. Schellnhuber sagt – und soweit ist es richtig – das wären alles „Szenarien“.
Aber das sind „Wenn-Dann“-Verknüpfungen. Gemeint ist: „wenn“ CO2 ansteigt, „dann“ wird die Erde wärmer. Und die Unterschiedlichkeit der daraus abgeleiteten Prognosen hängt ab von der CO2-Ausstoß-Menge. Mit anderen Worten: das, was die Rechenprogramme suggerieren zu beweisen, nämlich CO2 macht Wärme, wird als Fakt in die Rechnungen schon vorab eigegeben. Ein Zirkelschluß, für den ich auf der Penne die Versetzung riskiert hätte.
Die IPCC-Berichte beschreiben Wetter (nicht den Treibhauseffekt) und stilisieren diese Wetterereignisse zu Katastrophen, was sie im Einzelfall auch sein mögen – aber nirgends und nie geben sie einen Beweis für die CO2-Ursache. Darin sind diese Berichte und Rechenprogramme allesamt Fehlanzeige und billigster Hütchenspieler-Betrug.
Schlimmer noch: die einfachste Physik wird negiert. Nämlich: CO2 gibt im Gegensatz zu N2 Wärme in das Vakuum des Weltalls ab, und hat deshalb eine Kühlfunktion. Ein Treibhauseffekt ist wissenschaftlich unmöglich.
Je weniger wir Trugbilder bewundern, desto mehr vermögen wir die Wahrheit aufzunehmen. [Erasmus von Rotterdam]