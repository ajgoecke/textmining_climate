<doc sitename="EIKE - Europäisches Institut für Klima &amp; Energie" title="BEST mit praktizierten Sprüngen des Unsicherheits-Niveaus in ihren Klimadaten" author="Anthony Watts" date="2015-02-03" source="https://eike-klima-energie.eu/2015/02/03/best-mit-praktizierten-spruengen-des-unsicherheits-niveaus-in-ihren-klimadaten/" hostname="eike-klima-energie.eu" excerpt="Anthony Watts In einem längeren Beitrag beschreibt Brandon Shollenberger einige eigenartige Probleme in den BEST-Temperaturaufzeichnungen, und zwar Teil 1 hier und Teil 2 hier. Aber ich möchte eine Übersicht geben. BEST hat sein Unsicherheitsniveau nach folgender Methode berechnet: Man hat ein Achtel der Daten entfernt und die Mittelberechnungen noch einmal durchlaufen lassen (und danach die Varianz der Ergebnisse untersucht). Ich habe hier zwei Punkte beleuchtet, von denen ich noch nicht gesehen habe, dass sie diskutiert werden. Falls Ihnen die unzureichenden Vertrauensintervalle bei den Rekonstruktionen von Marcott et al. bekannt sind, wird Ihnen auch einiges hiervon bekannt vorkommen." categories="Klima" tags="" fingerprint="cItfYFek0hGjhBbeCpwjdQENV9k=">
  <main>
    <p>Bild rechts: Man beachte die Schritt-Änderung. Etwa im Jahre 1960 ist das Unsicherheitsniveau abgestürzt; oder anders ausgedrückt, BEST behauptet, dass man sich danach doppelt so sicher ist bzgl. ihrer Temperaturschätzungen, und zwar praktisch über Nacht.<lb/>Erstens, BEST lässt nur die Mittelwert-Berechnungen noch einmal durchlaufen, um deren Unsicherheit zu bestimmen. Sie berechnen aber nicht noch einmal den „Breakpoint“*. Man erinnere sich, BEST unterteilt die Daten von Temperaturstationen in Segmente, wann immer sie glauben, einen „Breakpoint“ gefunden zu haben. Hauptsächlich wird nach diesen Breakpoints gesucht, indem man die Stationswerte mit nicht weit entfernten benachbarten Stationen vergleicht. Falls an einer Station der Unterschied zu den Nachbarstationen zu groß ist, unterteilt man die Werte dieser Station in Segmente, die nachfolgend neu angepasst [realigned] werden können. Dies ist eine Form von Homogenisierung, ein Prozess, bei dem die Werte benachbarter Stationen so bearbeitet werden, dass sie sich ähnlicher sind.<lb/>[*Dieses Wort hat mehrere Bedeutungen, und weil ich nicht weiß, welche am besten passt, lasse ich den Begriff so stehen. Anm. d. Übers.]<lb/>Dieser Prozess wird bei den Unsicherheits-Berechnungen von BEST nicht wiederholt. Der ganze Datensatz ist homogenisiert, und Untergruppen dieser homogenisierten Datensätze werden verglichen, um zu bestimmen, wie viel Varianz darin steckt. Das ist unsachgemäß. Die Größe der Varianz, die BEST innerhalb eines homogenisierten Datensatzes findet, sagt nichts darüber, wie groß die Varianz in den BEST-Daten ist. Sie sagt uns nur, wie groß die Varianz ist, wenn BEST erst einmal die Homogenisierung der Daten abgeschlossen hat.<lb/>Zweitens, um zu berechnen, wie groß die Varianz in seinen (homogenisierten) Datensätzen ist, lässt BEST die Berechnungen noch einmal laufen, wobei zuvor ein Achtel der Daten entfernt worden ist; acht mal. Dies erzeugt acht verschiedene Reihen. Vergleicht man diese verschiedenen Reihen miteinander, passt BEST die Daten so an, dass sie alle der gleichen Grundlinie folgen. Der Grundlinien-Zeitraum, den BEST für diese Anpassung verwendet, ist 1960 bis 2010.<lb/>Das ist ein Problem. Wenn man die acht Reihen nach dem Zeitraum 1960 bis 2010 ausrichtet, wird die Varianz zwischen jenen Reihen im Zeitraum 1960 bis 2010 künstlich gedämpft (und die Varianz anderswo künstlich erhöht). Dies erweckt den Anschein, dass es im letzten Teil der BEST-Aufzeichnung mehr Sicherheit gibt als in Wirklichkeit vorhanden. Das Ergebnis: Es gibt einen künstlichen Stufenschritt im Unsicherheitsniveau bei BEST um das Jahr 1960. Dies ist das gleiche Problem, das schon bei Marcott et al. gezeigt worden ist (hier).<lb/>Alles in allem ist das Unsicherheitsniveau bei BEST ein einziges Durcheinander. Es ist unmöglich, diese in irgendeiner bedeutsamen Weise zu interpretieren, und sie können mit Sicherheit nicht dazu verwendet werden zu bestimmen, welche Jahre die wärmsten gewesen waren oder auch nicht.<lb/>Link: http://wattsupwiththat.com/2015/01/29/best-practices-increase-uncertainty-levels-in-their-climate-data/<lb/>Übersetzt von Chris Frey EIKE</p>
    <p>Bild rechts: Man beachte die Schritt-Änderung. Etwa im Jahre 1960 ist das Unsicherheitsniveau abgestürzt; oder anders ausgedrückt, BEST behauptet, dass man sich danach doppelt so sicher ist bzgl. ihrer Temperaturschätzungen, und zwar praktisch über Nacht.</p>
  </main>
  <comments>
    <p>Natürlich sollte man solche Vorgänge beachten und die Daten entsprechend anpassen. Nur ist die Methode von BEST und auch von GISS/NASA und anderen diese, dass man das nur auf dem Papier versucht, herauszubekommen. Nämlich indem man schaut, ob Stationen von Nachbarstationen auffällig abweichen.</p>
    <p>Und dann wird der Ausreißer glattgebügelt. Dabei wird missachtet, das ein lokales Klima ganz anders sein kann als der Durchschnitt.</p>
    <p>Bei der GISS-Datenreihe werden übrigens fehlende Temperaturmesstationen durch Daten von Nachbarstationen, die bis zu 1250km entfernt sind, ersetzt. Weil es eben in manchen Gegenden dazwischen nichts gibt. Das ist, als würde man die Temperatur von München mit der von London und Rom vergleichen und korrigieren.</p>
    <p>Glaube keiner Statistik, die du nicht selbst gefälscht hast!</p>
  </comments>
</doc>