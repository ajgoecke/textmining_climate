<doc sitename="EIKE - Europäisches Institut für Klima &amp; Energie" title="Die schwierige, nimmer endende Fehlerdiskussion! Oder wie gut sind historische meteorologische Temperaturdaten? Wie wurden sie ermittelt, wie zusammengestellt und wie verarbeitet? (Teil 1)" author="Admin" date="2020-04-28" source="https://eike-klima-energie.eu/2020/04/28/die-schwierige-nimmer-endende-fehlerdiskussion-oder-wie-gut-sind-historische-meteorologische-temperaturdaten-wie-wurden-sie-ermittelt-wie-zusammengestellt-und-wie-verarbeitet-teil-1/" hostname="eike-klima-energie.eu" excerpt="Eine Übersicht in 3 Teilen von Michael Limburg : &quot;...Using data from all these poor stations, the U.N.'s Intergovernmental Panel on Climate Change estimates an average global 0.64oC temperature rise in the past 50 years, &quot;most&quot; of which the IPCC says is due to humans. Yet the margin of error for the stations is at least three times larger than the estimated warming&quot; BEST Studienleiter Richard Muller 21.10.2011 Wallstreet Journal [1]" categories="Klima" tags="" fingerprint="C7smuwZrZz1VSzBwdyXE7geC/ws=">
  <main>
    <head rend="h3">Teil 2 hier und Teil 3 hier</head>
    <p>Mein Beitrag über zwei kleine Anfragen der AfD (hier) zur ungelösten Problematik der absoluten globalen Mitteltemperatur in jüngerer historischer Zeit – die Pariser Klimaübereinkunft bezieht sich sogar auf „Werte vorindustrieller Zeit“ ohne diese auch nur ansatzweise zu beziffern- hat eine große, manchmal hitzige, Diskussion ausgelöst. Mit aktuell 127 Kommentaren und über 3000 Aufrufen (Stand 27.4.20 12:00 Uhr) gehört er sicher zur Klasse der Meistgelesenen. Soweit so gut.<lb/>Die z.T. hitzigen Kommentare zeigen m.E.n. aber auch, dass viele Leser mit der zugrunde liegenden, manchmal komplizierten, meteorologischen Messtechnik nicht so vertraut sind, wie sie der Artikel vielleicht voraussetzt. Das hat zur Folge, dass sie nicht auf Anhieb verstehen, warum der Artikel Versäumnisse – gar Falschinformationen- der Bundesregierung – beanstandet.<lb/>Sie vertrauen dabei auf ihre eigenen, oft in wissenschaftlicher Arbeit gemachten Erfahrungen aus eigener Anwendung der Messtechnik und ihrer Behandlung von Messdaten. Diese stimmen aber – so meine eigene Erfahrung – wenig bis nicht mit den hier angewendeten Methoden und Verfahren, insbesondere bei der Erstellung und Bewertung von Zeitreihen, überein. Und sind deshalb in ihrer Schlichtheit selten auf diese Messdaten anzuwenden.<lb/>Dies voraus geschickt will ich die Leser jetzt ein wenig in die meteorologische Messtechnik von Temperaturdaten und deren Auswertung einführen. Dabei sollen die alten wissenschaftlichen Grundsätze gelten, klassifiziere erst und vergleiche dann, aber vergleiche immer nur Vergleichbares.<lb/>Zunächst schauen wir uns an, wie, wo, wann und wie oft die historischen Temperaturdaten gemessen und vorverarbeitet wurden.<lb/>Dann werden wir uns die Bildung von Anomalien aus diesen bereits vorverarbeiteten Daten und die sich daraus ergebenden Besonderheiten anschauen und zum Schluss die Bildung von Zeitreihen, und wie diese auf Fehlerkompensation bzw. Nichtkompensation Einfluss nehmen.</p>
    <head rend="h3">Was nicht behandelt wird.</head>
    <p>Gänzlich außen vor lassen wir hier den magischen Trick der Klimaforscher, aus bekannten absoluten Unsicherheiten der Messwerte durch Bildung von Anomalien aus vielen Temperaturdaten statistische Wahrscheinlichkeiten werden zu lassen, die sich mit der bekannten Wahrscheinlichkeitsverteilung der Normalverteilung über ein oder zwei Sigma berechnen ließen. Darüber hat Kip Hansen eine sehr eindrückliche Kritik geschrieben. Sie finden Sie hier unter dem Titel„Der „Trick“ anomaler Temperatur-Anomalien“<lb/>Weil es das Thema viel zu weit ausdehnen würde, lassen wir auch die merkwürdige Behauptung des Stammvaters aller Klimaängste, James Hansen außen vor, der in einem paper zusammen mit seinem Kollegen Lebedeff 1987[2] behauptet, es wäre möglich auch mit einer sehr geringen Zahl von ca. 80 Gitterzellen gleicher Fläche von 2500 x 2500 km die Temperaturentwicklung der gesamten Erde zu bestimmen. Und dies obwohl in den meisten Zellen nur sehr wenige, in anderen überhaupt keine Messstationen vorhanden waren. Dieser Artikel war die Grundlage und ist es noch heute, für die Behauptung, dass die Anomalienbildung und deren Korrelation über große Flächen hinweg aussagefähig für die Temperaturentwicklung sehr große Flächen seien. Diese Idee, die jeder Korrelations- und Fehlerrechnung widerspricht, wird hier unter dem Titel Von Korrelationen, Trends bei Anomalien! besprochen<lb/>Und ebenfalls lassen wir die Erkenntnis von Dresdner Statistikern außen vor, die nachwiesen, dass selbst echt zufällige Fehler, sofern sie sich in Daten finden, die für autokorreliert sind und in entsprechenden Zeitreihen verwendet werden, systematische Komponenten in der Größe von einigen Zehntel Grad entwickelten. Details dazu finden Sie im Artikel mit dem Titel „Konfidenz-Intervalle* für zeitliche Mittelwerte bei langfristigen Korrelationen: eine Fallstudie zu Anomalien der Temperatur auf der Erde“<lb/>Am Schluss der Vorbemerkung erlaube ich mir noch den Hinweis auf die Tatsache, dass mit der Bildung von raum- und zeitüberspannenden Durchschnittswerten, die darin enthaltenen Temperaturwerte ihre physikalische Bedeutung als Temperatur verlieren. Es entsteht stattdessen irgendeine Art von Index. Er ist für statistische Zwecke begrenzt brauchbar, wenn man die Randbedingungen beachtet, aber keine physikalische energetisch relevante Größe mehr. Wer dazu genaueres wissen möchte, der sei auf den schöne Beitrag von C. Essex und R. McKittrick von 2006 verwiesen mit dem schönen Titel „Does A Global Mean Temperature exist“[3]</p>
    <head rend="h2">Wie, wo und wann wurden die historischen meteorologischen Messdaten gewonnen?</head>
    <list>
      <item>Thermometer</item>
    </list>
    <p>Am Anfang stand und steht das Thermometer. Dabei ist es zunächst egal, ob es ein mechanisches oder ein elektronisches ist, obwohl jede Art einen erheblichen Einfluss auf die (systematischen) Fehler hat, natürlich auch auf die erzielbare Genauigkeit.</p>
    <list>
      <item>Messumgebung</item>
    </list>
    <p>Doch mindestens ebenso wichtig ist die Messumgebung. Diese wird bestimmt durch das Gehäuse (Messhütte), welches das Thermometer vor verfälschenden Umgebungseinflüssen, wie Strahlung, Wind oder Niederschlag aber auch Beschädigung schützen soll, aber auch in starkem Maße von der unmittelbaren Umgebung. Um vergleichbare Ergebnisse wenigstens annähernd zu erreichen, muss dies bestimmten überall vergleichbaren Standards genügen und sorgfältig gebaut und gepflegt werden.</p>
    <list>
      <item>Messregime</item>
    </list>
    <p>Und auch das Messregime ist von entscheidender Bedeutung, nämlich wann und wie oft, von wem und wie das Thermometer abgelesen wird.</p>
    <list>
      <item>Die geografische und zeitliche Abdeckung</item>
    </list>
    <p>Und da das Wetter- wie das Klimageschehen nicht an nationalen Grenzen haltmacht, muss die geografische Abdeckung mit Messstationen gesichert sein, und das sowohl nach Zahl als auch nach Ort und Zeit. Alle drei Anforderungen werden erst seit dem Aufkommen der Satelliten hinreichend erfüllt. Dabei tauscht man die Vorzüge der Satellitenmessung gegen einen enormen Nachteil ein, denn man musste einen unangenehmen Kompromiss akzeptieren. Es ist die sehr schlechte vertikalen Auflösung. Eine Satellitenmessung ist daher immer der Mittelwert von Temperaturänderung über eine Distanz von ca. 5000 m. (z.B. der lower troposphere)</p>
    <list>
      <item>Was geschieht mit den meteorologischen Messdaten? Die Weiterverarbeitung!</item>
    </list>
    <p>Bis zur „Erfindung“ der Idee von der Notwendigkeit einer weltweiten Mitteltemperatur, gab es nur wenige Meteorologen, die die vorhandenen Messdaten nutzen wollten, um evtl. aus ihnen eine weltweite Mitteltemperatur zu extrahieren. Man begnügte sich damit – nachdem solche großartigen Wissenschaftler der Meteorologie wie Köppen die Definition von Klimazonen etabliert hatten- für diese Klimazonen oder Regionen Mitteltemperaturen und deren Jahresgänge zu berechnen. Normalerweise dienten die meteorologischen Messdaten aber vor allem der örtlichen Wettervorhersage und bez. ihrer Langzeit-Analyse auch dazu ggf. lokale Klimaänderungen im Nachhinein zu erkennen.<lb/>Anmerkung: Achten Sie mal auf die örtlichen Wetterberichte, dort wird man ihnen so gut wie nie die erwartetet Durchschnittstemperatur der nächsten Tage nennen, sondern ausschließlich deren Maxima und Minimawerte.<lb/>Dazu erstellte man Zeitreihen (hier Temperatur-Ganglinien) entweder aus absoluten Werten oder von Anomalien je nach Bedarf, deren erzielbare Genauigkeit für die gewünschten Zwecke völlig ausreichte.</p>
    <head rend="h3">Zu 1. Thermometer</head>
    <p><lb/>Abbildung 1 zeigt eine klassische, deutsche Wetterhütte (erweiterter Stevenson Screen) und (rechts) ein darin befindliches modernes Thermometerensemble gezeigt, wie es immer noch in vielfältiger Vewendung ist. Die senkrechten Thermometer sind (links) das Assmansche Aspirationthermometer zur Temperaturmessung, rechts daneben ein befeuchtetes und belüftetes Psychrometer zur Messung der Luftfeuchte. Quer darunter befinden sich zwei liegende Thermometer zur Messung von Max. und Min. Temperaturen. Die runde Plastiktrommel enthält den Federmotor mit Ventilator zur Erzeugung des Luftstromes mit Ø 2,5 m/s vor und während der Messung. Im März 2009 vom Autor im Metereologischen Institut der Freien Universität Berlin aufgenommen<lb/>Obwohl schon Galileo erste Versuche unternahm mit einem Thermometer meteorologische Messungen zu machen, und sich danach ein ziemlicher Wildwuchs an solchen Thermometer ausbreitete, weil viele Landesfürsten die aufkommende Meteorologie aus naheliegenden Gründen für äußerst wichtig hielten, kann man erst ab 1892 von einem Standardinstrument für meteorologische Zwecke sprechen. Es handelt sich dabei um das zwangsbelüftete Assmansche (Quecksilber gefüllte) Aspirationsthermometer (linkes, senkrechtes Thermometer). Die zuvor verwendeten tlw. mit Alkohol gefüllten Thermometer mit mehr als ± 1 Kelvin Gesamt-Unsicherheit wurden nach und nach weltweit ersetzt. In den angelsächsisch orientierten tlw. auch regierten Ländern setzte sich hingegen oft zusätzlich das Max-Min-Thermometer durch.<lb/>Doch allein die Wahl des Thermometers, sein jeweiliger Zustand, seine Montage, sein Betrieb und seine Ablesung sorgen für eine ganze Reihe von zufälligen wie systematischen Fehlern. Diese wurden vielfach sorgfältig untersucht, aber nur in geringer Anzahl auch quantitativ bestimmt und damit überhaupt erst theoretisch korrigierbar.</p>
    <head rend="h3">Zu 2. Messumgebung</head>
    <p>Für die Messumgebung war von Anfang an klar, dass dieses zuallerst eine Art Schutzhütte sein sollte, von denen besonders im 19 Jahrhundert eine ganze Reihe von Konstruktionen erprobt wurden, wobei sich letztendlich die erweiterte englische Hütte als Standard durchsetzte. Doch bis dahin waren die (wenigen) Messergebnisse praktisch unvergleichbar, weil die Hütte auf ihre eigene Art die Messergebnisse beeinflusste.<lb/>Abbildung 2 Darstellung der zum Ende des 19. Jahrhunderts und bis heute verwendeten Hüttentypen bzw. Screens von links: französische Hütte, daneben (kaum erkennbar) Mitarbeiter zum Größenvergleich, daneben (Original) Stevenson Hütte, dann Wildsche Hütte, dann mehrere erweiterte Stevenson Hütten und moderne runde kleine Screens. (Quellen. Linkes Bild [Sprung, 1890] rechtes Bild: KNMI Messfeld von van der Meulen für die Zeit von 1989-1995)<lb/>Praktisch unkorrigiert, obwohl seit langem bekannt, ist der systematische Fehler, der allein durch die Wärmeträgheit der Hütte entsteht, und der ca + 1 bis 2 Kelvin zu jedem Ergebnis der eigentlich gesuchten Außentemperatur hinzuaddiert. Praktischerweise definieren jedoch die Meteorologen die Temperatur in der Hütte[4], als die gesuchte Außentemperatur. Woraus geschlossen werden muss, dass jede durch diese Messungen gewonnene absolute Temperatur im Mittel im 1 bis 1,5 Kelvin gegenüber der wahren Außentemperatur zu hoch sein muss.<lb/>Abbildung 3(links): Englische Hütte und moderne Messstation am Messort Detroit Lakes USA, kurz vorher aufgenommen und ins Netz gestellt am 25.07.07 von A. Watts; (rechts) Temperaturverlauf aus der GISS Datenbank zur Messstelle Detroit Lakes USA<lb/>Beispiel für den Zustand und Umgebung einer meteorologische US Messstation sowie deren Zeitreihe ihrer Jahresmittelwerte, die vom Projektteam untersucht wurde. Man beachte den Sprung ab etwa dem Jahr 2000 und die Entlüftungen der Klimaanlagen in unmittelbarer Nähe.<lb/>Nicht weniger Einfluss hat die unmittelbare Umgebung in der die Messhütte.<lb/>Zur Klärung dieses Sachverhaltes hat sich der Meteorologe und Betreiber des inzwischen weltweit erfolgreichsten Klimarealisten- Blogs Wattsupwiththat Antony Watts große Meriten erworben. Er rief 2007 das SurfaceStationorg. Netzwerk ins Leben, das 2009 erfolgreich mit einer Veröffentlichung [5]abgeschlossen wurde. Freiwillige im ganzen Land untersuchten über 850 der 1200 meteorologischen Messstationen, die von der US-Wetter- und Ozeanbehörde NOAA betrieben wurden.<lb/>Abbildung 4 Untersuchung von bisher (Ende Februar 2009) 854 von 1221 offiziell verwendeten Messstationen der unterschiedlichsten Art. Man beachte, dass nur 3 % aller Stationen dem Standard voll entsprechen, weitere 8 % können evtl. noch als dem Standard genügend zugeordnet werden. Die restlichen 89 % zeigen jedoch potentielle Abweichungen nach den Vorgaben der CRN von mindestens 1 °C (20 %) über 2 °C ( 58% ) bis 5 °C (11%).<lb/>Auswertung der Überprüfung von 807 US Messstationen in Bezug auf die offizielle Einstufung ihrer Qualität und Umgebung (CRN).<lb/>Das Ergebnis war erschreckend. Der Zustand der Messstationen und damit die Messergebnisse waren miserabel. 89 % von ihnen lagen oberhalb der Fehlerklasse 2, was bedeutet, dass sie &gt; 2 ° C bis &gt; 5 °C fehlerhafte Ergebnisse zeigten. Nur 3 % lagen gleich oder unterhalb von 1 °C. Und noch schlimmer war, dass diese Stationen für die gesamte Welt sozusagen als Goldstandard in Punkto Qualität galten. D.h. die vom IPCC und ihren Zuarbeitern immer wieder herangezogenen Daten aus den anderen Kontinenten waren mit hoher Wahrscheinlichkeit noch schlechter.<lb/>Es würde hier zu weit gehen, die vielfältigen Fehlerquellen allein der landbasierten Messstationen aufzulisten, daher beschränke ich mich auf zwei kurze Ergänzungen.</p>
    <list>
      <item>Selbst bei identischer Konstruktion von Hütte und Messinstrument, ergeben sich große Unterschiede allein durch das Material und seinen Anstrich, bzw. dessen Veränderung über der Zeit, z.B. durch Alterung</item>
      <item>Sind die landbasierten Messungen schon oft mit mehr (systematischen) Fehlern behaftet als ein Hund Flöhe hat, so ist das bei den seegestützten Messungen noch weit schlimmer. Und weil das Land nur 29 % der Erdoberfläche bedeckt, aber 71 % der Erde Ozeane sind, ist dieser Umstand eigentlich noch schlimmer. Denn dort sind die Messungen wirklich nur punktuell, d.h. äußerst selten sowohl in räumlicher als auch in zeitlicher Hinsicht. Ein wenig wird dieser Mangel gelindert durch die Tatsache, dass statt der eigentlich gesuchten Lufttemperatur, stattdessen, als Proxy, die Temperatur der Seewasser Oberfläche (SST Sea Water Surface Temperature) verwendet wird. Wegen der großen Wärmeträgheit und guten -Leitfähigkeit des Wassers deckt ein Messpunkt im Vergleich zum Land viel größere Gebiete ab.</item>
    </list>
    <p>Wer Interesse daran hat, diese Fehler etwas genauer behandelt zu sehen, der sei auf das Video „Wie glaubwürdig sind die Zeitreihen historischer Klimadaten“ (hier) verwiesen, dass einige der wichtigsten Aspekte zu diesen Fehlern aufzeigt.<lb/>Ende Teil 1<lb/>[1] https://www.wsj.com/articles/SB10001424052970204422404576594872796327348#printMode<lb/>[2] HANSEN, JAMES LEBEDEFF, SERGEJ (1987) Global Trends of Measured Surface Air Temperature. JOURNAL OF GEOPHYSICAL RESEARCH 92:13345 13372.<lb/>[3] Essex, C, Mc Kittrick, R. Andresen, B. (2006) Does A Global Temperature Exist? Non-Equilibrium Thermodynamics<lb/>[4] Weischet [Weischet, 1995] S115 – S 109 ff<lb/>[5] Is the U.S. Surface Temperature Record Reliable? By Antony Watts SurfaceStations.org ISBN 13: 978-1-934791-29-5<lb/>Für alle, die diese Information am Stück lesen wollen hier das Ganze als pdf Die schwierige nimmer endende Fehlerdiskussion</p>
  </main>
  <comments>
    <p>Nein. Vielleicht sollten Sie mal einen Blick auf die Algorithmen werfen, statt Ihrer eigenen Kreativität freien Lauf zu lassen. Siehe z.B. Homogenization of Temperature Series via Pairwise Comparisons</p>
    <p>Man kann sicherlich diskutieren, ob das die betrachteten Fehlerquellen eleminiert oder deren Fehler reduziert, oder … Aber einfach frei Dinge erfinden ist kein gutes Zeichen.</p>
    <p>Herr Marvin Müller, auch wenn Ihre Geduld mit mir zu Ende geht: Übersehen Sie doch bitte nicht das Fragezeichen am Satzende. 😉 Sie dürfen dann mit Recht annehmen, dass ich nichts erfunden, sondern maximal eine Hypothese aufgestellt habe.</p>
    <p>Ein Auszug aus Ihrem Link *paarweise Homogenisation* übersetzt:</p>
    <p>„… Der Algorithmus bildet paarweise Differenzreihen zwischen seriellen monatlichen Temperaturwerten aus einem Netzwerk von Beobachtungsstationen…..Der Algorithmus verwendet auch Stationsverlaufsinformationen, sofern verfügbar, um die Identifizierung künstlicher Verschiebungen in Temperaturdaten zu verbessern. Zusätzlich wird eine Bewertung durchgeführt, um Trendinhomogenitäten von abrupten Verschiebungen zu unterscheiden. Wenn die Größe einer scheinbaren Verschiebung, die einer bestimmten Station zugeordnet ist, zuverlässig geschätzt werden kann, wird eine Anpassung für die Zielserie vorgenommen. … “</p>
    <p>Das scheint doch ziemlich in die von mir angedachte Richtung zu gehen….</p>
    <p>Aber egal, zurück zum Ursprung: Auf meine hinter allem stehende Frage, ich formuliere einmal zusammenfassend: „Wie kann es sein, warum darf es sein, dass in einem GISS- Diagramm von 1989 die *US- Main- Temperaturen* der 1930-er Jahre anders angegeben sind als in einem GISS- Diagramm von 2012?“</p>
    <p>antworten Sie, ich fasse zusammen: „Das hat alles seine Richtigkeit. Die Daten werden ständig Prüfungen unterzogen, werden homogenisiert, periodisch durch Algorithmen mit anderen Stationen verglichen und korrigiert. Die Qualitätskontrollen sind transparent, die Algorithmen öfentlich. das ergibt regelmäßig ein neues Bild…“</p>
    <p>Ist das so in etwa richtig, Herr Marvin Müller?</p>
    <p>Ich jedenfalls finde die Frage gut, warum wir hinnehmen sollen, das die Data zweck-homogenisiert werden.</p>
    <p>Siehe auch hier https://jennifermarohasy.com/2019/02/changes-to-darwins-climate-history-are-not-logical/</p>
    <p>und</p>
    <p>hier https://ipa.org.au/publications-ipa/bureau-cooling-the-past-to-declare-record-heat</p>
    <p>Was bei der Kritik an den fuer die ATE-Forschung fuer relevant gehaltenen Messungen voellig uebersehen wird, ist dass CO2 nur etwa 1/4 des seit vorindustrieller Zeit im globalen Mittel (?) beobachteten transienten Temperaturanstiegs von fast 1 Grad bewirkt hat und praktisch kaum aus den Messwerten zu bestimmen ist. Haupttreiber der Temperaturaenderungen sind (abgesehen von Waermeinseln) Solaraktivitaet, Wasserdampf und Wolkenbedeckung.</p>
    <p>Mit der auf Basis hochpraeziser Labor- sowie Satelliten- und Strahlungsmessungen heute recht gut bekannten Verdoppelungs-Sensitivitaet des CO2 von 0,6 °C am Boden* (im Gleichgewicht, mit Wolken, Feedback etc.) ergibt sich rechnerisch deltaT=0,6*ln(ppm/280)/ln(2). Auch der emissionsbedingte CO2-Anstieg unter Beruecksichtigung der ppm-abhaengigen Netto-Senkenfluesse von Ozeanen und Biomasse (die pro 20 ppm um ~20 GtC/a steigen) – weshalb eine Dekarbonisierung garnicht noetig ist – kann mit einem C-Modell berechnet werden, siehe unter http://www.fachinfo.eu/dietze2020.pdf (engl. Version unter 2020e).</p>
    <p>*) Der Wert 0,6 °C von ECS (equilibrium climate sensitivity) laesst sich – unter der Praemisse dass der heute bei 410 ppm transient beobachtete Temperaturanstieg von 0,95 °C etwa um den Faktor 0,7 geringer ist und CO2 (wie Regressionsanalysen ergaben) einen Anteil von 25% daran hat – leicht berechnen mit</p>
    <p>ECS = 0,95/0,7/ln(410/280)*ln(2)*0,25 = 0,62 °C</p>
    <p>Netto-Senkenfluesse von Ozeanen und Biomasse (die pro 20 ppm um ~1 GtC/a steigen)</p>
    <p>Dann gibt es die vereinzelten Messungen der Wassertemperatur in den Ozeanen, die theoretisch 1m unter der Oberfläche gemessen werden sollten. Dann gibt es die Satellitenmessungen auf Basis der Sauerstoffbande im 0,5-0,6 cm-Band, welche z. B. bei Roy Spencer regelmäßig geführt wird. Diese Messung ergibt offenbar eine vertikale Mitteltemperatur üblicherweise der Troposphäre, aber auch andere Luftdruckzonen sind möglich.</p>
    <p>Und schließlich gibt es die Methode der Okkultation, welche die dichteabhängige Laufzeit von Funksignalen quer durch die Erdatmosphäre als Referenz für die Temperatur nimmt.</p>
    <p>All diese Verfahren haben unterschiedliche Eigenschaften bzw. Vor- und Nachteile. Und sie korrelieren teilweise, teilweise aber auch nicht. Auf welcher Basis stellt nun die Mainstream-Hysteriker-Klimatologie fest (auf zwei Stellen hinter dem Komma), welche Temperatur wir haben. Die Unterschiede in den einzelnen Methoden zeigen doch, daß wir „hochgenau“ eigentlich gar nichts wissen. Und wenn eine Harmonisierung nicht gelingt, müßte man ehrlicherweise eine Bandbreite angeben, innerhalb der der wahre Wert mit einer gewissen Wahrscheinlichkeit liegt. Ggfs. noch unterteilt in unterschiedliche Wahrscheinlichkeitsbänder, also z. B. 20%, 50%, 80%, 100%. Und, man wäre dadurch auch gezwungen, einmal klar zu definieren, was man eigentlich unter einer „Weltmitteltemperatur“ überhaupt versteht!</p>
    <p>Zur Sauerstoffmessung gibt es z. B. die Beschreibung: „Satelliten messen die thermische Mikrowellenemission von Luftsauerstoff im Sauerstoffabsorptionskomplex von 50-60 GHz. Die resultierenden kalibrierten Helligkeitstemperaturen (Tb) sind nahezu gleichwertig mit der thermometrischen Temperatur, insbesondere einem vertikal gewichteten Durchschnitt der Lufttemperatur, wobei die vertikale Gewichtung durch „Gewichtungsfunktionen“ dargestellt wird.“</p>
    <p>Frage: kennt jemand Details zu diesen Messungen, speziell wie das genaue Meßprinzip arbeitet? Allgemein wird doch behauptet, zweiatomige Moleküle wie O₂ strahlen nicht. Nach der Beschreibung scheinen sie aber mit temperaturabhängiger Frequenz doch zu strahlen?</p>
    <p>Spencer schreibt dazu in „UAH Version 6 Global Satellite Temperature Products: 3Methodology and Results“</p>
    <p>Die Aussage, das O_2 nicht strahlt ist in der Regel, dass es nicht im langwelligen Infrarotbereich absorbiert und strahlt. hier wird die Abstrahlung im Gigahertz-Bereich genutzt …</p>
    <p>Satelliten zur Temperaturmessung haben Bahnen unter 90° zum Äquator, laufen also auf einer Polarbahn, während die Erde sich unter ihnen durchdreht. Wenn so ein Satellit wie üblich 12 Umläufe pro Tag macht, erfolgt der nächste Scan um 30° versetzt zum letzten. Weil sich die Erde kontinuierlich dreht, verlaufen die Projektionen der Bahn auf die Oberfläche aber nicht auf Meridianen sondern schräg dazu. Dadurch ergibt sich ein Muster, bei dem ein Zweieck zunächst von Süd nach Nord abgetastet wird und ein dazu um 15° versetztes Zweieck 12 Stunden später von Nord nach Süd.</p>
    <p>Wenn man die Länge des Äquators grob mit 40 Mio. km rechnet, ergeben 30° am Äquator einen Abstand von 3,33 Mio. km und 15° ergeben 1,66 Mio. km. Es erhebt sich also die Frage, erfaßt das Meßprinzip die tatsächliche Fläche der entstehenden schrägen Kugelzweiecke oder was wird eigentlich genau gemessen?</p>
    <p>Wenn die Definition der Mitteltemperatur jene ist, die 2 m über der Oberfläche gilt, wären solche Satellitenmessungen nur dann überhaupt heranziehbar, wenn es eine eindeutige Beziehung gäbe, wie man Troposphärendurchschnitte in bodennahe Werte umrechnen kann, die es meines Wissens aber nicht gibt.</p>
    <p>Nebenbei bemerkt ist der „langwellige Infrarotbereich“ mit Wellenlängen im Micrometerbereich wesentlich kurzwelliger als der Gigaherzbereich mit hier um 0,5 cm Wellenlänge.</p>
    <p>der Erdumfang ist natürlich nicht 40 Mio. km sondern 40.000. Damit sind 30° 3333 km und 15° 1670 km. Sorry …</p>
    <p>Keine. Denn alle beruhen sie auf denselben Rohdaten und unterscheiden sich nur durch die Art der „Datenpflege“. Die Rohdaten, bzw. ihre nicht mehr verbesserabre Qualität sind das Problem. Und das Nichtwissen der „Klimaforscher“, bzw. ihre Ignoranz das vorhandene Wissen nicht anzuwenden, wie man mit fehlerbehafteten Zeitreihen umzugehen hat.</p>
    <p>M.W.n. sind nur noch die „korrigierten“ Daten verfügbar. Suchen Sie mal nach Ewert auf unserer Suchleiste, der hat das verfolgt. Oder hier: https://www.eike-klima-energie.eu/2019/06/28/adjustierte-unadjustierte-daten-nasa-nutzt-den-zauberstab-des-frisierens-und-erzeugt-erwaermung-dort-wo-es-nie-eine-gab/</p>
    <p>oder hier https://realclimatescience.com/wp-content/uploads/2018/06/NASA-US-1999-2016-2.gif</p>
    <p>Hier ist mehr dazu: https://realclimatescience.com/2019/02/61-of-noaa-ushcn-adjusted-temperature-data-is-now-fake/</p>
    <p>Ich bin selbst etwa 2012 auf Diskrepanzen in den Datenreihen gestoßen, als ich über ökologisch verträgliche(re) Anbaumethoden in der LW recherchierte (Strip- Farming z. B.), die die Dürre im Mittleren Westen der USA in den 1930- er Jahren hervorbrachte und bin in diesem Zusammenhang in einer Arbeit von Hansen 1989 auf ein Diagramm mit der *US- Main- Temp* gestoßen. Ich habe dann eine aktuelle Version dieser *US- Main- Temp* auf den GISS- Seiten gesucht und besonders bei den Temp genau dieser 1930-er jahre erhebliche Unterschiede gefunden (Temp. der Mitte der 30-er Jahre erniedrigt in der Neuversion).</p>
    <p>In der Folge stieß ich dann auf die Arbeiten von Friedrich Karl Ewert und erkannte das mögliche Ausmaß des Betruges, wohl ausgehend vom GISS, milder kann man es ja nicht bezeichnen.</p>
    <p>Was mich wundert ist die Stille in der Wissenschaftsgemeinde. Normalerweise müsste ein Aufschrei durch die Reihen gehen, wenn eine wichtige Datenbasis quasi unbrauchbar gemacht wurde….</p>
    <p>NOAA stellt sowohl unadjusted, als auch adjusted Daten zu Verfügung:</p>
    <p>Warum setzen Sie GISSTEMP und GHCN gleich? GISSTEMP verwendet GHCN4 adjusted als Input für die Analysen, arbeitet also nicht mit „raw data“. Aber das ist auch dokumentiert auf deren Seiten (z.B. in deren FAQ). Sie verweisen allerdings auf ISTI als die ultimative Quelle für die Rohdaten.</p>
    <p>Wenn Sie sagen, dass Gisstemp die GHCN- *adjustet* Daten zur Grundlage nimmt, sollten dann also die nachträglichen Korrekturen z. B. der Erniedrigung historischer Werte von den GHCN- Daten ausgehen und können keine Rohdaten sein…????</p>
    <p>Man kann bei GISStemp nachlesen, wann es welche Veränderung in den GISStemp-Versionen gab. Und wenn man Version 2 und Version 3 oder 4 miteinander vergleicht, dann muss ein Unterschied nicht daran liegen, dass die Roh-daten verfälscht wurden, sondern es kann sein, dass es andere Input Daten sind (Vorverarbeitete Daten statt Rohdaten; mehr Stationen; andere Stationen ringsherum die Einfluss auf sowas wie die paarweise Homogenisierung haben, …) oder das der Algorithmus geändert wurde, oder …</p>
    <p>Herr Limburg hat im Artikel den Artikel von Richard Muller zitiert. Ich würde Ihnen empfehlen, den mal komplett zu lesen und vielleicht auch mal die anderen Temperaturreihen, die es so gibt, in Ihre Überlegungen einzubeziehen.</p>
    <p>Also etwa so: Wenn eine Station in X seit 100 Jahren misst, dann vor 50 Jahren 2 Stationen in der Nähe hinzukommen und beständig ca. 1,5° weniger messen, geht man dann einfach davon aus, dass die Station in X auch die 50 Jahre zuvor 1,5° *zuviel* gemessen hat?</p>
    <p>Nun, da kann man mit etwas Kreativität alles mögliche veranstalten… Danke trotzdem.</p>
    <p>Das wird auch in Zukunft schwierig bleiben, denn das für die Originaldatensätze verantwortliche britische Hadley Center schrieb in einer mail (http://rogerpielkejr.blogspot.com/2009/08/we-lost-original-data.html ) an Prof. R. Pielke (12.8.09). „We are not in a position to supply data for a particular country not covered by the example agreements referred to earlier, as we have never had sufficient resources to keep track of the exact source of each individual monthly value. Since the 1980s, we have merged the data we have received into existing series or begun new ones, so it is impossible to say if all stations within a particular country or if all of an individual record should be freely available. Data storage availability in the 1980s meant that we were not able to keep the multiple sources for some sites, only the station series after adjustment for homogeneity issues. We, therefore, do not hold the original raw data but only the value-added (i.e. quality controlled and homogenized) data.“</p>
    <p>Mit anderen Worten, die Originaldatensätze sind verschwunden und somit der Forschung nicht mehr zugänglich</p>
    <p>… Hmmm, damit wird einiges klar. Aber irgendwie glaube ich nicht an Zufall. Andererseits könnte dadurch auch niemand der Behauptung entgegentreten, die jetzt verwendeten Temperaturdatensätze seien durchweg gefälscht und Klimamodelle, die historische Temperaturdaten als Eingangsparameter nutzen, seien allein dadurch unbrauchbar…</p>
    <p>Das die *Wissenschaftsgemeinde* soetwas nicht thematisiert, ist zumindest erstaunlich…</p>
    <p>Zumindest für den Bereich des DWD: die Monats(mittel)werte wurden (mindestens) bis in die 80er Jahre in gedruckter Form verteilt. Klarheit darüber und Infos dazu könnten doch einfach von den hier mitlesenden Meteorologen kommen.</p>
    <p>MfG</p>
    <p>Ketterer</p>
    <p>Vielleicht liegt das einfach daran, dass sich hier jemand irrt?</p>
    <p>Wenn das CRU die Roh-Daten nicht archiviert und die dann dort verloren gehen, sind die dann auch an der Stelle weg, an der die Daten erhoben wurden? Hat z.B. der deutsche Wetterdienst jetzt keine Daten mehr von vor 2000? Woher stammen die Daten, die BEST, GHCN, ISTI zur Verfügung stellen?</p>
    <p>Es wäre schön, wenn Sie nicht nur hypothetische Fragen stellen würden, sondern Quellen lieferten. Wir alle wären dankbar.</p>
    <p>Der ist in den Messdaten reichlich vorhanden, hat sich doch die Weltbevölkerung von rd. 1,5 Mrd. Menschen auf rund 3 Mrd. 1960 verdoppelt und liegt heute bei 7,8 Mrd. Menschen, wovon rd. 50 % in Städten leben. Diese wiederum begannen dank ihres Wachstums auch die ursprünglich außerhalb liegenden Messstationen zu umwachsen. Er liefert also einen kräftigen Teil der systematischen Fehler.</p>
    <p>Das war aber nicht das Thema dieses Berichtes, sondern die Frage, warum die historischen Daten für die Berechnung einer globalen Mitteltemperatur schlicht nicht taugen.</p>
    <p>Das ist das eigentliche Problem. Alle bisher vorgeschlagenen Korrekturmittel, z.B. Messung des Nachtlichts per Satellit, um die Ausbreitung der Städte zu bestimmen, leiden unter dem einen oder anderen Mangel, und lassen sich deshalb nicht wirklich als Korrekturmaß verwenden. Bei den Satelliten ist das anders, da die eine extrem viel bessere Flächenauflösung haben, mittelt sich der WI dort aus.</p>
    <p>Ist es tatsächlich so, dass Satellitenmessungen, die O2-Mikrowellenstrahlen messen, eine so hohe Flächenauflösung haben, dass sie WI-Muster klar erkennen und „ausmitteln“ können? So gesehen müsste man mit Satellitendaten „WI-bereinigte“ Ergebnisse bekommen können? Der Vergleich von Satellitendaten mit dem, was z.B. Herr Kowatsch ausführt, der sich auf Bodenmessungen bezieht und 2/3 des Temperaturanstiegs mutmaßlich den WI zuschreibt, wäre dann sehr interessant! Mein Eindruck ist, dass zumindest die mir bisher bekannten Satellitendaten – vielleicht dank Adjustierung – keine niedrigeren Wert zeigen.</p>
    <p>„Schaut man sich die RMSS-Satellitendaten an, dann gibt es im Südpolarbereich praktisch keine Temperaturanstiege“</p>
    <p>Bei</p>
    <p>http://images.remss.com/msu/msu_data_monthly.html</p>
    <p>habe für die Antarktis keine Satelliten-Daten gefunden.</p>
    <p>https://www.derklimarealist.de/</p>
    <p>Dort hat er seine meist unveröffentlichten Leserbriefe an die Augsburger Allgemeine veröffentlicht. Ich habe sie immer gerne gelesen. Er ist wahrscheinlich auch der Autor des 1970 erschienenen Buches „Die Temperaturmessungen in München, 1781-1968“. Seine letzter Beitrag behandelte die Horror-Hitze im Februar in der Antarktis: „Man hat wirklich selten so viel Unsinn gelesen, wie in diesem Artikel! Die – noch nicht einmal bestätigten – über 20 Grad wurden an einer…“. Die Skeptiker sterben leider alle aus.</p>
    <p>http://www.remss.com/measurements/upper-air-temperature/</p>
    <p>Und dort rechts im Bild die Region einstellen. Links im Bild kann man die verschiedenen Höhenbereiche wählen.</p>
    <p>„Ich bin da aber eher skeptisch“.</p>
    <p>Ich glaube ich habe die Ursache des anomalen Temperaturanstiegs der Wetter-Station Amundsen Scott in den Sommermonaten seit ca. 2000 gefunden. Hilfreich sind die monatlichen Wetterballon-Daten dieser Station (NOAA IGRA). Für den Monat Dezember im Zeitraum von 1989-2019 ergibt sich für den Boden ein positiver Trend von 0,6 +/- 0,4 °C/Dek, in 5,6 km Höhe (500 mbar) ein negativer Trend von -0,1 +/- 0,3 °C/Dek. In den Sommer-Monaten 9..2 beträgt der Trend 1989-2019 an der Oberfläche 0,4 +/- 0,2 °C/Dek und 0,1 +/- 0,2 °C/Dek in 5,6 km Höhe (500 mbar). Die Erwärmung ist vermutlich ein Wärme-Insel-Effekt der Wetter- und Forschungs-Station.</p>
    <p>Wie schon wiederholt erwähnt, die Station Sonnblick misst seit vielen Jahren unter vergleichbaren Bedingungen :</p>
    <p>http://www.zamg.ac.at/histalp/dataset/station/csv.php?c=AT&amp;s=_127</p>
    <p>Andere Stationen mit (cum grano salis)</p>
    <p>Konstanten Bedienungen</p>
    <p>Niwot Ridge LTER, die oberen Stationen</p>
    <p>Und sicherlich weitere Stationen des LTER Netzwerks.</p>
    <p>MfG</p>
    <p>Ketterer</p>
    <p>Die Sensoren werden heute mit einer standardisierten Grundtoleranz bis herunter auf Klasse AA von ±0,1 K bei 0°C (oder noch besser…) angeboten, die die für sich durchaus auch bringen, aber mit der „Konfektionierung“ gehen die eigentlichen Probleme los, da wissen die allermeisten Anwender wirklich nicht, was sie tun. Neben den Fragen der Ankopplung an das Meßobjekt gibt es auch noch die Problematik mit den parasitären Thermospannungen (durch vielfältige Materialkombinationen und Temperaturgradienten im Messkreis), die man nur mit einem gewissen Aufwand eliminieren kann, da nützt der höchstauflösende AD-Wandler nix, da muß man sich was einfallen lassen.</p>
    <p>Wenn ich also in Meteorologie / Klimagedöns Temperaturangaben mit zwei Stellen nach dem Komma sehe, da stehen auch mir die Haare zu Berge, die „Verursacher“ solcher Angaben können nur messtechnische Volltrotttel sein ….</p>
    <p>Es gab noch eine weitere, sogar durch ein peer review gelaufene Veröffentlichung:</p>
    <p>„Analysis of the impacts of station exposure on the U.S. Historical Climatology Network temperatures and temperature trends“</p>
    <p>Souleymane Fall, Anthony Watts, John Nielsen‐Gammon, Evan Jones, Dev Niyogi, John R. Christy, Roger A. Pielke Sr.</p>
    <p>https://doi.org/10.1029/2010JD015146</p>
    <p>Dort wird auch diskutiert, welche Auswirkungen die Stationsklassifizierung auf den Trend von Minimal-, Maximal- und Durschnittstemperatur hat.</p>
    <p>Das setzt allerdings ein Wissenschaftsverständnis und eine Ethik bei den Akteuren voraus, die erkennbar nicht (mehr) vorhanden sind – vom Publikum und den Staatsmedien ganz zu schweigen.</p>
    <p>Ungeachtet dessen: sehr verdienstvoll, uns an Ihrer Expertise teilnehmen zu lassen, Herr Limburg!</p>
    <p>Rainer Facius</p>
    <p>Aber das interessiert die sog. „Klimaforscher“ nicht. Die nehmen das, was für ihre Vorstellungen paßt und nicht das, was man nehmen muß, was also korrekt ist.</p>
  </comments>
</doc>