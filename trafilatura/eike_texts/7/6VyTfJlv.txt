Im Kommentarbereich meines jüngsten Beitrages zu den GAST (Global Average Surface Temperature)-Anomalien (und warum es ein Verfahren ist, mit dem die Klimawissenschaft sich selbst austrickst) kam (erneut) zur Sprache, dass das, was die Klimawissenschaft als die Tägliche Average Temperatur von irgendeiner Wetterstation angibt, nicht das ist, was wir denken, nämlich das Average der Temperaturen über den Tag (indem alle gemessenen Temperaturen addiert werden und die Summe durch die Anzahl der Werte dividiert wird). Stattdessen werden einfach die tägliche Maximum- bzw. Minimum-Temperatur addiert und durch zwei dividiert. Das kann man übersetzt in die Sprache der Mathematik so schreiben: (Tmax + Tmin)/2.
Jeder, der hinsichtlich der verschiedenen Formen von Averages versiert ist, wird erkennen, dass Letzteres in Wirklichkeit der Median von Tmax und Tmin ist – also die Mitte zwischen beiden Werten. Dies ist offensichtlich auch gleich dem Mean der beiden Werte – aber da wir nur ein tägliches Maximum bzw. Minimum für eine Aufzeichnung heranziehen, in welcher heutzutage viele Messungen einen Tagessatz bilden, finden wir einen Median, wenn wir alle Messungen nach ihrer Magnitude angleichen und den Mittelpunkt zwischen dem höchsten und dem tiefsten Wert finden (allerdings machen wir das unter Ignoranz aller anderen Messungen zusammen, und wir kommen auf den Median eines Datensatzes aus zwei Werten, der nur aus Tmax und Tmin besteht).
Das ist sicher kein Geheimnis und das Ergebnis der historischen Tatsache, dass Temperaturaufzeichnungen in irgendeiner entfernten Vergangenheit vor Einführung automatisierter Wetterstationen auf diese Weise erstellt wurden – beispielsweise mit einem Max-Min-Thermometer wie diesem hier:
Jeden Tag zu einer festgesetzten Zeit würde ein Beobachter zum Stevenson screen* gehen, ihn öffnen und darin befindliche Thermometer ablesen. Man würde die durch die Marker angezeigten Maximum- und Minimum-Temperaturen ablesen und dann den Reset-Schalter drücken (in der Mitte), um die Messung neu laufen zu lassen.
[*Einschub des Übersetzers: Mit ,Stevenson Screen‘ bezeichnen die Amerikaner eine simple Wetterhütte wie die unten Abgebildete, welche im DWD bis vor einigen Jahren in Gebrauch waren. Darin befinden sich links ein Thermo-/Hygrograph, rechts quer liegend ein Maximum- und ein Minimum-Thermometer, darunter noch ein weiteres Minimum-Thermometer, das abends vor der Hütte über dem Boden abgelegt wird, um das nächtliche Minimum in Bodennähe zu ermitteln. Dahinter befinden sich senkrecht zwei Thermometer, links zur Messung der Feuchttemperatur {bitte googeln, falls nicht bekannt}, rechts zur Messung der aktuellen Temperatur.
Es entzieht sich der Kenntnis des Übersetzers, ob der im Original vorgestellte, verlinkte ,Stevenson Screen‘, welcher ja nur mit Thermometern der Art aus Abbildung 1 bestückt ist, exemplarisch für alle Stationen in den USA oder ein hier vorgestellter Sonderfall ist.
Ende Einschub]
Mit lediglich diesen beiden aufgezeichneten Messwerten lässt sich nur die tägliche Average-Temperatur berechnen, die dem Median der beiden am nächsten liegt. Um heutige Temperaturwerte mit solchen aus der Vergangenheit vergleichen zu können, war es erforderlich, das gleiche Verfahren anzuwenden, um das tägliche Average von heutigen Temperaturen zu erhalten, obwohl von automatisierten Messpunkten alle sechs Minuten eine neue Messung vorliegt.
Nick Stokes schrieb einen Beitrag über die Verwendung von Min-Max-Thermometern nebst deren Problemen in Relation zu Adjustierungen bzgl. der Beobachtungszeit. In jenem Beitrag schreibt er:
Ab und zu erscheint ein Beitrag wie dieser, in welchem jemand entdeckt, dass die Messung der täglichen Temperatur mittels (Tmax + Tmin)/2 nicht genau dem entspricht, was man mit einer Integration der Temperatur über die Zeit erhält. Na und? Beides sind einfach Messungen, und man kann damit Trends abschätzen.
Und da hat er absolut recht – man kann jede Zeitreihe von irgendwas heranziehen, alle Arten von Averages finden – Means, Mediane, Modes – und deren Trends auffinden über verschiedene Zeiträume.
[Einschub des Übersetzers: Am Meteorologischen Institut der Freien Universität Berlin wurde seit seiner Gründung durch Prof. Richard Scherhag im Jahre 1953 ein Klimamittel errechnet, aber nicht nach der hier beschriebenen, vorsintflutlich anmutenden Methode. Es wurden jeweils täglich die Temperaturwerte von 7 Uhr, 14 Uhr und 2 x 21 Uhr (MEZ; MESZ 1 Stunde später) gemessen (das sind die sog. ,Mannheimer Stunden‘) und durch 4 dividiert. Vermutlich wurde dieses Verfahren auch beim DWD so durchgeführt. Wie es in anderen europäischen Ländern gehandhabt wird, ist nicht bekannt, zumindest in einigen Ländern ist es aber wohl das hier von Kip Hansen beschriebene Verfahren.
Seit den achtziger Jahren werden am Institut auch stündliche Temperaturwerte ermittelt und durch 24 dividiert. Seit Jahr und Tag werden die so gewonnen Mittelwerte mit dem der Mannheimer Stunden verglichen. Es ergeben sich Differenzen bis 1°C. Da wir es aber hier mit Zehntel oder sogar Hundertstel Grad (!) zu tun haben, ist das natürlich erheblich.
Weiteres hierzu kann man über die Website des Instituts erfahren.
Ende Einschub]
In diesem Falle müssen wir die Frage stellen: Was ermitteln sie eigentlich? Ich musste immer wieder diesen Beitrag heranziehen, wenn ich etwas über aktuelle wissenschaftliche Forschung schrieb, welche irgendwie einen bedeutenden ,roten Faden‘ wahrer Wissenschaft verloren zu haben scheint – dass wir extreme Vorsicht walten lassen müssen bei der Definition dessen, was wir erforschen – welche Messungen von welcher Eigenschaft welcher physikalischen Größe uns das zeigen wird, was wir wissen möchten.
Stoke schreibt, dass jedwede Daten von Messungen irgendwelcher Temperatur-Averages offenbar genauso gut sind wie jede andere – dass der Median von (Tmax+Tmin)/2 genauso brauchbar für die Klimawissenschaft als wahres Average ist wie häufigere Temperaturmessungen wie etwa heutige Messungen alle sechs Minuten.
Damit haben wir mindestens drei Frage aufgeworfen:
1. Was versuchen wir mit Temperaturaufzeichnungen zu messen? Wenn wir die Berechnungen monatlicher und jährlicher Means nebst deren Trends sowie den Trends von deren Anomalien berechnen – was werden wir daraus erkennen? (Mit Anomalien sind hier immer Anomalien von irgendwelchen klimatischen Means gemeint).
2. Was wird mit (Tmax+Tmin)/2 wirklich gemessen? Ist es quantitativ unterschiedlich zur Mittelung [averaging] von alle sechs Minuten (oder stündlich) gemessenen Temperaturen? Sind die beiden qualitativ unterschiedlich?
3. Erfüllt das derzeit gebräuchliche (Tmax+Tmin)/2-Verfahren alles, um Frage Nr. 1 beantworten zu können?
Ich werde diese Fragen im Folgenden beantworten. Jeder kann aber in Kommentaren seine eigenen Antworten kundtun.
Was versuchen wir zu messen?
Die Antworten auf Frage 1 sind davon abhängig, wer man ist und in welchem Bereich der Wissenschaft man arbeitet.
Meteorologen messen die Temperatur, weil es in ihrem Bereich ein Schlüsselparameter ist. Ihre Aufgabe ist es, Temperaturen der Vergangenheit zu kennen und diese dazu heranzuziehen, um zukünftige Temperaturen im Kurzfrist-Zeitraum vorherzusagen – Höchst- und Tiefstwerte für morgen, Wochenend-Wetter und Jahreszeiten-Prognosen für die Landwirtschaft. Vorhersagen von Extremtemperaturen sind ein wichtiger Bestandteil ihrer Aufgabe – und „vor Wetterereignissen zu warnen, die zu einer Gefahr für die öffentliche Sicherheit und Ordnung führen können“*
[*Das ist keine Übersetzung, sondern ein Original-Auszug aus dem deutschen Wetterdienst-Gesetz. Anm. d. Übers.]
Klimatologen befassen sich mit langzeitlichen Averages sich ständig verändernder Wetterbedingungen für Gebiete, Kontinente und den Planeten als Ganzes. Mit diesen langfristigen Averages wollen sie die verschiedenen Gebiete der Welt in die 21 Zonen der Klima-Klassifikation nach Köppen einteilen und nach Änderungen in diesen Regionen Ausschau halten. [Statt des Links lieber nach „Köppen Klimazonen“ googeln. Anm. d. Übers.] In Wikipdedia findet sich eine Erklärung, warum dieser Studienbereich schwierig ist:
Klimaforschung ist schwierig wegen der Größenordnung der Gebiete, der langen Zeiträume und der komplexen Prozesse, welche das Klima steuern. Klima wird gesteuert durch physikalische Gesetze, die man als Differentialgleichungen darstellen kann. Diese Gleichungen sind gekoppelt und nichtlinear, so dass angenäherte Lösungen nur erhalten werden können mittels numerischer Verfahren, um globale Klimamodelle zu konstruieren. Klima wird manchmal modelliert als ein stochastischer (Zufalls-)Prozess, aber es ist allgemein als eine Approximation von Prozessen akzeptiert, welche anderenfalls zu kompliziert sind, um analysiert werden zu können. (Hervorhebung von mir {dem Autor}).
Die Temperaturen der Ozeane und die verschiedenen Schichten der Atmosphäre sowie die Unterschiede zwischen Regionen und atmosphärischen Schichten sind zusammen mit einer langen Liste anderer Faktoren Treiber des Wetters, und die langfristigen Unterschiede der Temperatur folglich von Interesse für die Klimatologie. Der momentane Gleichgewichtszustand des Planeten hinsichtlich Ein- und Ausstrahlung ist gegenwärtig einer der Brennpunkte in der Klimatologie, und Temperaturen sind ein Teil davon.
Wissenschaftler der anthropogenen globalen Erwärmung (IPCC-Wissenschaftler) befassen sich mit der Beweisfindung, dass anthropogenes CO2 dazu führt, dass das Klimasystem der Erde zunehmend einfallende Solarstrahlung zurückhält, und berechnen globale Temperaturen allein aufgrund dieser Objektive. Folglich konzentrieren sich die AGW-Wissenschaftler auf regionale und globale Temperaturtrends sowie die Trends der Temperatur-Anomalien und anderer klimatischer Faktoren, die ihre Position stützen könnten.
Welche Aussagen erhoffen wir uns von Berechnungen monatlicher und jährlicher Means nebst deren Trends?
Meteorologen sind an Temperaturänderungen bzgl. ihrer Prognosen interessiert und verwenden „Means“ von Temperaturen der Vergangenheit, um eine zu erwartende Bandbreite der Temperatur-Variabilität zu erhalten und vorherzusagen, wenn es Temperatursprünge nach außerhalb dieser Bandbreite gibt. Temperaturunterschiede zwischen Örtlichkeiten und Regionen treiben das Wetter, was diese Aufzeichnungen für ihre Arbeit bedeutsam macht.
Klimatologen wollen wissen, wie sich das längerfristige Bild ändert – Erwärmt es sich in dieser Region wirklich, kühlt sie ab, gibt es immer mehr/weniger Regen? Und das alles über Zeiträume von Jahrzehnten oder auch 30 Jahren. Dafür brauchen sie Trends. (Anmerkung: Nicht einfach dümmliche, selbst generierte ,Trendlinien‘ in Graphiken, die von ihren Beginn- und Endpunkten abhängen – sie wollen reale Änderungen der Bedingungen mit der Zeit erkunden).
AGW-Wissenschaftler müssen in der Lage sein zu zeigen, dass sich die Erde erwärmt und ziehen Temperaturtrends heran – regional und global, als Absolutwerte und Anomalien – in dem Bemühen, die AGW-Hypothese zu beweisen, der zufolge das Klimasystem der Erde mehr Energie von der Sonne zurückhält infolge zunehmender CO2-Konzentrationen.
Was wird mit (Tmax+Tmin)/2 wirklich gemessen?
(Tmax+Tmin)/2, die tägliche Tmittel [Tavg] der Meteorologen, ist das Median der täglichen Höchst- und Tiefsttemperatur (man klicke auf den Link, falls man nicht so genau weiß, warum das der Median und nicht das Mean ist). Die monatliche Tavg ist tatsächlich das Median des Monatlichen Means täglicher Höchsttemperaturen und des monatlichen Means der Täglichen Tiefsttemperatur. Die Monatliche Tavg ist der grundlegende Eingangswert für alle daraus abgeleiteten Berechnungen regional, staatenweit, national, kontinental und global der Average-Temperatur (in 2 m Höhe auf dem Festland). Mit dieser Berechnung will man den Median der Means von Tmax und Tmin finden für den Monat an der Station, indem man all die täglichen Tmax und Tmins jeweils addiert und ihr Mean (arithmetisches Average) berechnet, um dann den Median jener zwei Werte zu finden. (Dies so zu definieren war nicht einfach – ich musste zu den Original-GHCN-Aufzeichnungen gehen und mich per E-Mail an den Customer Support von NCEI wenden, um Klarheit zu bekommen).
Wenn wir jetzt also wissen, wie die Zahl zustande gekommen ist, die wir die monatliche Tavg nennen, können wir jetzt einmal betrachten, was damit eigentlich gemessen wird.
Ist es eine Maßzahl für das Average der Temperatur für den Monat? Eindeutig nicht. Dieses würde man berechnen mittels Addition von Tavg für jeden Tag und Division der Summe durch die Anzahl der Tage des jeweiligen Monats. Dieses Verfahren könnte einen Wert ergeben, der überraschend nahe der aufgezeichneten monatlichen Tavg liegt – unglücklicherweise haben wir aber schon gesagt, dass die täglichen Tavgs nicht die Average-Temperaturen für ihre jeweiligen Tage sind, sondern die Mediane der täglichen Höchst- und Tiefsttemperatur.
Das in diesem Beitrag behandelte Problem illustriert das Bild ganz oben. Hier folgt es vergrößert:
Diese Abbildung stammt aus einem Beitrag, in welchem Means und Medians definiert werden. Man erkennt, dass falls die violetten Einzelschritte für die Temperatur über einen Tag stehen, wäre der Median für stark unterschiedliche Temperaturprofile identisch, aber das wahre Average, das Mean wäre sehr unterschiedlich. (Man beachte: das rechte Ende der Graphik ist abgeschnitten, aber beide Einzelwerte enden am gleichen Punkt rechts – das Äquivalent einer Höchsttemperatur für den Tag). Falls das Profil dicht an einer „Normalverteilung“ liegt, liegen Median und Mean dicht beieinander – falls nicht, sind beide ziemlich unterschiedlich.
Ist es quantitativ unterschiedlich zum Averaging der Temperatur des Tages alle sechs Minuten (oder stündlich)? Sind die beiden qualitativ unterschiedlich?
Um eine Antwort zu finden, müssen wir zu den täglichen Tavgs zurückkehren. Wodurch ändert sich die Tägliche Tavg? Jedwede Änderung der täglichen Höchst- oder Tiefsttemperatur, falls die tägliche Tavg 72 beträgt, können wir dann die Höchst- und Tiefsttemperatur daraus ablesen? Nein, können wir nicht. Die Tavg des Tages sagt uns wenig über diese beiden Tagesextrema. Auch erfahren wir durch die Tavg nicht viel darüber, wie sich die die Temperatur im Tagesverlauf entwickelt und geändert hat.
Tmax 73, Tmin 71 = Tavg 72
Tmax 93, Tmin 51 = Tavg 72
Tmax 103, Tmin 41= Tavg 72*
[*Das sind natürlich Grad Fahrenheit. Da es um das Prinzip und nicht um die Werte als solche geht, spare ich mir die Umwandlung. Anm. D.Übers.]
Der erste Tag wäre ein milder Tag und eine sehr warme Nacht, der zweite ein heißer Tag und eine Nacht mit normaler Temperatur. Dabei könnte es ein wolkiger warmer Tag gewesen sein, wobei eine Stunde purer Sonnenschein die Temperatur auf einen Momentanwert von 93 hätte steigen lassen oder auch ein durchweg sonniger Tag mit Erwärmung auf 93 Grad bis 11 Uhr und Verharren der Temperatur über 90 bis Sonnenuntergang mit einem nur sehr kurzen Zeitraum am frühen Morgen, in welchem es 51 Grad kühl geworden ist.
Unser drittes Beispiel, wie es für die Hochwüsten im Südwesten der USA typisch ist, zeigt einen sehr heißen Tag mit einer nachfolgenden kalten Nacht. Ich selbst habe Tage mit Temperaturwerten über 90°F [34°C] erlebt mit Frost während der folgenden Nacht. (Tmax+Tmin)/2 ergibt nur den Median zwischen zwei Temperatur-Extrema, von denen jedes Einzelne mehrere Stunden oder auch nur ein paar Minuten erreicht haben kann.
Die tägliche Tavg, der Median von Tmax und Tmin, sagt uns nichts über den „Wärmegehalt“ oder das Temperaturprofil des Tages. Wenn das so ist, weist die monatliche Tavg die gleiche Schwäche auf – ist sie doch der Median der Means der Tmaxs und Tmins.
Vielleicht hilft eine Graphik bei der Veranschaulichung des Problems:
Diese Graphik zeigt die Differenz zwischen der täglichen Tavg (mittels des (Tmax+Tmin)/2-Verfahrens) einerseits sowie dem wahren Mean der täglichen Temperatur Tmean andererseits. Man erkennt Tage mit einer Differenz von drei oder mehr Grad mit einem Augenschein-Average von einem Grad oder so, mit ziemlich vielen Tagen mit einer Differenz von ein oder zwei Grad. Man kann eine ähnliche Graphik für die monatliche TAVG und den wahren monatlichen Means plotten, entweder von tatsächlichen täglichen Means oder durch averaging aller Temperaturaufzeichnungen für den jeweiligen Monat.
Die gegenwärtig gebräuchlichen Tavg und TAVG (täglich und monatlich) sind nicht das Gleiche wie tatsächliche Means der Temperaturen über den Tag oder den Monat. Sie sind beide quantitativ und qualitativ unterschiedlich – und sagen uns unterschiedliche Dinge.
Also: JA, die Daten sind qualitativ und quantitativ unterschiedlich.
Erfüllt das derzeit gebräuchliche (Tmax+Tmin)/2-Verfahren die Zwecke jedweder Antwort auf Frage 1?
Betrachten wir dazu mal wieder die Forschungsbereiche:
Meteorologen messen Temperaturen, weil es einer der zentralen Bereiche ihrer Arbeit ist. Die Wetterleute sind zufrieden mit Temperaturmessungen nahe des nächsten ganzen Grads. Ein Grad mehr oder weniger war nicht groß von Belang (außer bei Temperaturwerten um den Gefrierpunkt). Average Wetter kann auch eine Unsicherheit von ein oder zwei Grad aushalten. Also denke ich, dass das Verfahren (Tmax+Tmin)/2 für die Wetterleute ausreichend ist – auch hinsichtlich des Wetters und der Wettervorhersage. Bzgl. Wetter weiß der Meteorologe, dass die Temperatur auf natürliche Weise um ein oder zwei Grad variiert.
Klimatologen befassen sich mit den langzeitlichen, sich immer ändernden Wetterbedingungen in Regionen, auf Kontinenten und den Planeten als Ganzes. Klimatologen wissen, dass Wetterbeobachtungen der Vergangenheit nicht gerade präzise waren – sie akzeptieren, dass (Tmax+Tmin)/2 keine Maßzahl der Energie im Klimasystem ist, sondern sie damit nur einen Hinweis von Temperaturen an einer Station, in einem Gebiet oder auf kontinentaler Basis bekommen, ausreichend um sich ändernde Klimate zu beurteilen – ein Grad mehr oder weniger der average Sommer- bzw. Wintertemperatur für eine Region ist möglicherweise keine klimatologisch bedeutende Änderung – es ist einfach jährliches oder vieljähriges Wetter. Die meisten Klimatologen wissen, dass nur Temperaturmessungen aus jüngster Zeit auf ein oder zwei Grad präzise sind.
AGW-Wissenschaftler (IPCC-Wissenschaftler) befassen sich damit zu beweisen, dass das Klimasystem der Erde durch menschliche CO2-Emissionen Energie von der Sonne zurückhält. Und hier kommen die Differenzen in den quantitativen und qualitativen Werten ins Spiel zwischen (Tmax+Tmin)/2 bzw. einer wahren täglichen/monatlichen Mean-Temperatur.
Es gibt Leute, die (korrekt) argumentieren, dass Temperatur-Averages (mit Sicherheit die GAST genannte Größe) keine genauen Indikatoren der Energie-Rückhaltung im Klimasystem sind. Aber bevor wir uns dieser Frage annehmen können, müssen wir korrekte quantitative und qualitative Messwerte der Temperatur haben, welche die sich ändernde Wärmeenergie an Wetterstationen reflektiert. Aus (Tmax+Tmin)/2 geht nicht hervor, ob wir einen heißen Tag und eine kühle Nacht hatten oder einen kühlen Tag und eine relativ warme Nacht. Temperatur ist eine intensive Eigenschaft (in diesem Fall von Luft und Wasser) und nicht wirklich geeignet für Addition, Subtraktion und Averaging im normalen Sinne – die Temperatur einer Luft-Stichprobe (wie etwa in einer automatischen Wetterstation ASOS) – steht in Beziehung zu, ist aber nicht das Gleiche wie die Energie (E) in der Luft an jener Stelle und steht in Relation zu, ist aber nicht das Gleiche wie die Energie im lokalen Klimasystem. Verwendet man (Tmax+Tmin)/2 bzw. TMAX und TMIN, um eine monatliche TAVG zu erhalten, reflektiert dies nicht einmal genau, wie die Temperaturen waren und kann und wird uns daher nicht (akkurat und präzise) informieren über die Energie im lokal gemessenen Klimasystem. Daher kann sie uns auch nicht, wenn man sie über Regionen und Kontinente kombiniert, (akkurat und präzise) über die Energie im regionalen, kontinentalen oder globalen Klimasystem informieren – nicht quantitativ in Absolutwerten und nicht in Gestalt von Änderungen, Trends oder Anomalie-Trends.
In der AGW-Wissenschaft geht es um das Zurückhalten von Energie im Klimasystem, und die gegenwärtig verwendeten mathematischen Verfahren – bis hinab zum Niveau des täglichen Averages – sind trotz der Tatsache, dass dies alles ist, was wir in der Historie klimahistorischer Aufzeichnungen haben, nicht geeignet, um eine sich eventuell ändernde Energie-Zurückhaltung zu berechnen bis zu irgendeinem Grad quantitativer oder qualitativer Akkuratesse oder Präzision.
Für die Wetterprognostiker mag es ausreichen, diese trügerische Größe als „ausreichend für die Wettervorhersage“ anzusehen. Die Vorhersage von Hurrikanen kann sehr gut mit Temperaturwerten innerhalb einer Genauigkeit von ein oder zwei Grad vorhergesagt werden – solange alles miteinander vergleichbar ist.
Selbst Klimawissenschaftler – jene, die nicht an den Klima-Kriegen interessiert sind – können gut mit Temperaturwerten plus/minus 1 Grad oder so leben – gibt es doch eine Fülle anderer Faktoren, von denen viele wichtiger sind als eine „Average Temperatur“ und die in Kombination das Klima jedweder Region ausmachen.
Nur AGW-Aktivisten bestehen darauf, dass die minimalen Änderungen, die sich aus langzeitlichen Klima-Aufzeichnungen der falschen Parameter ergeben, wahrlich signifikant sind für das Weltklima.
Unter dem Strich:
Die gegenwärtig angewandten Verfahren zur Berechnung sowohl der globalen Temperatur als auch globaler Temperatur-Anomalien stützen sich aus historischen Gründen auf einen Parameter, der in vielfacher Weise ungeeignet ist, um mit Präzision zu bestimmen, ob sich das Klimasystem der Erde infolge Zurückhalten von Sonnenenergie erwärmt oder nicht. Er ist ungeeignet zur Bestimmung der Größenordnung einer solchen eventuellen Änderung und womöglich nicht einmal dazu geeignet, überhaupt das Signal für eine solche Änderung auszumachen. Das gegenwärtige Verfahren misst nicht in angemessener Weise eine physikalische Eigenschaft, welche eine solche Berechnung gestatten würde.
Schlussbemerkungen des Autors:
Das diesem Beitrag zugrunde liegende Thema ist viel einfacher als es aussieht. Die Messungen zur Erstellung einer GAST (Anomalie) und GAST (absolut) – besonders (Tmax+Tmin)/2, egal ob täglich oder monatlich – sind nicht geeignet für eine Berechnung jener globalen Parameter in der Art, wie sie uns von den AGW-Aktivisten-Wissenschaftlern präsentiert werden. Sie werden oftmals herangezogen zu zeigen, dass das Klimasystem mehr Energie zurückhält und sich folglich erwärmt … aber die kleinen Änderungen dieser unpassenden Größe über klimatologisch signifikante Zeiträume können uns darüber nichts sagen, messen sie doch in Wirklichkeit nicht die average Temperatur, nicht einmal die an einer einzelnen Wetterstation. Die zusätzliche Unsicherheit dieses Faktors lässt die Gesamt-Unsicherheit der GAST nebst deren Anomalien bis zu einem Punkt unsicher werden, an dem sie über die gesamte Zunahme seit Mitte des 20. Jahrhunderts hinausgeht. Diese Unsicherheit wird nicht eliminiert durch wiederholtes Glätten und Averaging entweder von absolute Werten oder deren Anomalien.
Ich empfehle der Leserschaft dringend, die immer präsente Hypothese zurückzuweisen, die da lautet „falls wir immer weiter fortfahren, Averages zu mitteln [averaging averages] wird die Variation früher oder später so gering werden, dass sie keine Rolle mehr spielt – egal ob es Fehler, Unsicherheiten oder einfach schlechte Daten sind“. Das führt zu wissenschaftlicher Verrücktheit.
Die Argumentation wäre anders, falls wir tatsächlich genaue und präzise Temperatur-Averages von Wetterstationen hätten. Viele würden immer noch nicht dem Argument zustimmen, dass die Temperaturaufzeichnung allein eine Änderung der Zurückhaltung von Sonnenenergie anzeigen kann. Die in das System eintretende Energie wird nicht wie von Zauberhand automatisch in der Luft 2 Meter über Grund oder auch an der Oberfläche der Ozeane in fühlbare Wärme umgewandelt. Änderungen der fühlbaren Wärme, gemessen in 2 m Höhe und an der Oberfläche der Ozeane, reagieren nicht notwendigerweise sofort auf eine Zu- oder Abnahme zurückgehaltener Energie im Klimasystem der Erde.
Es wird Einwände gegen die Schlussfolgerungen dieses Beitrags geben – aber die Fakten sind so, wie sie sind. Einige werden diese Fakten unterschiedlich interpretieren, verschiedenen Werten verschieden starke Wichtigkeiten zuordnen und unterschiedliche Schlussfolgerungen ziehen. Aber genau das ist Wissenschaft.
Link: https://wattsupwiththat.com/2018/10/02/daily-averages-not-so-fast/
http://report.ipcc.ch/sr15/pdf/sr15_spm_fig1.pdf
zeigt.
Es gibt keinen Treibhauseffekt!
Da haben wohl die Klimaforscher hier in Deutschland so was falsch verstanden, was!
Denn es soll sich ein stationärer Zustand für die Temperatur, also bei etwa „net zero“ bei 3000 Gt CO2 pro Jahr einstellen. Was ja einem einfachen Wärmetransport – nämlich ins Weltall – von einer Wärmequelle aus entspricht. Nix anderes zeigt die Grafik. Und die Wärmequelle ist die Atmosphäre, die ja von der Sonne und von uns ein wenig geheizt wird.
Wen stören denn da 0,021% an CO2 das wir mit 42 Gt je Jahr zusätzlich in die Atmosphäre pusten, viel mehr Auswirkungen auf das Wetter hat der Wasserdampf als Wärmeträger und seiner Volumenänderungsarbeit, den wir in die Atmosphäre so eintragen.
Also frohes Heizen!
Wenn Klimaforscher über eine globale Erwärmung sprechen, reden sie aber immer von Temperaturen, nicht von Energien. Und dann bestimmen sie die Temperaturen über einen täglichen Maximal- und einen Minimalwert, der Rest dazwischen ist irrelevant. Kein vernünftiger Mensch käme auf die Idee, den elektrischen Energieverbrauch eines Haushalts für jeden Tag über einen Maximal- und einen Minimalwert z. B. des Verbraucherstroms zu bestimmen. Daraus kann nur etwas kommen, was mit dem Tages-Gesamtverbrauch rein gar nichts zu tun hat.
Wenn von einer globalen Erwärmung (Erwärmung => Erhöhung des energetischen Zustands) gesprochen wird, muss erst einmal festgelegt werden, wie man richtig die Energie aller Räume der Atmosphäre unseres Globus lückenlos und vor allen Dingen richtig vermisst. Dabei müssen die vermessenen Räume eine solche Größe haben, dass darin die jeweilige Temperatur als gleich verteilt angenommen werden kann. Um daraus ein aussagefähiges Temperaturmittel zu bestimmen, müssen die betrachteten Räume untereinander entsprechend dem durch sie definierten Volumen und auch entsprechend der vom jeweiligen Luftdruck abhängigen Masse gewichtet werden. Natürlich kann man sich darauf beschränken, nur Räume mit einer begrenzten Höhe über dem Boden zu vermessen und das auch nur noch mit einer maximalen Höhe über dem Meeresspiegel. Immer jedoch, wenn Begrunzungen wirken, ist ein Teil der Wirklichkeit ausgeschlossen worden und verloren gegangen und genau dies eröffnet die Möglichkeit für die wildesten Spekulationen und Untergangsszenarien.
Trends aus der bisherigen Messpraxis von Temperaturen abzuleiten, ist wenig sinnvoll, da die Messorte sich in der Regel nicht in einem stationären Umfeld befinden. Das betrifft insbesondere Messstationen in der urbanen Umgebung. Die Folge ist, dass die dadurch erzeugten Ungenauigkeiten größer oder mindestens gleich sind zu dem, was als Ziel zur Erhaltung der Welt -wie auch immer begründet- gesetzt worden ist.
In den USA, China, Indien, Australien, Frankreich, Japan u. a. m. gibt es ein derartiges Problem bekanntlich nicht!
schauen Sie sich mal in Ruhe die Grafik
http://report.ipcc.ch/sr15/pdf/sr15_spm_fig1.pdf
und die Erklärungen dazu an.
Wie man sehen kann stagnieren die Temperaturen trotz Zunahme des CO2–Gehaltes.
Eine bessere Darstellung, dass es den Treibhauseffekt nicht gibt, gibt es nicht.
Es zeigt eine ganz normale Erwärmung, wie sie in Wärme leitenden Materialien bei Transport von Wärme vorkommt. Von der Leistung abhängig stagniert die Temperatur bei Wärmedurchgang. Und es geht ja nunmal Wärme ausgehend von einer Temperatur vom „Erdboden“, also durch Wasserdampf in die Atmosphäre. Natürlich auch geringstfügig vom CO2 aus den 15 Mikrometer banden. Welche dann – wie sie ja so schön schreiben – in den Weltraum transportiert wird.
Gut, dass die Koreaner nun das Zepter beim IPCC schwingen was!
warum machen Sie es den CO2 Klimaschwindlern so leicht, Ihre vermeintliche Kritik zu Recht in den Mülleimer zu werfen?
Dass konzentrationsabhängige Selbstabschirmung auch die Emission der mehr als zweiatomigen Gase in eine Sättigung führt, ist physikalisches Grundwissen.
Gleiches gilt inzwischen für die energetische Rückkopplung aus den von der Materie an der Erdoberfläche ausgehenden Energieflüssen, gewöhnlich atmosphärischer Treibhauseffekt genannt.
In einer Reihe von Artikeln hatte ich begründet, warum CO2 trotz seiner Emission von Wärmestrahlung keinen Einfluss auf diese Wirkung haben kann. Der link führt Sie direkt zu der Möglichkeit, darin Fehler aufzudecken:
https://de.scribd.com/document/372778420/Klimasensitivita-t-des-CO2-eine-Seifenblase
jedenfalls habe ich mal eine Betrachtung zum Energiehaushalt(Enthalpie der Luft) in Abhängigkeit von Druck und Temperatur für die Atmosphäre durchgeführt und kam bei einer Höhe der Atmosphäre von 10 km auf eine Temperaturerhöhung von 0,5K innerhalb von 11 Jahren aus Abwärme aus Abgasen und KKW Kühlung. Jedenfalls für ein geschlossen adiabates System. Bei den Mengen an Energie, die derzeit in die Luft geblasen werden, sind also Erwärmungen im 1°C Temperaturbereich über 30 Jahre mal so als Hausnummer zu erwarten, wenn man bedenkt, dass die Atmosphäre ja hauptsächlich aus inertem Stickstoff besteht und sich daher bei guter Durchmischung langsam halt durch Eintrag von Energie erwärmt.
Sehr geehrter Herr Holger Neulen,
ich möchte an dieser Stelle noch einmal in Erinnerung rufen,welche Energieformen es gibt.
Wenn im physikalischen Sprachgebrauch von Wärme die Rede ist,handelt es sich um thermisch ausgetauschte Energie mit dem Formelzeichen [Q] in Watt
Wenn im selben Zusammenhang von Arbeit gesprochen wird,geht es um mechanisch ausgetauschte Energie mit dem Formelzeichen [W] in Watt
Wenn im selben Zusammenhang von innerer Energie gesprochen wird,geht es um die im (ruhenden)System gespeicherte Energie mit dem Formelzeichen[U] in Watt
Damit solche Systeme sinnvoll beschrieben werden können,muss eine Energiebilanz vorliegen.Ansonsten betreibt man Kaffeesatz lesen auf pseudo-naturwissenschaftlichen Niveau,wir wollen aber richtige Physik betreiben.
Eine einfache Energiebilanz sieht dann folgendermaßen aus:
der Thermische Wärmestrom addiert mit dem mechanischen Energiestrom ergibt die Summe der ausgetauschten Energie.Und was vorne reingeht,muss hinten wieder raus.
Man spricht dann von Prozessleistung.Dafür brauchen Sie zwei Betrachtungspunkte.
Wenn diese Randbedingungen richtig erfüllt sind,beginnt Physik.Vorher nicht !!!!
Herr Prof. Dr.-Ing. Peter Puschner macht seinen Einwand zu recht.
Wärme wird gedanklich gesehen als eine Fläche.Temperatur für die y-Koordinate und der Entropiestrom für die x-Koordinate.
Und jetzt beginnt das Dilemma,weil die internationale Sprachregelung sich nicht im klaren ist,wie dieser Entropiestrom beschrieben wird.Aber wenigstens über das Formelzeichen haben sich die Herren Physiker geeinigt.Als Würdigung des Herr Boltzmann mit dem Formelzeichen[S] in Watt/Kelvin
Die Quälerei, diesen Entropiestrom dem Energiestrom aufzuprägen,hat so manchen Physik-Interessierten zur Verzweiflung gebracht, und zum Abbruch des Studiums bewegt.Und das ist nach meinen Recherchen Absicht.
Mit herzlichem Glückauf
Glückwusch und viel Glück!
Er ist diejenige Zahl, die eine Gesamtheit von Meßwerten in zwei Teile teilt, von denen die eine Hälfte kleiner-gleich, die andere Hälfte größer-gleich dem Median ist. Deswegen ist der Median ein Wert, der (besonders) bei (stark) unsymmetrischen Häufigkeitsverteilungen die „typische“ Lage der Meßwerte wesentlich besser repräsentiert als der (arithmetische) Mittelwert oder auch der Modalwert. Technisch gesehen ist er die robusteste Schätzung der Lage, weil er durch einzelne „Ausreißer-“ Meßwerte kaum verzerrt wird – im Gegensatz zum Mittelwert.
Rainer Facius
Es kommt nicht nur darauf an, ob die verwendeten Algorithmen zu einem ähnlichem Langzeittrend führen oder nicht, sondern auf die Tatsache, das alle diese verschiedenen „Averages“ munter miteinander vermischt werden und neue Durchschnitte mit neuen Trends zu ermitteln.
https://tinyurl.com/Solarkonstante
nur umgekehrt. Viele rechnen mit dem Maximum von 1370 W/m^2, obwohl ja eigentlich nur ein Mittelwert, ja welches Mittel nur den Boden tatsächlich erreicht. Denn dabei haben ja aus den 1370 W/m^2 soviel Wellenlängen gar nicht so mit Wärme halt zu tun!
Die Abkühlung der Atmosphäre unter den Gefrierpunkt, bis unter mehr als – 50 Grad C in der den Erdball umspannenden Tropopause, besorgen Eispartikel. Sie absorbieren unterschiedliche Energieflüsse aus dem Raumwinkel von 360 Grad und emittieren Energie in Form von Wärmestrahlung in den gleichen Raumwinkel. Die Intensität dieser Strahlung beschreibt bekanntlich das Stefan Boltzmann Gesetz. Aus diesen Volumina erfolgt zweifellos die Emission der absorbierten Solarenergie in den Weltraum.
Darunter verstärken Eis- und Wasserpartikel bei höheren Temperaturen in sichtbaren Wolken die energetische Rückkopplung der Solarenergie zur Materie an der Erdoberfläche. Auch die Intensität dieser Komponente der Rückstrahlung beschreibt das Stefan Boltzmann Gesetz.
Noch weiter darunter agieren die mehr als zweiatomigen sog. Treibhausgase. Ihr vergleichsweise geringer Beitrag zur Rückstrahlung klingt von einem Maximum an der Erdoberfläche ausgehend konzentrations- und temperaturabhängig nach oben ab und verringert ebenfalls den Durchfluss von Energie nach oben. Als Folge dieser räumlichen Trennung der Wirkungen reduziert sich die Rückstrahlung der darüber wirkenden Wasser- und Eispartikel entsprechend. Das Ergebnis:
Für eine Korrelation von Wetter- und Klimaänderungen seit Beginn der Industrialisierung mit einem Anstieg der CO2 Konzentration in der Atmosphäre fehlt daher zweifellos die Kausalität!
Dazu bedarf es keines weiteren Beweises!!