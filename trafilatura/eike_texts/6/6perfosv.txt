Irgendwie geriet dieses Wissen in Vergessenheit oder wurde verdrängt. Ab 1980 war plötzlich die CO2-Treibhaustheorie wieder in der Diskussion und beschäftigt die Klimaforschung inzwischen aufs Heftigste. Fast panikartig blickt man in eine vermeintlich bedrohliche Zukunft und ist von jeder Form von eventuellen Rekorden wie Hitzerekorden fasziniert und schockiert. In aller Eile wird die Energieversorgung einer ganzen Nation in großer Hektik umgekrempelt.
Eine alte Weisheit aber besagt, man soll den Fluss überqueren und dabei die Steine unter seinen Füßen spüren. Bezogen auf den Klimawandel und die Energiewende gilt dies ebenso. Jeder gewissenhafte Unternehmer prüft ständig die Betriebsbilanz. Immer wieder schaut er auch zurück, ob die Grundlagen für einmal getroffene Entscheidungen noch haltbar sind.
Im Hinblick auf Klimaveränderungen ist seit Jahrhunderten bekannt, dass es natürliche Schwankungen gibt. Gerade deshalb ist es zwingend erforderlich, die Entwicklung konzentriert zu verfolgen. Dabei darf der Blick zurück im Maßstab einzelner Klimaperioden nicht vernachlässigt werden. Auch das vorhandene Datenmaterial muss wiederholt gesichtet und überprüft werden, ob die verwendeten Daten überhaupt noch repräsentativ sind. Hier scheint es erheblichen Arbeitsbedarf im deutschen Klimadatenfundus zu geben. Manche Stationen zeigen einen vermeintlichen Temperaturanstieg durch globalen Klimawandel. Benachbarte Stationen belegen 100 Jahre Stagnation der Temperatur. Was ist da los? Spiegeln sich hier nur Veränderungen im näheren Umfeld in den Daten wider mit der Folge einer notwendigen und konsequenten Disqualifikation der Station, oder entspricht das Gemessene tatsächlich einer natürlichen Entwicklung. Am Beispiel der Extremwerte verschiedener Wetterstationen von der Zugspitze, über den Hohenpeißenberg bis hinunter nach Schwerin in Ostseenähe wird aufgezeigt, welche Kraft tatsächlich für den Klimawandel im deutschen Temperaturdatensatz verantwortlich ist: der Mensch oder die Sonne.
Allgemeiner Einfluss der Sonne auf die Temperatur
Der heiße Sommer 2015 hat gezeigt, wie Rekordtemperaturen tatsächlich zustande kommen. Es sind klare, wolkenlose Tage an denen es zu den vermeintlichen neuen deutschen Temperaturrekorden kam. CO2 in der Atmosphäre soll für diese Rekorde verantwortlich gewesen sein.
Allerdings nur tagsüber, denn an der Rekordstation in Kitzingen lag vor und nach diesem Rekord (05.07. 2015 und wiederholt am 7.08.2015) die Minimumtemperatur in den Nächten um ca. 25 °C niedriger als tagsüber. Trotz hoher CO2-Konzentrationen kann es also innerhalb von 12 Stunden zu einem Wärmeverlust von ca. 25 °C kommen. Kann CO2 tatsächlich nur tagsüber Temperaturrekorde erzeugen, nachts dagegen aber weitgehend wirkungslos sein? Nein! 140 Jahre Klimaforschung in Deutschland haben immer wieder den Einfluss der Sonne auf das Temperaturgeschehen weltweit nachgewiesen. Solche Hitzetage bei klarem Himmel sind eindeutig einer sehr hohen Sonneneinstrahlung geschuldet. In den Nächten fehlt die Einstrahlung und mangels schützender Wolkendecke purzeln die Temperaturen dann „in den Keller“. Die Klimakunde nennt dies „Steppenklima“, wie es schon lange für den unterfränkischen Raum bekannt ist.
Doch nicht nur hier, überall auf der Erde kann man den Einfluss der Sonne auf die Temperatur am eigenen Leib spüren. Ist man direkt der Sonne ausgesetzt, ist an einem klaren Tag ein kräftiger Sonnenbrand aufgrund der intensiven UV-Strahlung unausweichlich. Dies passiert im Hochgebirge, auf dem flachen Land und am Meer. 90 % der Masse der Atmosphäre befinden sich in den unteren 20 km. Ein Teil der auf der Erde eintreffenden Sonnenenergie wird durch Wolken, Luft und Boden (hier besonders von Schnee) zu 30 % wieder in den Weltraum reflektiert. Die restlichen 70 % werden absorbiert: rund 20 % von der Atmosphäre, 50 % von der Erdoberfläche (Kontinente und Ozeane). Wenn also 50 % der Sonnenenergie bis auf die Erdoberfläche gelangen, dann ist das Auftreten eines Sonnenbrandes verursacht durch einen Teil dieses Strahlungsmixes, dem UV-Anteil, verständlich. Entsprechend müssten dann aber auch die bekannten Strahlungsschwankungen der Sonne direkt auf der Erdoberfläche im Lebensraum des Menschen wirksam werden und dort festgestellt werden können. Es verwundert schon, warum dann der Einfluss der Sonne auf das Klima so gering sein soll, wie in den Rechenmodellen des PIK e.V. und des IPCC suggeriert wird.
Jahrzehntelange Solarforschung hat ergeben, dass einzelne Anteile der Energieabstrahlung der Sonne sich dabei in durchaus respektablen Schwingungsbreiten oder Amplituden verändern. Die UV-Strahlung schwankt bis um 40 %, bei der Röntgenstrahlung sollen es sogar über 100 % sein, Faktor 2 (!). Die Sonnenfleckenzahl kann zwischen Null und maximal 500 Stück pro Tag variieren. Ähnliches gilt für solare Massenauswürfe, den Sonneneruptionen. Dabei verändert sich auch das Magnetfeld (Ursache der Sonnenflecken und –eruptionen) und auch der Sonnenwind. Wer sich die neuen Fotos der NASA der Raumsonde SDO (Solar Dynamics Observatory) z.B. auf der Webseite der NASA ansieht, kann erkennen, was es bedeutet, wenn im Sonnenmaximum bei hunderten von Sonnenflecken unglaublich große und spektakuläre Energiefreisetzungen stattfinden. Diese beeinflussen zwangsweise auch die Erde. Denn die kleine Erde ist ja gerade nur wenige 107 Sonnendurchmesser von ihrem riesigen Mutterstern entfernt und somit dem riesigen Solarreaktor direkt ausgeliefert.
Link zum Video 5 Jahre Solar Dynamics Observatory SDO: https://www.nasa.gov/content/goddard/videos-highlight-sdos-fifth-anniversary
und http://spaceplace.nasa.gov/review/solar-tricktionary/solarcycle.en.jpg
Abbildung 1: Schwankende Sonnenaktivität zwischen solarem Minimum (1996 und 2006) und Maximum 2001
140 Jahre Klimaforschung in Deutschland
Auf eike-klima-energie.eu, auf kaltesonne.de und vielen anderen nationalen und internationalen Webseiten wird intensiv über die Aktivitätszyklen der Sonne berichtet, beispielhaft seien die Arbeiten von Dr. Theodor Landscheidt (1) und von Raimund Leistenschneider genannt (2). Weniger bekannt oder für viele fast vergessen sind dagegen die Forschungen über den Einfluss der Sonne auf das Klima, die noch vor dem Paradigmenwechsel zum CO2 als Klimaantreiber in den 80 Jahren des 20. Jahrhunderts stattfanden. Herausragende Klimaforscher waren damals Prof. Dr. Julius Hann, Prof. Dr. Wladimir Peter Köppen und Prof. Dr. Artur Wagner, siehe Abbildung 2.
So berichtet Prof. Dr. Julius Hann (3), Universät Wien, in seinem „Handbuch der Klimatologie“ schon 1908 von intensiven Untersuchungen über den Einfluss der schwankenden Sonnenaktiviät auf das Klima. Insbesondere beim Verlauf der Temperatur konnten damals schon zyklische Veränderungen im Rhythmus des 11-jährigen Sonnenzyklus nachgewiesen werden. Er verweist dabei neben vielen anderen Autoren auch auf die Forschungen von Prof. Dr. Wladimir Peter Köppen in der Meteorologischen Zeitschrift Ausgabe 1873, S. 241 und 257: Über mehrjährige Perioden der Witterung insbesondere über die 11-jährige Periode der Temperatur. Köppen war bekanntlich ein deutscher Geograph, Meteorologe, Klimatologe und Botaniker. Er veröffentlichte über 500 Publikationen, die sich zumeist mit den Klimaverhältnissen der Ozeane und Kontinente beschäftigten, jedem Klimawissenschaftler aus vielen Bereichen der Klimakunde bekannt.
Abbildung 2: Bekannte Meteorologen, die sich für die deutsche Klimaforschung vor 100 Jahren verdient gemacht haben.
Kritiker werden gleich aufwerfen, dass man damals ohne Computer und Rechenmodelle solche Zusammenhänge gar nicht zweifelsfrei hat untersuchen können. Bestätigt wurde die Klimabeeinflussung durch die Sonne auch durch die Arbeit von Professor Dr. Artur Wagner, der 1940 das gesamte damalige Wissen über Klimaänderungen und Klimaschwankungen zusammengetragen hat (4). Hintergrund der in diesem Werk veröffentlichten Klimaforschungen war folgender:
Auszug aus dem Vorwort von Prof. A. Wagner zu Klimaänderungen und Klimaschwankungen 1940
„Seit Beginn unseres Jahrhunderts wird eine Änderung verschiedener Klimaelemente immer auffälliger. Es scheint an der Zeit zu sein, die zahlreichen Einzelarbeiten, welche kleinere Gebiete der Erde und einzelne Klimaelemente oder kürzere Zeitintervalle betreffen, zusammenzufassen und die Ergebnisse übersichtlich darzustellen. So gelangt man zu der Feststellung, dass das, was man im landläufigen Sinne als Klima bezeichnet, nichts Unveränderliches ist, sondern recht merklichen Abwandlungen im Laufe von Jahrzehnten und Jahrhunderten unterworfen ist… Die Eisbedeckung der Meere in hohen Breiten nimmt ebenso ab wie die der hohen Gebirge auf der ganzen Erde, die Temperatur des Meerwassers nimmt zu, ja sogar im Tier- und Pflanzenleben lassen sich bereits eindrucksvolle Änderungen nachweisen.“ (4).
Mit den von ihm zitierten 184 Forschungsarbeiten versuchte er die markante Erwärmung, die sich bereits ab Anfang des 19. Jahrhunderts abzeichnete und zu Beginn des 20. Jahrhunderts zu deutlichen Änderungen in den Polarregionen führte, zu erklären. A. Schmauss (1932, zitiert in 3) soll geradezu von einer „Klimaverwerfung“ gesprochen haben. So betrug im Mittel von 7 Orten in Deutschland mit sehr langen Temperaturreihen die Abweichung der Jahresmitteltemperatur 0,9 °C von –0,29 (1891-1895) bis +0,61 °C (1931-1935). In Anbetracht des schon damals über 100 Jahre andauernden Klimawandels mit ansteigenden Temperaturen hat die Internationale Klimakommission empfohlen, den Betrachtungszeitraum für Klimaaussagen auf maximal 30 Jahre zu beschränken und eben nicht möglichst langjährige Beobachtungen zugrunde zu legen. Dies würde einmal die gesetzmäßige Verteilung der Klimaänderungen über der ganzen Erde von Epoche zu Epoche viel klarer erfassen und genauere Aussagen über die Klimaentwicklung ermöglichen. Zusätzlich könnte so die statistische Aussage im Hinblick auf Extremwerte verbessert werden, für sicherere Vorgaben bei Prognosen und Planungen.
Professor Wagner verweist hier wiederum auf die umfangreichen Arbeiten von Prof. W. Köppen zum solaren Einfluss auf die Temperaturen der Erde. Seine Auswertungen von über 100 Jahren Klimaforschung ergaben damals, dass aufgrund der 11-jährigen solaren Schwankung eine die ganze Erde erfassende Schwankung der Temperatur von 0,36 °C resultiert. In einzelnen Regionen können die Abweichungen dabei durchaus größer sein, von bis zu 0,73 °C Temperaturunterschied zwischen Fleckenminimum und –maximum wird berichtet. Der Einfluss der Sonne auf die Temperaturen der Erde wurde zwar eindeutig erkannt, aber es war klar, dass mit den damaligen Mitteln die Wirkungszusammenhänge nicht abschließend gesichert festgestellt werden konnten.
Nach weiteren zwei Jahrzehnten Klimaforschung erschienen 1967 die Forschungsergebnisse von Dr. Hans von Rudloff über die „Schwankungen und Pendelungen des Klimas in Europa seit dem Beginn der regelmäßigen Instrumentenbeobachtungen“ (5) (Leider konnte von Dr. Rudloff kein Foto gefunden werden). Dr. Rudloff hat dazu Beobachtungen und Messungen aus über 300 Jahren zusammengetragen. Hintergrund seiner Arbeit war, dass es nach der Klimaerwärmung bis etwa 1940 zu einer markanten Klimaabkühlung kam. Die Weltgemeinschaft machte sich Sorgen, dass eventuell die Atomwaffentestversuche einen Einfluss auf das Klima haben könnten. „Die Besorgnis weiter Bevölkerungskreise über die Atom-Versuche und ihre eventuellen Auswirkungen auf das Großwettergeschehen wurde durch die seit etwa 1950 zu beobachtende „Klimaverschlechterung“ erheblich genährt. Dass wir uns aber zwischen 1920 und 1953 innerhalb eines säkularen Klimaoptimums befanden, wurde allzu rasch vergessen. Erscheinungen wie der in Süddeutschland extrem trockenheiße Juli 1964 sollten dieser verbreiteten Ansicht der „Klimaverschlechterung infolge Atom-Versuchen“ einen gewissen Einhalt gebieten“, so Rudloff in seinem Vorwort.
Insgesamt ergaben seine Auswertungen der umfangreichen Literatur (1114 Literaturzitate (!)),
dass in jedem Zeitalter mit außergewöhnlichem Verhalten des Wetters zu rechnen ist. So zeigten sich im nördlichen Teil Europas selbst in längeren Abschnitten Temperaturschwankungen, die auf
die Tier- und Pflanzenwelt deutliche Auswirkungen hatten. Die optimalen Wärmeverhältnisse beobachtete man im Norden Europas zwischen 1920 und 1953. Danach gingen in vielen Teilen Europas die Temperaturen zurück, gleichzeitig nahmen die Niederschläge zu. Es war wichtig zu erfahren, dass solche Schwankungen auch in früheren Jahrhunderten auftraten, als es weder Industrie noch Autoabgase gab und Atomwaffentests stattfanden.
„Wie wir heute wohl abschließend feststellen können, übte auch die Industrialisierung durch die Veränderungen im CO2 und SO2-Gehalt der Atmosphäre keinen entscheidenten Einfluss auf den Ablauf unseres Klimas aus… Man wird wohl nicht fehlgehen in der Annahme, dass die Hauptursache der Schwankungen und Pendelungen unseres Klimas in dem von Strahlung, Land- und Meeresverteilung in Gang gehaltenen Zirkulationssystem unserer Atmosphäre zu suchen ist. Die Sonne ist dabei die Kraft, die diesen Motor Zirkulation in Bewegung hält.“ (Dr. Hans von Rudloff, 1967 (5)).
Beim Studieren dieser „alten“ aber dennoch hochaktuellen Werke war der Autor dieses Artikels mehrfach sprachlos. Vor mehr als 6 Jahren begann er selbst sich vertiefter in die Problematik des „Klimawandels“ einzuarbeiten. Seit einigen Jahren werden eigenständige Auswertungen von deutschen und internationalen Klimadatenreihen vorgenommen. Dabei ist der eindeutige Zusammenhang zwischen den solaren Zyklen und zyklischen Schwankungen in verschiedenen Temperaturmessreihen aufgefallen. Auf kaltesonne.de wurde bereits zu diesem Thema „Solares Paradoxon Deutschlands, Teil I“ berichtet (6). Es war wie eine Offenbarung über 100-jährige Literatur in den Händen zu halten, in der sogar der bekannte Professor Köppen schon 1873 vom Einfluss solarer Zyklen auf Schwankungen der Temperatur in verschiedenen Regionen der Erde berichtete.
Mit dem Solaren Paradoxon Deutschlands – Teil II soll hier explizit noch einmal aufgezeigt werden, dass gerade in Deutschland seit über 140 Jahren der Einfluss solarer Aktivitätszyklen auf zyklische Veränderungen der Temperatur hinlänglich bekannt ist. Außer beim PIK e.V. in Potsdam. Dass der solare Einfluss auch heute noch wirksam ist, wird an den nachfolgenden Beispielen erläutert.
Abbildung 3: Die Entwicklung der Wintertemperaturen der Wetterstation Zugspitze im Einfluss der Solarstrahlung.
Abbildung 3 zeigt die Entwicklung der Wintertemperatur auf der Zugspitze im Zusammenhang mit dem Verlauf der Solarstrahlung. Die Temperatur ist hier als 5-jähriges gleitendes Mittel aus den Mittelwerten der Monate Januar bis März errechnet. Deutlich ist zu erkennen, dass sich die solaren Aktiviätsschwankungen häufig direkt in Temperaturänderungen niederschlagen. Nur in der Abkühlungsphase der 60er und 70er Jahre kam der Rhythmus „aus dem Tritt“. Die Solarstrahlung ist ja auch nur einer von mehreren solaren Parametern, die temperaturverändernd auf die Erde wirken. Ein völliger Gleichlauf zwischen Solarstrahlung und Temperatur ist bei der Komplexität des Wetters ohnehin nicht zu erwarten.
Weiter ist zu erkennen, dass die „Klimaerwärmung“ der Zugspitze bezogen auf das Winterhalbjahr bereits Vergangenheit ist, dies gilt auch für andere Alpengipfel. Nach einem singulären Erwärmungspeak um 1990, gingen die Wintertemperaturen wieder zurück und liegen bereits heute wieder auf dem Niveau der Winter vor 1980. Der Temperaturpeak um 1990 geht auf eine bekannte, markante Umstellung der Nordatlantischen Oszillation NAO Ende der 80er Jahre zurück.
Doch auch in den Sommermonaten verhält sich das Temperaturgeschehen in Deutschland paradox zu den Theorien des PIK e.V.. Steigen wir dazu von der Zugspitze hinab auf den 988 m hohen Hohenpeißenberg zum meteorologischen Observatorium des Deutschen Wetterdienstes. Abbildung 4 zeigt die Temperaturentwicklung der Wetterstation Hohenpeißenberg am Beispiel der kältesten Nächte im August im Zeitraum von 1893 bis 2015. Das Klimageschehen erscheint für den sehr langen Zeitraum sehr komplex. Für einen besseren Überblick wurde nachfolgend in Abbildung 5 der Zeitraum 1940 bis 2015 „herausgezoomt“.
Abbildung 4: Klimawandel am Beispiel der August-Min-Temperaturen der Wetterstation Hohenpeißenberg im Einfluss der Sonnenaktivität und dem Index der NAO 1893 bis 2015.
Im Hinblick auf die oben genannten Forschungsarbeiten der Autoren Prof. Hann, Prof. Köppen, Prof. Wagner und Dr. Rudloff, Hinweise unten in den Abbildungen 4 und 5, bot es sich zunächst an, den gesamten vorhandenen Datensatz auszuwerten. Gemeinsam mit der Temperatur der kältesten Nächte im August ist noch der Verlauf der Nordatlantischen Oszillation NAO und eine Sonnenaktivitätszahl S4 aufgeführt. Für diese Kennzahl wurden die solaren Faktoren: Solarstrahlung, Sonnenstürme, geomagentischer Index und Sonnenfleckenzahl bezogen auf ihr jeweiliges Minimum und Maximum auf Werte zwischen Null und Eins normiert und der Mittelwert daraus gebildet. Die Sonnenaktivitätszahl erlaubt damit die Darstellung mehrere solarer Faktoren in einer einzigen Kennlinie. Die genaue Vorgehensweise führt an dieser Stelle zu weit und wird deshalb im Anlage 1 erläutert.
Gleiches gilt für die Darstellung des Index für die Nordatlantische Oszillation NAO. Um die Daten in einem Diagramm zusammen mit der Temperatur und der Sonnenaktivitätszahl darstellen zu können, wurde der NAO Index ebenso auf Werte zwischen Null und 1 umgerechnet, in Anlage 1.
Verfolgt man nun den Verlauf der Nachttemperaturen Min im August, in Abbildung 4 dargestellt als 5-jähriges gleitendes Mittel, fällt das zyklische Verhalten aller Parameter auf. Zehn von zwölf solaren Zyklen spiegeln sich in entsprechenden Zyklen der Temperatur wider. Bis 1945 erfolgte ein Anstieg der Temperatur, der ja auch Auslöser für Professor Wagner war, die Ursachen einer Klimaerwärmung zu erforschen. Im gleichen Zeitraum stieg auch die Sonnenaktivität an, wie am Verlauf der Sonnenaktivitätszahl gut zu erkennen ist. Bis etwa 1980 kam es dann zu der Abkühlungsphase, die Dr. Rudloff zu seinen Forschungen zu den Schwankungen und Pendelungen des Klimas veranlasste. Die Abkühlung dauerte bis in die 80er Jahre. In diesem Jahrzehnt hatte auch die NAO eine Schwächephase. Gleichzeitig gab es einen Rückgang der Minima bei der Sonnenaktivitätszahl (1955 und 1965) und einen schwachen Sonnenzyklus in den 70er Jahren.
Abbildung 5: Klimawandel am Beispiel der August-Min-Temperaturen der Wetterstation Hohenpeißenberg im Einfluss der Sonnenaktivität und dem Index der NAO 1940 bis 2015.
Abbildung 5 zeigt die Entwicklung für die zweite Hälfte des 20. Jahrhunderts. Ausgehend von dem Tiefpunkt der 70er Jahre schaukelten sich die Min-Temperaturen der Nächte im August immer höher und erreichten Anfang 2000 ein Maximum bezogen auf das 5-jährige gleitende Mittel. Es ist zu erkennen, dass die zyklischen Veränderungen der Temperatur seit 1950 bei 7 Zyklen ohne Unterbrechung im gleichen Rhythmus wie die solaren Zyklen verliefen. Vergleicht man drei Jahre um das solare Maximum mit drei Jahren um das Minimum, ergibt sich bei der Min-Temperatur für den Monat August ein Anstieg um das solare Maximum von ca. +1,3 °C. W. Köppen hat, wie oben beschrieben, aufgrund der 11-jährigen solaren Schwankung eine die ganze Erde erfassende Schwankung der Temperatur von 0,36 °C festgestellt (4). In einzelnen Regionen können die Abweichungen dabei durchaus größer sein, mit bis zu 0,73 °C. Am Hohenpeißenberg beträgt die Schwankung der August-Min-Temperatur in der Phase einer sehr aktiven Sonne zwischen 1950 und 2015 sogar erstaunliche 1,3 °C.
Im Hintergrund der Graphik ist zwischen 1965 und 1995 auch bei der NAO der solare Rhythmus immer wieder erkennbar. Da es weniger wahrscheinlich ist, dass die Menschheit von der kleinen Erde aus der Sonne ihren „hektischen vermeintlich klimaschädlichen“ Rhythmus aufzwingen könnte, muss es wohl umgekehrt sein. Die Sonnenzyklen wirken sich eindeutig auf dem Hohenpeißenberg auf die Nachttemperaturen im August aus und das über 6 Jahrzehnte lang und nicht nur dort. Mit dem bekannten Einbruch der Sonnenaktivität seit ca. 10 Jahren, ist die NAO in den kälteren Modus zurückverfallen und die Nächte im August werden wieder kälter.
Es scheint, dass für die Erklärung der Klimaentwicklung in Deutschland die Theorie „CO2 führt zum Treibhaus Erde“ gar nicht benötigt wird. Ein Einfluss des CO2 mit seinem exponentiellen Anstieg auf die Temperaturentwicklung in Deutschland ist überhaupt nicht erkennbar!
In Anbetracht dessen wird ein besonderer Ausschnitt der jüngeren Klimageschichte Deutschlands vorgestellt. Begeben wir uns dazu vom Hohenpeißenberg in die Niederungen der Klimaforschung nach Potsdam.
Abbildung 6 zeigt die Entwicklung der Extremwerte für die heißesten Tage und kältesten Nächte im Monat August für Potsdam zwischen 1893 und 2015 im Vergleich zur Temperaturentwicklung in Schwerin. Links oben in Abbildung 6 erkennt man für den gesamten Messzeitraum für Potsdam einen Anstieg der Messwerte für beide Messgrößen. Ist daraus schon tatsächlich auf einen Klimawandel mit Klimaerwärmung zu schließen? Analysiert man den Messzeitraum zwischen 1923 und 1989 so fällt allerdings auf, dass die Temperatur der heißesten Tage zunimmt, die der kältesten Nächte dagegen abnimmt. Gleichzeitig Erwärmung und Abkühlung? Ein Blick auf das Temperaturgeschehen der Wetterstation Schwerin (200 km Entfernung) zeigt zwar für den gesamten Messzeitraum 1890 bis 2015 ebenso einen Trend der Erwärmung, im Zeitraum zwischen 1890 und 1989 herrscht aber fast 100 Jahre eine Stagnation der Temperatur der heißesten Tage und kältesten Nächte. Kein Klimawandel über 100 Jahre!
Wie am Beispiel der Wetterstation Zugspitze in Abbildung 3 bereits erläutert wurde, hat es zwischen 1985 und 1990 ein markantes Erwärmungsereignis durch die NAO gegeben. Vor diesem Ereignis war in Schwerin von einer Klimaänderung allerdings 100 Jahre lang überhaupt nichts zu spüren. Was lies dann aber in Potsdam ab 1923 die Temperaturen auf dem Telegrafenberg extremer werden, tagsüber heißer und nachts kälter? Könnte es sein, dass zu Beginn der Messungen die Wetterstation Potsdam auf dem Telegrafenberg zunächst in einer relativ kleinen Waldlichtung gelegen hat? Der umliegende, schützende Wald milderte damals die Temperaturextreme ab, weniger heiß tagsüber, weniger kalt in den Nächten. Mit der baulichen Entwicklung der meteorologischen Forschungseinrichtungen, der Ausdehnung der Waldlichtung, dem Bau von Erschließungsstraßen, die ehemalige geschotterte Waldwege ersetzten, die Umzäunung des Messgeländes mit einem schwarz gestrichenen Jägerzaun etc., letztlich die gesamte bauliche
Abbildung 6: Das Klimapendel schlägt zurück. Der Verlauf der Minimumtemperaturen im August zwischen 1890 und 2015 der Wetterstationen Potsdam und Schwerin. Seit 1989 zeigt sich ein deutlicher Trend der Abkühlung.
Erschließung könnte die extremeren lokalen Klimabedingungen auf dem Telegrafenberg bei Potsdam ausgelöst haben. Insgesamt ein Beispiel für viele Wetterstationen, bei denen Änderungen im lokalen Umfeld einen Trend zu wärmeren Temperaturen beim lokalen Kleinklima ausgelöst haben. Viel häufiger als das Beispiel dieser in klimatischer Sicht veränderten Waldlichtung ist die Problematik des Stadt-Land-Effektes bei vielen Wetterstationen. Anmerkungen zum Wärmeinseleffekt in deutschen Wetterdaten hat der Autor im Sommer diesen Jahres bereits hier geäußert: http://www.eike-klima-energie.eu/climategate-anzeige/waermeinseleffekt-in-deutschen-wetterdaten/
Beim Sport reichen manchmal schon wenige Millimeter zur Disqualifikation. Bei der Klimadatenmessung dürfen scheinbar alle „mitlaufen“, egal wie stark das Umfeld von Stationen im Laufe langjähriger Messreihen gegenüber ihrem Startzustand verändert wurde. Welcher neutrale Schiedsrichter entscheidet bei der Klimadatenmessung über einen Ausschluss vom Wettkampf? Wieviel Wärmeinseleffekt darf eine Station in den Gesamtdatensatz von Deutschland einbringen und damit einen CO2-bedingten Klimawandel vorgaukeln ohne Disqualifikation?
Die klimawirksame Umstellung der Nordatlantischen Oszillation NAO hin zu einem sehr „wärmefördenden“ Modus für Mitteleuropa geschah um 1988. In den Graphiken in Abbildung 6 unten ist offensichtlich zu sehen, dass die NAO-bedingte Erwärmung Deutschlands nicht lange anhielt. Seit 1990, seit 27 Messjahren, setzte sowohl in Potsdam als auch in Schwerin für die heißesten Tage im August ein Temperaturrückgang ein: Potsdam –0,6 °C; Schwerin – 1,7 °C. In Potsdam stagniert die Temperatur der kältesten Nächte, in Schwerin betrug der Rückgang –0,3 °C.
Dem Kritiker mag dies marginal erscheinen, zudem sind es nur einzelne Tage im August! Die Überraschung wird aber noch größer, wenn man die Entwicklung der Wintertemperaturen in Potsdam analysiert, siehe Abbildung 7. Dargestellt ist das Mittel der kältesten Nächte der Monate Januar bis März für den gesamten Messzeitraum. Es ist gut zu erkennen, dass die kältesten Nächte im Winter eindeutig parallel mit der NAO verlaufen. Um 1988 war das Maximum im Wärmemodus der NAO. Seither sind in Potsdam, und nicht nur da, die Temperaturen der kältesten Winternächte wieder markant gefallen, nämlich –2,8 °C in 27 Messjahren. J. Kowatsch z.B. in Leistenschneider et al. (7) hat bereits auf die immer kälter werdenden Winter in Deutschland aufmerksam gemacht.
Abbildung 7: Klimawandel durch den Einfluss der NAO am Beispiel der Winterminima-Temperatur der Monate Januar bis März (gemittelt) der Wetterstation Potsdam.
Die kältesten Nächte sowohl in den Sommermonaten als auch in anderen Jahreszeiten sind auf bestimmte Wetterlagen zurückzuführen. Vermutlich sind es klare Nächte bei Nord- bis Nord-Ost-Wetterlage, die diese Minimumtemperaturen verursachen. Möglicherweise wird hier die Abkühlung des Nordatlantiks bereits in Deutschland erkennbar und draußen im Freien auch spürbar.
Mit einer Analyse des heißen Sommers 2015 wird abschließend aufgezeigt, wie paradox sich die Klimadiskussion in Deutschland insgesamt darstellt. Abbildung 8 zeigt die Augusttemperaturen der langen Messreihe des Deutschen Wetterdienstes. Es ist bekannt, dass erst ab 1880 das deutsche Messnetz vereinheitlicht wurde und die offizielle Messreihe deshalb erst ab diesem Datum startete. Dennoch sind die früheren Werte ebenso fachkundig gemessen und taugen deshalb für die Beurteilung der langfristigen Entwicklung. Es geht hier ja nicht um Nyancen von Zehntelgrad. In die Graphik ist unten der Verlauf der Sonnenaktivität am Beispiel der Sonnenfleckenzahlen eingefügt.
Abbildung 8: Klimawandel in Deutschland im Einfluss der Sonnenaktivität am Beispiel der langjährigen Messreihe der August-Temperaturen. Starke Sonne führt zu Temperaturanstieg, schwache Sonne zu Temperaturückgang! Ist das der Grund, warum der wissenschaftliche Beweis, dass CO2 einen Treibhauseffekt mit Klimaerwärmung verursachen soll, bis heute nicht erbracht wurde?
Die Auswertung geschah ohne Großcomputer im Unterschied zu den Klimauntersuchungen am PIK e.V. in Potsdam. Die Länge der Erwärmungs- und Abkühlungsperioden wurde nach Augenschein und unter Berücksichtigung der solaren Aktivität vorgenommen. Für Kritiker wahrscheinlich rein unwissenschaftliches „cherrypicking“. Für diejenigen, die eine Erklärung der tatsächlichen Ursachen für den Klimawandel suchen, ein weiterer Schritt, ein weiteres Indiz dafür, was wirklich passiert. Kritiker mögen im Gegenzug versuchen über den Verlauf der CO2-Konzentration der Atmosphäre eine bessere und überzeugendere Erklärung für die Temperaturentwicklung Deutschlands seit 1760 abzugeben.
Die erste Analyse der mittleren Augusttemperaturen ergab zunächst ein wirres, scheinbar ungeordnetes Auf und Ab. Aber immerhin war deutlich zu erkennen, dass es wärmere und kältere Phasen gab. Mit viel Aufwand ohne PC haben die Klimaforscher vor 100 Jahren gut erkannt, dass man den Zeitraum für die Beurteilung des Klimas und der Klimavariabilität einer Region zeitlich begrenzen muss, um genauere Aussagen für Klimaentwicklungen zu erhalten. Wie oben bereits erwähnt, hat die Internationale Klimakommission deshalb empfohlen den Betrachtungszeitraum für Klimaaussagen auf 30 Jahre zu beschränken, beginnend mit der Periode 1901 bis 1930. Wer diese 30-Jahresperiode einmal in Gedanken über den gesamten Zeitraum jahreweise verschiebt, erkennt die Variabilität der deutschen Augusttemperaturen und damit die Erkenntnisse bezüglich eines bestimmten Klimazustandes. Das eine „Klima für Deutschland“ gibt es wahrlich nicht, es gibt nur die ständige Variabilität innerhalb statistischer Extremwerte. Und diese verändern sich, je länger der Betrachtungszeitraum gewählt wird. Um einen Klimawandel überhaupt gesichert feststellen und beurteilen zu können, benötigt man mindestens zwei Messperioden also 60 Jahre!
Erst wenn man die Augusttemperaturen mit dem Verlauf der Sonnenaktivität koppelt, klärt sich das ungeordnete Verhalten und es zeigt sich der bedeutende Einfluss der Sonne auf unser Klima. Vereinfacht wurden die Sonnenzyklen in starke und schwache aufgetrennt. Dies ist durch die eingezogene orange-blaue Linie rein per Augenschein geschehen. Je länger man dieses Auf und Ab der Sonnenaktivität betrachtet, könnte man sehr vereinfacht meinen, dass die Sonne nur zwei Betriebszustände kennt; entweder deutlich überdurchschnittlich (Vollgas-Autobahn) oder deutlich unterdurchschnittlich (Gemächlich-Landstraße). Es scheint darüber hinaus selten der Fall einzutreten, dass nur ein Zyklus unter- oder überdurchnittlich ist. Meistens sind es drei, vier oder gar fünf in etwa gleichgeartete Zyklen. Phasen mit aktiver Sonne mit hohen Sonnenfleckenzahlen wurden mit der Farbe Orange belegt, schwache Phasen mit Blau.
Verfolgt man nun ab 1760 das Temperaturgeschehen in Deutschland für den Monat August im Vergleich mit der Sonnenaktivität, so fällt auf, dass es in Phasen überdurchschnittlicher Sonnenaktivität zu einer Erwärmung im Sommer kam, ansteigender Temperaturtrend. Immer, wenn die Sonne schwächer war und die Sonnenfleckenzahlen zurückgingen, kam es dann nachfolgend zu einer Temperaturabkühlung. Die Trends der einzelnen Erwärmungs- und Abkühlungsphasen wurden berechnet.
Weiter fällt auf, dass bis 1980 langfristig zwar ein Auf und Ab der Temperatur erfolgte. Würde man für den langen Zeitraum von 220 Jahren von 1760 bis 1980 einen Trend berechnen, so wäre der wenig ausgeprägt. Dies zeigten ja auch die Extemwerte für den Monat August der Wetterstation Schwerin über fast 100 Jahre zwischen 1890 und 1989. Erst ab ca. 1990 stiegen die Augusttemperaturen in Deutschland überdurchschnittlich an und erreichten Spitzenwerte um 2000.
Kritiker sehen hier endlich den Einfluss des CO2. Klimarealisten fällt allerdings auf, dass um 1970 der 20. solare Zyklus unterdurchschnittlich war, aber eben nur dieser. Eine längere Abkühlungsphase über mehrere Jahrzehnte wie in den 200 Jahren zuvor ist ausgeblieben. Ab ca. 1980 haben deshalb drei überdurchschnittlich starke solare Zyklen, die den drei starken Zyklen zwischen 1940 und 1960 nachfolgten, zu einer deutlichen Temperaturerhöhung geführt, die bekannte Phase der Klimaerwärmung. Wie oben ausgeführt, passt in die Erwärmung nach 1990 auch die Entwicklung der NAO. Für die letzten Jahre ab ca. 2000 muss auch für die lange Messreihe der Augusttemperaturen in Deutschland diagnostiziert werden, dass bereits ein deutlicher Abkühlungstrend eingesetzt hat. An diesem Trend hat selbst der heiße Sommer 2015 nichts geändert. Auch für den längeren Zeitraum ab 1990, 25 Jahre, ist die Temperaturentwicklung negativ. Es wird eindeutig wieder kälter!
So wie eine Schwalbe noch keinen Sommer macht, so machen einzelne Temperaturrekorde noch keinen Klimawandel. Analysiert man die Temperaturen der Rekordstation Kitzingen im Sommer 2015 genauer, so erkennt man, dass den heißen Tagestemperaturen vergleichsweise niedrige Nachttemperaturen gegenüber stehen. Die Differenz zwischen Tagesmaximimum 40,3 °C (Deutschlandrekord) und den Nachttemperaturen mit ca. 15 °C um diese Rekordtage herum beträgt fast 25 °C, siehe oben. Wenn Kohlenstoffdioxid tagsüber für diese Rekordtemperaturen verantwotlich sein soll, wieso hält es dann nicht auch in den Nächten die Wärme zurück? Wie könnte in der kleinen Waldlichtung in Potsdam das CO2 nur tagsüber für eine Erwärmung sorgen, nachts aber unwirksam sein und statt dessen eine Abkühlung zulassen, wie es im Zeitraum zwischen 1923 und 1988 in Potsdam passierte? Es bleibt festzustellen: Im deutschen Temperaturdatensatz gibt es Stationen, die man eigentlich disqualifizieren müsste. In der Summe aus „Guten“ und „Schlechten“ gibt es keine halbgute, sondern doch nur eine schlechte „Mannschaftsleistung“, ein falsches Ergebnis im Hinblick auf die Erkenntnis über den Klimawandel.
Bereits 1967 hat Dr. Rudloff den Einfluss der Sonnenaktivität auf die Sommertemperaturen in Mitteleuropa am Beispiel der Wetterstation Basel-Binningen für die Phase 1750 bis 1960 aufgezeigt (in (5), Seite 254), siehe Abbildung 9. Sie deckt sich genau mit dem in Abbildung 8 dargestellten Verlauf der Augusttemperaturen in Deutschland. Das sind über 250 Jahre solarer Einfluss auf die Temperatur in Deutschland und damit in Mitteleuropa.
Abbildung 9: Wirkung variabler Sonnenaktivität auf Sommertemperaturen und Niederschlag in Mitteleuropa nach H. Rudloff (in (4), S. 254)
Wie die moderne Solarforschung festgestellt hat, ist die Sonnenaktivität seit ca. 10 Jahren stark eingebrochen. Die Entwicklung der solaren Prozesse deutet an, dass mindestens die nächsten zwei Zyklen ebenfalls unterdurchschnittlich ausfallen werden. Aus 250 Jahren Temperaturentwicklung im Einklang mit der Sonne bleibt daraus nur die eine Schlussfolgerung: Das Klimapendel schlägt zurück, die Erwärmung ist vorbei, die Abkühlung hat bereits eingesetzt und wird wahrscheinlich mehrere Jahrzehnte andauern. An dieser Entwicklung hat selbst der heiße Sommer 2015 nichts ändern können. Die aktuelle Entwicklung in der Arktis lässt daran keine Zweifel.
Aktuelle Klimafakten einer Abkühlung der arktischen Polarregion
Abkühlung des Nordatlantiks, Zunahme der Schneebedeckung auf Grönland, Wachstum des arktischen Meereises (Maximum der letzten 10 Jahre, ohne die Küstenregionen betrachtet), Maximum der Eisbedeckung auf der Hudson Bay, schneereiche und kalte Winter im Osten Kanadas und der USA, Zunahme der Schneebedeckung der gesamten Nordhemisphäre im Winterhalbjahr, starke Abkühlung der Winter in den Hochalpen und in Deutschland insgesamt, Abkühlung der kältesten Nächte in den Sommermonaten in Deutschland um mehrere Grad seit Anfang 2000, kältester Sommer 2015 in Nordeuropa seit über 60 Jahren…
Fazit und Bewertung
Die Analyse der Entwicklung der Extremwerte von Wetterstationen zwischen Zugspitze und Ostsee zu verschiedenen Jahreszeiten hat gezeigt, dass nicht ein zusätzlicher Wärmeschub aufgrund eines vermeintlich CO2-bedingten Treibhauseffektes für die Klimaentwicklung in Deutschland verantwortlich war. Es war das Fehlen von Kälte aus der Polarregion. 5 Jahrzehnte mit starker Sonnenaktivität, führten zu einem Zurückschmelzen des Polareises. Die schnee- und eisbedingte Albedo veringerte sich. Die Polarregionen wurden weniger kalt. Eine Umstellung der Nordatlantische Oszillation NAO war die Folge. Dieser Zustand führte zu weniger kalten Wintern und weniger kalten Extremtemperaturen einzelner Monate auch in Mitteleuropa, allerdings nur für wenige Jahre. Dieser Prozess hat sich seit ca. 25 Jahren wieder umgekehrt.
Bei alleiniger Betrachtung von langjährigen Temperaturmittelwerten war der Schluss falsch, dass eine allgemeine globale Erwärmung stattfand und diese auch in Mitteleuropa relevant wäre. Wie bereits vor 100 Jahren in der deutschen Klimaforschung darauf hingewiesen wurde, erkennt man bei fundierter Analyse der 30-Jahresperioden und ihrer Extremwerte die tatsächliche Klimaentwicklung wesentlich klarer. Die Analyse der Extremwerte des Monats August und der Wintertemperaturen von Wetterstationen zwischen Zugspitze und Ostsee (Schwerin) zeigt seit 25 Jahren einen klaren Trend zu kälteren Temperaturen.
Die Deutsche Klimaforschung hat schon vor 1970 den Zusammenhang zwischen Klima und Sonnenaktivität klar erkannt. Bereits Dr. Rudloff betonte, dass die Sonne die Kraft ist, die Schwankungen und Pendelungen unseres Klimas verursacht. Sie hält den Motor in Gang, der für die Zirkulationssysteme der Atmosphäre und Meere verantwortlich ist. Schwankungen der Sonnenaktivität schlagen sich direkt auf Wetter und Klima in allen Regionen der Erde nieder. Die weitere Temperaturentwicklung in Deutschland zwischen 1970 und 2015 lässt sich ebenso allein mit dem Verlauf der Sonnenaktivität und der Nordatlantischen Oszillation NAO erklären. Eine Hinzuziehung des Kohlenstoffdioxids CO2 für weitere Erklärungsversuche ist nicht von Nöten. Entsprechend ist ein Einfluss des CO2 auf das Klima in Mitteleuropa nicht erkennbar und deshalb auch nicht gegeben. Deutschland muss sich nicht auf eine weitere Klimaerwärmung einstellen. Mit dem Einbruch der Sonnenaktivität droht eindeutig eine jahrzehntelange Klimaabkühlung mit allen negativen Begleiterscheinungen.
Die Klimaentwicklung der Zugspitze zeigt eindrücklich, dass die aktuelle Referenzperiode 1991 bis 2020 bisher komplett von einem markanten Abkühlungstrend beherrscht wird, sowohl bei den heißesten Tagen im August wie auch bei den kältesten im Winter von der Zugspitze bis hinab zu Ostsee. Es wäre fahrlässig diese Entwicklung zu ignorieren.
Nach der hier vorgestellten Analyse deutscher Klimadaten liegen die tatsächlichen Beweggründe für die Klimakonferenz in Paris zur Begrenzung der globalen Erwärmung durch Reduzierung der atmosphärischen CO2-Konzentration im Dunkeln…
Literatur
(1) Th. Landscheidt: Sonnenaktivität als dominanter Faktor der Klimadynamik. http://www.schulphysik.de/klima/landscheidt/sonne1.htm
(2) Leistenschneider, R. (2011): Dynamisches Sonnensystem – Die tatsächlichen Hintergründe des Klimawandels. Teile 1 bis 8. http://www.eike-klima-energie.eu.
(3) Hann, Julius (1908): Handbuch der Klimatologie. Band I: Allgemeine Klimalehre. Verlag von Engelhorn, Stuttgart.
(4) Wagner, Artur (1940): Klimaänderungen und Klimaschwankungen. Die Wissenschaft Band 92, Friedrich Vieweg und Sohn, Braunschweig.
(5) Rudloff, Hans v. (1967): Die Schwankungen und Pendelungen des Klimas in Europa seit dem Beginn der regelmäßigen Instrumenten-Beobachtungen (1670). Friedrich Vieweg und Sohn, Braunschweig.
(6) Michelbach, S. (2015): Solares Paradoxon Deutschlands, Teil I: Solare Zyklen in der deutschen und der globalen Temperaturmessreihe. http://www.kaltesonne.de
(7) Leistenschneider, R.; Kowatsch, J.; Kämpfe, S. (2015): Sommer 2015 – Die Sonne ist an allem schuld! http://www.eike-klima-energie.eu.
Anlage 1 Berechnung der Sonnenaktivitätszahl S4
Die Sonnenaktivitätszahl wurde hier eingeführt, um mehrere solare Parameter in einer Größe zu vereinigen. Damit ist die Darstellung der Sonnenaktivität im Vergleich mit anderen Messgrößen wie der Temperatur oder der Nordatlantischen Oszillation NAO in einem Diagramm möglich.
Als solare Parameter wurden die Solarstrahlung, die Anzahl der Sonnenstürme, das Solare Magnetfeld als „heliospheric magnetic field“ nach NOAA J. Lean, und die Anzahl der Sonnenflecken gewählt. Da jeder Parameter seine eigene Dimension besitzt, mussten sie zunächst auf Werte zwischen Null und Eins „normiert“ werden. In Anlehnung an ihre jeweiligen Minima und Maxima wurden folgende Werte als Null und Eins definiert.
Durchführung der Normierung für die Sonnenaktivitätszahl
Solarstrahlung Sin
Anzahl der Sonnenstürme Stn
heliospheric magnetic field HMF Bn
Anzahl der Sonnenflecken SSn
Minimum der Messreihe:
1364,3 W/m²
gewählt: 1358 W/m²
Minimum der Messreihe:
0
gewählt: 0
Minimum der Messreihe
4,06
gewählt: 4
Minimum der Messreihe
0
gewählt: 0
Maximum der Messreihe:
1366,7 W/m²
gewählt: 1367 W/m²
Maximum der Messreihe: 71
gewählt: 80
Maximum der Messreihe
9,48
gewählt: 10
Maximum der Messreihe
191
gewählt: 200
Beispiel
Anzahl der Sonnenflecken = 85,2
die normierte Sonnenfleckenzahl beträgt dann (85,2 – Min)/Max = (85,2-0)/200 = 0,456
Berechnungsformel für die Sonnenaktivitätszahl S4
Da nach R. Leistenschneider das solare Magnetfeld womöglich einen stärkeren Einfluss auf die Temperatur hat als die anderen Parameter, wurden in nachfolgender Formel eine entsprechende Gewichtung vorgenommen.
Sonnenaktivitätszahl S4 = 0,2xSin + 0,2xStn + 0,4xHMF Bn + 0,2xSSn
Um den Index der NAO in einem Diagramm zusammen mit der Sonnenaktivitätszahl und der Temperatur darzustellen war es aus programmtechnischen Gründen erforderlich, auch diese Größe in Werte zwischen Null und Eins umzurechnen. Dazu wurden als Minimumwert –6 und als Maximum +6 gewählt und daraus die normierten Werte wie im Beispiel für die Sonnenflecken, siehe ob
sie schreiben:
„Die Klimamodelle machen keine Annahmen, sondern stecken nur bekannte Zusammenhänge rein.“
Das ist irreführend. Jede Differentialgleichung mit der man versucht die Wirklichkeit zu beschreiben ist eine Annahme. Das Klimamodell ist die Annahme, also auch viele Zusammenhänge die drin stecken, auch wenn sie die mit dem Adjektiv „bekannt“ versehen.
Was sie mit „bekannter Zusammenhang“ bezeichnen ist eine Annahme. Das sollte gerade einem Naturwissenschaftler bewusst sein. Das ist aber vielleicht auch das Problem in der Klimadebatte. Viele Annahmen werden als sichere Fakten verkauft. Als Beispiel. Die ideale Gasgleichung ist ein bekannter Zusammenhang. Nichtsdestotrotz nur eine Annahme.
Sorry, aber was sie betreiben liest sich für mich wie eine Pseudodiskussion die Laien aufs Glatteis führen soll.
„Heutige Klimamodelle machen auch annahmen über Wirkzusammenhänge, die man nicht hinreichend kennt.“
Eben das ist ja falsch. Die Klimamodelle machen keine Annahmen, sondern stecken nur bekannte Zusammenhänge rein. Sie verstehen einfach immer noch nicht, wie physikalischen Modellle arbeiten und setzen fälschlicherweise Parametriesieren mit Annehmen von Hypothesen gleich!
nochmal eine Bemerkung von mir als Naturwissenschaftler.Sie habe doch die Epizykel-Fits für die Planetenbewegung genannt. Bei der Untersuchung von Herrn Michelbach und anderen Untersuchungen über mögliche Korrelationen von Sonnenaktivität zum Klima handelt es sich ebenfalls um nichts anderes als Fits zwischen Beobachtungsdaten. Das Vorgehen da ist durchaus mit dem Ptolemäischen Ansatz vergleichbar.
Sehr geehrter Herr Baecker
Wegen ihrer Erinnerung an anderer Stelle greife ich diesen Faden nochmals auf.
Die Methode, nach Fits zu suchen, ist durchaus eine wissenschaftliche im deduktiven Sinn. Sie ist nicht die einzige Methode und synthetische, induktive Methoden haben parallel auch ihre Berechtigung. Bei beiden Ansätzen muss natürlich die Prüfung der Plausibilität erfolgen, ohne die es ansonsten eine leere Übung wäre.
————— #108: NicoBaecker sagt:
Die GCMs heben sich vo diesen Fits ab, indem sie auf Basis der Physik arbeiten.
Während ie Plausibilitäet im deduktiven Verfahren die Übereinstimmung mit bekannten Gesetzen suchen muss, ist die empirische Bestätigung in induktiven Verfahren ein Muss. Und da mangelt es bei den GCMs. Die übrigens sehr wohl auch mit Fits arbeiten, um die Parameter abzuschätzen.
————— #108: NicoBaecker sagt:
Sie können also leicht folgende Analogie machen:
A) Fit von Epizykeln an Planetenpositionsdaten = Korrelationen zwischen Sonnenaktivitätsdaten und Klimadaten
B) Himmelsmechanishce Berechnung der Planetenpositionen auf Basis der Physik (newtonsche Mechanik) = heutige Klimamodelle
Das ist in der Zuspitzung schlicht irreführend. Zum Zeitpunkt des Epizyklusodells war das heuittige Verständnis nicht existent. Man suchte eine Ableitung aus der Beobachtung und kam durchaus nachvollziehbar zu einer falschen Lösung.
Heutige Klimamodelle machen auch annahmen über Wirkzusammenhänge, die man nicht hinreichend kennt. Es ist eher anzunehmen, dass sie ebenso daneben liegen wie das Epizyklenmodell.
————— #108: NicoBaecker sagt:
Die Modellierung auf Basis physikalischer Gleichungen (B) (inkl. Parametrisierung komplexer Prozesse durch einfachere Ersatzprozesse) betrachte ich als Physiker als Fortschritt – auch erkenntnistheoretisch.
Der Erkenntnisfortschritt ist unbestritten, aber der Verleich unpassend. Denn der Erkenntnisfortschritt ist der gesamten Entwicklung der Wissenschaften geschuldet, nicht dem hier eingesetzten Verfahren. Tatsächlich leistete das Epizyklen-Modell erstaunliches. Hierzu war ebenso eine ausgefeilte Mathemathik erforderlich, denn ansonsten hätte man die Planetenpositionen nicht so exakt vorhersagen können.
schade, daß Sie sich nicht mehr zu dieser interessanten diskussion melden. Ich hatte wirklich die Hoffnung, etwas substanzielles von der Fraktion der Skeptiker erfahren zu können.
„Sie haben den Punkt offensichtlich nicht verstanden. Prägendes Verständnis des Weltbildes wirkt nicht nur auf Denker oder Wissenschaftler oder das gemeine Volk, sondern prägt die Kultur und damit in der Rückkopplung auch wissenschaftliche Auffassungen.“
Ich habe den Punkt wahrscheinlich besser als Sie selber verstanden. Deshalb fragte ich Sie ja auch mehrfach schon, aber bislang ergebnislos danach, ob Ihr Problem mit den GCMs ein spezielles ist oder ob es daran verknüpft ist, daß Ihnen unser gegenwärtiges natuwissenschaftlich-technisch geprägtes Weltbild, welches seit rund 300 Jahren davon ausgeht, daß der Mensch selbst durch sein Denken die Natur erkennen kann, nicht ausreichend erscheint, um das Klimasystem zu verstehen.
Daher die oben gestaffelten Fragen a) bis c). Sie eiern mir zu sehr herum, und ich möchte mal den Punkt klären, ob Sie ein Problem mit der naturwissenschaftlichen Methodik der Modellbildung an sich haben. Dann wäre Ihr „Klimamodellproblem“ nur die Auswirkung genereller Vorbehalte. Verstehen Sie mich, und können Sie mir Ihre Position einfach mitteilen, indem Sie die Fragen beantworten?
auch zum exkurs muß ich Widerspruch leisten und Sie korrigieren. Aber dazu auch bei passenderer Gelegenheit.
Aber mir ist wichtiger, Ihre Meinung zu #108, #103, #105 und #100 (etwa in der Reihenfolge) zu erfahren.
PS: Falls diese Diskussion zu langatmig wird, wären Sie auch an einer Fortführung abseits per email interessiert?
die Diskussion mit Ihnen unter diesem Artikel finde ich wie gesagt interessant. Ich möchte nun gar nicht neue Themen aufreißen und mich an dieser albernen Diskussion um den jüngsten Stehlik et al. Artikel beteiligen.
Herr Stehik hat das Grundlagenwissen der Physik völlig verloren und daher weiß man nie, was für falsche Schlüsse er aus Erklärungen, die man ihm liefert, zieht.
Ich müßte mir Ihre, Stehliks und Kramms Kommentare noch einmal im Detail durchlesen, um zu einem abschließenden Urteil über die Debatte zwischen Ihnen und Stehlik bzw. Kramm zu kommen.
Da mir aber die angefangene Diskussion mit Ihnen hier jedoch wichtiger ist, möchte ich die Diskussion über die IR-Spektren nicht fortführen. Nur soviel: Wie ich schon öfter detailliert vorgerechnet habe, geht das Plancksche Strahlungsgesetz auch bei dem thermischen Emissionsspektum jedwegen Stoffes inkl. Gase ein. Denn dieses beschreibt ja die Energieverteilung thermischer Photonen ganz allgemein.
Auch wenn es hier ein Exkurs ist, so doch ein reizvoller:
———- #98: NicoBaecker sagt:
„Das, was wir heute Naturgesetze nennen, war keineswegs stets Wissen der Menschheit. … Das mechanische Weltbild wurde mit der Entwicklung der modernen Naturwissenschaften allmählich zum Standard. Laplace drückte es am markantesten aus.
Dieses Weltbild bekam aber mit den Relativitätstheorien bereits erhebliche Risse und wurde durch die Quantenmechanik weitgehend auf den Kopf gestellt. Einige Menschen wollen dies auch nach hundert Jahren nicht zu Kenntnis nehmen.“
Das ist populistischer Quark, so einfach war die Wissenschaftshistorie nicht. Das „mechanistische Weltbild“ war ein „Ideal“ Anfang es 19. Jahrhdts.: Man hatte die Hoffnung, daß sich auch irdische Bewegungsprozesse so elegant und exakt wie die Planetenbewegung vorhersagen lassen. Aber zur gleichen Zeit, wurden dieselben Forscher mit neuen Entdeckungen wie der Elektrizität konfrontiert.
Elektrizität erschüttert das mechanistisch-deterministische Weltbild nicht, sonder erweitert es lediglich. Es gibt in der Elektrizität nichts, was sich am vorherigen Weltbild revidieren müsste.
Meinen sie wirklich, dass sich meine Deutung mit dem Begriff ‚populistischer Quark‘ beschreiben ließe? Ich dachte immer, dass sie vielleicht in manchem anderer Meinung sind, hielt sie aber nicht solchen Nonsens für fähig.
———- #98: NicoBaecker sagt:
Das mechanistische Weltbild war niemals in Stein gemeißelt. Was da „Risse“ durch RT und QM bekam wäre dann auch nicht das mechanistische Weltbild, sondern die klassische Physik knapp 100 Jahre später.
Sie haben den Punkt offensichtlich nicht verstanden. Prägendes Verständnis des Weltbildes wirkt nicht nur auf Denker oder Wissenschaftler oder das gemeine Volk, sondern prägt die Kultur und damit in der Rückkopplung auch wissenschaftliche Auffassungen.
———- #98: NicoBaecker sagt:
Die „Risse“ haben auch nur die zu beklagen, die unbedingt die Physik abschließen wollen und ein Weltbild damit gründen wollen.
Ich weiß nun nicht, wer diese Risse beklagt. Ich stelle lediglich fest, dass es sie gibt und festgefügte Vorstellungen aufbricht. Ich begrüße das ausdrücklich!
Sie meinen vermutlich die Fraktion jener, die mit der Parole aufschlagen ‚Science is settled!‘
———- #98: NicoBaecker sagt:
Für die Physik sind solche „philosophischen Überbauten“ völlig irrelevant, denn die Laplacesche Himmelsmechanik ist heute so gültig und genau wie sie immer schon war. Durch die Relativitätstheorien (RT) und Quantenmechanik (QM) hat sich daran nichts geändert, diese beiden Theorien beschreiben ja Effekte, mit denen man zu Laplace Zeiten gar
nicht durch empirische Beobachtungen konfrontiert war.
Laplace ist nicht nur durch seine Himmelsmechanik, sondern durch seinen ‚wissenschaftlichen‘ Determinismus, der sich im Laplaceschen Dämon niederschlägt, in die Geschichte und Philosophie eingegangen. Ihre Ansicht, dass die Physik nicht das Weltbild präge, halte ich für reichlich uninformiert.
wenn ich mir Ihre Beiträge zum Stehlik, Eichten, Hüne-Artikel ansehe, frage ich mich, ob Ihr Problem vielleicht größtenteils darin liegt, daß Sie sich einfach verzetteln und nicht die Zeit nehmen, mal systematisch zu arbeiten. Sie haben nun eine Debatte mit vielen ausgelöst, nur weil Sie ein paar Diagramm nicht richtig verstehen und die Kommentare dazu auch noch misssverstehen.
Sehr geehrter Herr Baecker
Es fällt mir schwer, den Sinn ihrer Aussage zu verstehn. Selbst wenn ich nicht in Verteidigung meiner Aussagen gehen will und es als gut gemeinte Hinweise deuten wollte, dass ich mich verrennen würde, so habe ich dennoch das Argument nicht erkennen können: Welche Diagramme sollte ich nicht richtig verstanden haben? Und wie wären diese korrekt zu verstehen?
————– #107: NicoBaecker sagt:
Was Herr Kramm z.B. schreibt, ist richtig, …
… und zwar konkret?
————– #107: NicoBaecker sagt:
auch Herr Stehlik hat recht, wenn er schreibt, daß man die IR-Emissionspektren von jeden beliebigen Stoff (z.B. Gase) i.a. nicht hinreichend durch Planck-Strahler beschreiben kann.
Das war meine Aussage. Herr Dr. Stehlik schrieb dagegen in Antwort auf meinen Text:
————— #13: Dr. Gerhard Stehlik sagt:
(3) „Die unterschiedlichen Elemente lassen sich aus den Spektren im jeweiligen Bedingungskontexten hervorragend erkennen und interpretieren.“
Solche „unterschiedlichen Elemente“ sind im „Schwarzen Strahler“ nicht vorgesehen. Sie stehen im völligen Wiederspruch zur Formel von Max Planck (also nach Abb. 2.)
… und jedes hier nicht gezeigte IR-Spektrum eines beliebigen chemischen Stoffes rein gar nichts mit einem kontinuierlichen „Schwarzen Strahler“ nach Max Planck zu tun hat.
Sind sie ebenso wie Dr. Stehlik der Ansicht, dass das mit den Erkenntnissen der Physik im Einklang steht? Sollte man Planck wirklich in die Tonne treten? Das müsste man, wenn die Formael nichts mit den realen Stoffen zu tun hätte.
Im Übrigen haben sie in diesem Thread ja durchaus die kritische Konsequenz des Artikels zutreffend benannt. Mir bleibt darum ihre Aussage hier völlig unklar.
nochmal eine Bemerkung von mir als Naturwissenschaftler.Sie habe doch die Epizykel-Fits für die Planetenbewegung genannt. Bei der Untersuchung von Herrn Michelbach und anderen Untersuchungen über mögliche Korrelationen von Sonnenaktivität zum Klima handelt es sich ebenfalls um nichts anderes als Fits zwischen Beobachtungsdaten. Das Vorgehen da ist durchaus mit dem Ptolemäischen Ansatz vergleichbar. Die GCMs heben sich vo diesen Fits ab, indem sie auf Basis der Physik arbeiten. Sie können also leicht folgende Analogie machen:
A) Fit von Epizykeln an Planetenpositionsdaten = Korrelationen zwischen Sonnenaktivitätsdaten und Klimadaten
B) Himmelsmechanishce Berechnung der Planetenpositionen auf Basis der Physik (newtonsche Mechanik) = heutige Klimamodelle
Die Modellierung auf Basis physikalischer Gleichungen (B) (inkl. Parametrisierung komplexer Prozesse durch einfachere Ersatzprozesse) betrachte ich als Physiker als Fortschritt – auch erkenntnistheoretisch.
Wie sehen Sie das?
wenn ich mir Ihre Beiträge zum Stehlik, Eichten, Hüne-Artikel ansehe, frage ich mich, ob Ihr Problem vielleicht größtenteils darin liegt, daß Sie sich einfach verzetteln und nicht die Zeit nehmen, mal systematisch zu arbeiten. Sie haben nun eine Debatte mit vielen ausgelöst, nur weil Sie ein paar Diagramm nicht richtig verstehen und die Kommentare dazu auch noch misssverstehen. Was Herr Kramm z.B. schreibt, ist richtig, auch Herr Stehlik hat recht, wenn er schreibt, daß man die IR-Emissionspektren von jeden beliebigen Stoff (z.B. Gase) i.a. nicht hinreichend durch Planck-Strahler beschreiben kann. Der Haken bei seiner Argumentation liegt natürlich tiefer, indem er nämlich diese Behauptung den in seinem Artikel angegriffenen Herren unterstellt. Was m.E. eine absurde Unterstellung ist.
„Die mit heutiger Parametrisierung und grid-cell-Größe erzielbare Genauigkeit sehen Sie ja an den Ergebnissen der heutigen Modelle.“
Sie offensichtlich nicht, denn 2 bis 6 °F Differenz für einen 70-jährigen lokalen Vergleich zwischen gemessener Realtität (Hier tägliches TMAX) und dem Hindcast des CMIP5-Modells GFDL-ESM2G sagen alles über die Fähigkeiten dieser General-Circulation-Models (Earth-System-Models) aus = Dringend Verbesserungsbedürftig in der Auflösung. Beim Niederschlag sieht es noch schlechter aus.
Wie konnte eine Lufthansa-Crew beim Nachtflug den Airport von Manchester treffen?: „We are just passing the City of Bremen.“ Blick aus dem Fenster: Unter mir der Flugplatz Bremerhaven-Luneort. Abweichung der Positionsangabe von der Realität: 60 km. In etwa so sehen die Ergebnisse der GCM aus.
Hier gibt es eine Passage, in der Sie sich m.E. widersprechen. Vielleicht reflektieren Sie auch mal persönlich ob Ihnen noch klar ist, was die Punkte in dieser Diskussion mit mir eigentlich sein sollen, oder ob Sie lieber nur schwätzen wollen.
Ich nenne Ihnen meine Punkte, die ich von Ihnen wissen will.
a) Glauben Sie, daß sich GCMs grundlegend von anderen physikalischen Modellen zu komplexen Abläufen unterscheiden? Wenn ja, ganz konkret bitte mit Beispielen antworten, worin und warum? Denken Sie daran an meine Beispiele für andere physikalische Modelle (Atommodelle mit Hartree-Fock-Näherungen, Approximationen für die Störungsrechnung bei der Planetenbewegung durch Gauß (und viele andere Astronomen), Gebäudestatik durch finite Elemente o.ä., und die Ignoranz der Quantenmechanik und Relativitätstheorien dabei. Sie müssen Gründe nennen, die die Methodik bei GCM methodisch grundlegend von diesen Beispielen unterscheiden! Bedenken Sie: alle diese Beispiele parametrisieren ebenfalls komplexe Prozesse durch vereinfachende Ersatzprozesse.
b) Wenn Sie a) beneinen, glauben Sie, daß sich komplexe physikalische Prozesse grundsätzlich nicht befriedigend mit Modellen beschreiben lassen?
c) Wenn Sie auch b) verneinen, was ist dann Ihr Problem?
Nun zur Passage:
a)„———– #98: NicoBaecker sagt:
Über praktizierte wissenschaftliche Methoden, um eine konkrete wissenschaftliche Frage zu behandeln, kann man freilich erkenntnistheoretische Debatten starten. Aber solange diese Debatten keine konkreten Erkenntnisse zur behandelten wissenschaftlichen Frage liefern, sind sie für die Frage wissenschaftlich nicht zielführend.
Das ist ein Beispiel für das Ausblenden grundlegender Fragen. Aus der Wissenschaftstheorie und Erkenntnistheorie wissen wir, dass vieles, was man für selbstverständlich hält gar nicht selbstverständlich ist.“
Die grundlegende Frage beim Klimamodellieren ist das Klima quantitativ zu beschreiben (das ist die wissenschaftliche Fragestellung)! Und die ist jedem Modellierer bewußt. Das bitteschön wollen Sie denen doch wohl nicht absprechen. Wenn doch, so belegen Sie das.
Als selbstverständlich wird dabei angenommen, daß man die Naturgesetze auch im Klimasystem gelten. Dieselbe Selbstverständlichkeit hat auch der Modellierer der Baustatik oder für Atome!
Da also die Fragestellung klar ist, also nochmals meine Frage an Sie zurückgespielt: Was liefert die Erkenntnistheorie konkret zur Beantwortung der oben erneut ins Gedächtnis geholten wissenschaftlichen Fragestellung, was die Wissenschaft mit ihren eigenen Methoden (konkret hier physikalischen Modellen) nicht vermag?
Ich sehe da beim besten Willen nicht, wie die Erkenntnistheorie unterstützen kann, wenn ein Statikmodell die Statik eines Hauses vorrechnet. Auch die Überprüfung der Gültigkeitsgrenzen des Modells ist schließlich – wie schon mehrmals betont – in die Modellentwicklung integriert.
Ich habe den Eindruck, Sie verallgemeinern schlechte Erfahrungen, die Sie mit komplexen Projekten gemacht haben. Bedenken Sie, daß physikalische Modelle, die man nicht testet, wohl von jedem Modellierer als Schrott angesehen werden und daß dies kaum mit seinem Arbeitsziel vereinbar ist. Bedenken Sie auch, daß die Arbeitsteilung bei der Modellierung gar nicht so groß ist, wie Sie es vielleicht befürchten (Stichwort: Betriebsblindheit, so groß ist der „Ameisenhaufen“ gar nicht).
b)„———– #98: NicoBaecker sagt:
Sie können gerne an den anerkannten Verfahren, physikalische Modelle zu entwickeln, um komplexe Prozesse zu verstehen, herummäkeln. Aber solange Sie kein revolutionierend besseres Verfahren zur Erklärung komplexer Prozesse vorstellen, wird man daran festhalten.
Es geht nicht ums herummäkeln, sondern um die Unmöglichkeit, angesichts des mehrdimensionalen Nichtwissens, zuverlässige Ergebnisse zu erzielen. „
Schon, aber Sie haben bislang trotz Aufforderung überhaupt gar nicht gezeigt, daß dies bei den GCMs zutrifft. Wissen Sie denn überhaupt, was die Wissenschaft als „zuverlässiges Ergebnis“ eines heutigen GCMs einstuft? Und woher wollen Sie wissen, daß ein potentielles „mehrdimensionalen Nichtwissen“ überhaupt relevant ist?
Das hängt doch logischerweise davon ab, was man sich als Genauigkeitsziel des Modells vorgenommen hat, oder?
Nehmen Sie das Baustatikmodell. Die Baustatiker haben 10 verschiedene Modelle gebastelt, die Gebäudeschwankungen bei bestimmt definierten Testbedingungen zwischen 15 und 20 cm vorhersagen. Nun wird im Test bei diesen Bedingungen eine Schwankung von 20,5 cm gemessen. Sind die Modelle also alle für den Eimer? Vielleicht wissen die Modellierer sogar, warum Ihre Modelle dieses Testergebnis alle unterschätzen und vielleicht ist diese Abweichung sogar als potentieller „Schönheitsfehler“ in Kauf genommen worden, weil die Modelle dann 20 mal einfacher sind als eines, welches diese Abweichung „im Griff“ hat.
Nehmen Sie mal das von Ihnen zitierte paper von Mauritzen et al. Sie sehen da, welche Parameter zum Tunen benutzt wurden und wie sich diese in den verschiedenen Modellen unterscheiden. Das sind keine irrwitzigen Unterschiede und zudem keine physikalisch absurden Zahlen (ich habe den Eindruck, Sie wollten gerne unterstellen, Tunen würde physikalischem Unsinn gleichkommen, vielleicht erklären Sie sich mal explizit dazu, um mich von diesem Eindruck zu befreien). Die globale Mitteltemperatur liegen bis 1,2 K auseinander, als Klimasensitivität kommen Werte zwischen 2,8 bis 3,5 K raus. Sind diese Modelle nun „zielführend“ und genau genug oder nicht?
„Die Erkenntnis, dass ein Lösungsversuch eben nicht zielführend ist,“
Gut, aber woher wollen Sie wissen, ob das hier der Fall ist, wenn Sie das Ziel der nicht kennen? Sind Sie sich sicher, daß Sie es sich nicht zu einfach machen, und einfach die Möhre (das Ziel) einfach mal etwas höher hängen? Mit „moving targets“ kann an alles als „nicht zielführend“ umdefinieren.
c)„———– #98: NicoBaecker sagt:
Physikalische Modelle nutzt man in den Ingenieurswissenschaften wie auch sonst in der Physik, …
…. und das ist im Allgemeinen auch gut so.“
Wenn ich also diesen Kommentar mit Ihrem Kommentar zu a) kombiniere, heißt das, daß Sie es im Allgemeinen gut finden, wenn bei der Erstellung von physikalischen Modellen der Ingenieurswissenschaften und der Physik erkenntnistheoretische und grundlegende Fragen ausgeblendet werden… also wie nun? Es wird Zeit, daß Sie mal konkret werden und bitte die Fragen a) bis c) oben beantworten.
„———– #98: NicoBaecker sagt:
… die GCMs in den Klimawissenschaften sind methodisch in einer Linie mit diesen, und zu den Naturgesetzen, die wir heute kennen, gibt es keine Alternative Beschreibungen der Natur, die treffender wären.
Sie scheinen das Problem nicht verstanden zu haben. Wie können sie unter diesen Umständen überhaupt ein GCM beurteilen?“
Welche Umstände meinen Sie?
Ich habe doch schon weiter oben öfter beschrieben, wie ein GCM beurteilt wird. Was haben Sie daran auszusetzen?
Und welche bessere Alternative hätten Sie denn anzubieten – den Kopf in den Sand stecken oder Glaskugellesen?
„———– #98: NicoBaecker sagt:
Wenn Sie eine fundamentalistische Diskussion über die heutigen naturwissenschaftlichen Methoden eröffnen wollen, so tun Sie das. …
Es gibt einen massiven Unterschied zwischen fundamentalen und fundamentalistischen Argumenten:
Fundamentale Fragestellungen adressierend grundlegende Probleme. Fundamentalistisch ist, wenn man das Fundament eben nicht in Frage stellt, sondern als gegeben voraussetzt.“
Nun, ich meine wie gesagt eine fundamentalistische Diskussion über die heutigen naturwissenschaftlichen Methoden. Das Fundament wäre hier die Überzeugung, daß diese zu Erkenntnissen führen, die der „Wahrheit“ näherkommen. Ich als Naturwissenschaftler bin natürlich davon überzeugt, daß dem so ist.
Bei Ihnen weiß ist nicht, ob Sie auch dieses Fundament unter sich fühlen…
„Nochmals: Die Probleme von hochkomplexen Simulationen, in denen viele Zusammenhänge nicht testbar sind und Parameter abgeschätzt werden, hat nicht mit dem Grundtypus der Modellbildung im Allgemeinen zu tun. Das hätte ihnen, nachdem isch es ausdrücklich gefühlte 50 mal erklärt habe, eigentlich klar sein müssen.“
Klar, aber nun liefern Sie doch bitte auch den konkreten Beweis dafür, daß dies für die GCMs de facto auch zutrifft. Ich sehe dies überhaupt nicht! Ihr Mauritzen-Paper zeigt doch sehr schön, wie getunt wird. Was soll da nicht testbar oder gar „subtil“ abgeschätzt sein?
Nochmals: bei GCMs geht man nicht grundlegend anderes als bei anderen physikalischen Modellen vor. Ihre Erfahrungen mit irgendwelchen hochkomplexen Simulationen können Sie nicht einfach ohne konkrete Beweise verallgemeinern. Ich bitte Sie zudem, meine Fragen a) bis c) zu beantworten, um Mißverständnisse bei mir über Ihren Standpunkt ausschließen zu können.
„Wenn GCM mit Grid Cells arbeiten, deren Kantenlänge meist über 100 km sind, muss mit Durchschnittsannahmen gearbeitet werden, die auf mehr oder minder guten Schätzungen basiert. In wie weit man dann noch von realitätsnahen Abbildungen sprechen kann, die ein langfristiges Verhalten im Bereich von 0,01 Grad beschreiben, kann ich mir nicht vorstallen. Wie sollte man dies auch nur für eine einzige Zelle testen können? Die Inputs gehen über die Zellgrenzen hinaus und lassen eine Überprüfung der Modellierung gar nicht mehr zu.“
Der Punkt ist, daß Sie diese Genauigkeit ja nicht brauchen für die angestrebte bzw. heute überhaupt denkbare Genauigkeit von Klimamodellen. Das brauchen ja nicht mal Wettermodelle.
Die mit heutiger Parametrisierung und grid-cell-Größe erzielbare Genauigkeit sehen Sie ja an den Ergebnissen der heutigen Modelle.
den Unterschied zwischen induktiv und dekuntiv kenne ich. Die Naturgesetze werden induktiv entdeckt. Wie scon gesagt, physikalische Modelle, einschließlich der heutigen Klimamodelle sind aber logischerweise deduktiv: sie basieren nur auf die bekannten Naturgesetze.
„Wollte ich ihnen konkrete Erfahrungen erläutern, würden sie sie entweder ohne nähere Details nachvollziehen können, da sie bei ihrer selbstkritischen Betrachtung eigentlich die gleichen Erfahrungen gemacht haben müssten, oder aber sie verstehen dies mangels gemeinsamer Erfahrung trotz Erläuterungen nicht. “
Ich meinte konkrete Beispiele aus der Klimamodellforschung und nicht aus Ihrer Arbeitswelt, über die diskutieren wir ja nicht.
Zu Ihrer Textstelle:
„Wenn Sie schreiben: ‚Heutige Modelle in der Physik basieren immer aus Sätzen von physikalischen Gleichungen, die entweder direkt verwendet oder durch Approximationen allgemeiner Naturgesetze gewonnen werden.‘
… dann ist mir nicht klar, ob sie wirklich meinen, dass es sich bei den Modellen ausschließlich um die konsequente Anwendung der bekanntnen Naturgesetze wären und sonst nichts.“
Ja, aber der Punkt ist natürlich – und das ist nun wirklich wichtig zu begreifen – daß physikalische Modelle immer nur einen Teil der Gesamtheit aller Naturgesetze abbildet. Ich hatte Ihnen schon die Beispile der Statikmodelle für Gebäude oder Atommodelle genannt, und die GCMs betrifft dies auch. Der Ausgangspunkt ist die Untersuchung welche Naturgesetze einflußreich sind und welche weniger (quantitativ). Ein Modell berücksichtigt immer nur eine relevante Teilmenge der Naturgesetze oder parametrisiert komplexe Zusammenhänge durch Ersatzprozesse. Beispile dafür sind genannt: Gauß‘ Näherung für die Gravitationswirkung der Asterioden, die Vernachlässigung der RT bei der Statik, die Parametrisierung von sub-grid Prozessen bei GCMs, die Hartree-Fock-Näherungen bei Mehrelektronenatomen,…
„Denn wenn dies möglich wäre, würde es notwendig nur ein Klimamodell geben.“
Nein, denn es gibt viele mögliche Realisierungen physikalischer Modelle.
„Die Existenz unterschiedlicher Modelle zeigt aber, dass die modellierer Freiheitsgrade hatten, die eben nicht durch Naturgesetze determiniert sind.“
Es gibt die Freiheitsgrade durch die Vereinfachung der Prozesse. Wie gesagt, man muß natürlich testen, ob diese Näherungen gut genug sind, um den komplexen Prozeß zu ersetzen. Zur Verifikation dieser gehören auch Messungen.
Auch ich will ihnen ausdrücklich Respekt zollen. Auch wenn ich punktuell mit Ihrer Diktion nicht immer glücklich war, so erkenne ich an, dass sie vor allem einen sach- und argumentbezogenen Stil pflegen, und das ist weit mehr, als man von vielen Kommentatoren sagen kann. Auch wenn man unterschiedlicher Meinung ist, so kann man sich doch persönlich durchaus schätzen.
———– #98: NicoBaecker sagt:
Zum Thema „betriebsblind“, Sie schrieben „Eine Rückbesinnung auf die Grundkonzepte findet selten bis nie statt.“. Können Sie mal mal erklären, welche die „Grundkonzepte“ und die fehlende „Rückbesinnung“ bei den GCMs sind?
Eben das gesagte: Man will möglichst zutreffend die Realität nachbilden und somit das Verhalten der Sachverhalte verstehen. Dabei muss man sich stets der Grenzen des Vorhabens bewusst bleiben.
Ich habe den Eindruck, dass dies zwar, sofern es Thematisiert wird, auch weitgehend dissensfrei kommuniziert wird, aber in der Praktischen Arbeit wieder vollständig ausgeblendet wird.
———– #98: NicoBaecker sagt:
Und nennen Sie doch bitte die „grundlegende Irrtümer“, die man bei Modellen auf Basis physikalischer Gleichungen Ihrer Meinung nach annimmt. Ihr Vorwurf, daß die Modellierer „den Wald vor Bäumen nicht mehr sehen“ würden, müssen Sie mir nochmal erläutern.
Bitte auf die Diktion achten: Es gibt keine grundlegenden Probleme allgemeiner Art, wenn man physikalische Befindlichkeiten mit Gleichungen, sofern man sich des Abstraktionscharakters bewusst bleibt.
Grundsätzliche Probleme entstehen bei der Simulation komplexer Systeme im Gegensatz zu einfachen und testbaren Zusammenhänge. Denn komplexe Probleme, deren Einflussfaktoren sowohl qualitativ nicht vollständig bekannt sind, deren quantitative Effektkopplungen nicht geklärt ist und deren Rückkopplungsparameter in vielfältiger Weise als Funktion des Ortes und der Zeit verstanden werden müssen, lassen keine zuverlässige Abbildung mehr zu.
Wir können uns vielleicht vorstellen, was die Vorgänge über einem See bei Windstille, Temperatur 22 Grad, rel. Luftfeuchte 60% und wolkenloser Himmel und Sonnenstand 45 Grad ausmacht. Dann können wir halbwegs zuverlässig ausrechnen, wie hoch die Verdunstung und Konvektion sein kann. Aber schon bei variierenden Parametern, z.B. Wind und Wolkenbedeckung wird es ungleich schwieriger bis unmöglich, eine zutreffende kleinräumige Simulation zu fahren. Bei wechselnden Bodeneigenschaften um so mehr.
Wenn GCM mit Grid Cells arbeiten, deren Kantenlänge meist über 100 km sind, muss mit Durchschnittsannahmen gearbeitet werden, die auf mehr oder minder guten Schätzungen basiert. In wie weit man dann noch von realitätsnahen Abbildungen sprechen kann, die ein langfristiges Verhalten im Bereich von 0,01 Grad beschreiben, kann ich mir nicht vorstallen. Wie sollte man dies auch nur für eine einzige Zelle testen können? Die Inputs gehen über die Zellgrenzen hinaus und lassen eine Überprüfung der Modellierung gar nicht mehr zu.
Wenn ich in komplexen Projekten gearbeitet habe die ich nicht mehr überschauen kann. Konnte ich genau dann erfolgreich arbeiten, wenn ich mich auf meine Teilaufgabe konzentrierte. Also bewusst auf Betriebsblindheit hinsichtlich des Gesamtzieles schaltete. Und das ist schlicht ein Stück Notwendigkeit. Selbiges habe ich nicht nur bei mir, sondern ausnahmslos bei anderen Teammitgliedern beobachtet.
———– #98: NicoBaecker sagt:
Auch diesen Vorwurf werte ich als Vorwurf, die Leute seien zu blöde. Also zu blöde, selber zu sehen, was sie eigentlich machen und was sie daraus schließen dürfen. Bitte erläutern Sie Ihren Vorwurf mal an konkreten Beispielen.
Ich könnte derartige Erfahrungen zu Hauf berichten, müsste aber dazu zu tief ins Detail gehen. Zuweilen stellt man sich auch kritische Rückfragen, die an dem Sinn des Projektes oder der Tätigkeit zweifeln lassen. Aber was soll man damit anfangen? Zumeist kann man dies gar nicht operationalisieren.
Wollte ich ihnen konkrete Erfahrungen erläutern, würden sie sie entweder ohne nähere Details nachvollziehen können, da sie bei ihrer selbstkritischen Betrachtung eigentlich die gleichen Erfahrungen gemacht haben müssten, oder aber sie verstehen dies mangels gemeinsamer Erfahrung trotz Erläuterungen nicht. Falls ihnen diese Beobachtungen wirklich so fremd sind, würde mich ernsthaft interessieren, wie bei Ihren Arbeiten die Konzentration auf Sachthemen funktioniert, oder ob sie keine fundamentalen Zweifel kennen.
Darum erscheint mir der Vorwurf, irgend jemand sei blöde, weil er eben nur seinen Job tut, unpassend. Angesichts der umfassenden kritischen Design-Entscheidungen, die man bei der Bearbeitung Komplexer Projekte, wie z.B. einem GCM, ist es m.E. gar nicht möglich, ein konsistent zielkonformes Arbeiten in der realen Welt durchzuführen.
———– #98: NicoBaecker sagt:
Über praktizierte wissenschaftliche Methoden, um eine konkrete wissenschaftliche Frage zu behandeln, kann man freilich erkenntnistheoretische Debatten starten. Aber solange diese Debatten keine konkreten Erkenntnisse zur behandelten wissenschaftlichen Frage liefern, sind sie für die Frage wissenschaftlich nicht zielführend.
Das ist ein Beispiel für das Ausblenden grundlegender Fragen. Aus der Wissenschaftstheorie und Erkenntnistheorie wissen wir, dass vieles, was man für selbstverständlich hält gar nicht selbstverständlich ist.
Wenn also das Ergebnis der Erkenntnistheorie Zweifel an den Methoden belegt, mag man das als Störfaktor schlicht ausblenden. Und dann haben wir bereits eine Triebkraft der Betriebsblindheit.
———– #98: NicoBaecker sagt:
Sie können gerne an den anerkannten Verfahren, physikalische Modelle zu entwickeln, um komplexe Prozesse zu verstehen, herummäkeln. Aber solange Sie kein revolutionierend besseres Verfahren zur Erklärung komplexer Prozesse vorstellen, wird man daran festhalten.
Es geht nicht ums herummäkeln, sondern um die Unmöglichkeit, angesichts des mehrdimensionalen Nichtwissens, zuverlässige Ergebnisse zu erzielen. Die Erkenntnis, dass ein Lösungsversuch eben nicht zielführend ist, ist dann eben hilfreicher als ein ‚weiter so‘ ohne begründete Aussicht auf Erfolg. Sie liefern gerade hervorragende Gründe der Betriebsblindheit.
———– #98: NicoBaecker sagt:
Physikalische Modelle nutzt man in den Ingenieurswissenschaften wie auch sonst in der Physik, …
…. und das ist im Allgemeinen auch gut so.
———– #98: NicoBaecker sagt:
… die GCMs in den Klimawissenschaften sind methodisch in einer Linie mit diesen, und zu den Naturgesetzen, die wir heute kennen, gibt es keine Alternative Beschreibungen der Natur, die treffender wären.
Sie scheinen das Problem nicht verstanden zu haben. Wie können sie unter diesen Umständen überhaupt ein GCM beurteilen?
———– #98: NicoBaecker sagt:
Wenn Sie eine fundamentalistische Diskussion über die heutigen naturwissenschaftlichen Methoden eröffnen wollen, so tun Sie das. Aber bitte seien Sie dann auch so
konsequent und greifen es allgemein an, denn diese Diskussion würde nicht nur die Klimamodelle infrage stellen, sondern jedes physikalische Modell: von Modellen zur
Berechnung von Elektronenniveaus in Atomen bis hin zur Statik von Gebäuden.
Es gibt einen massiven Unterschied zwischen fundamentalen und fundamentalistischen Argumenten:
Fundamentale Fragestellungen adressierend grundlegende Probleme. Fundamentalistisch ist, wenn man das Fundament eben nicht in Frage stellt, sondern als gegeben voraussetzt.
Nochmals: Die Probleme von hochkomplexen Simulationen, in denen viele Zusammenhänge nicht testbar sind und Parameter abgeschätzt werden, hat nicht mit dem Grundtypus der Modellbildung im Allgemeinen zu tun. Das hätte ihnen, nachdem isch es ausdrücklich gefühlte 50 mal erklärt habe, eigentlich klar sein müssen.
———– #98: NicoBaecker sagt:
„Hier unterliegen sie einem methodischen Fehler.“ Welcher da wäre?
Ich habe gar nicht in induktiv und deduktive unterschieden. Wenn Ihnen das aber wichtig ist, erkläre ich es Ihnen: Physikalische Modelle sind offensichtlich deduktiv, die GCMs eingeschlossen, denn sie basieren auf bekannten und erprobten Gesetzen.
Wenn Sie schreiben: ‚Heutige Modelle in der Physik basieren immer aus Sätzen von physikalischen Gleichungen, die entweder direkt verwendet oder durch Approximationen allgemeiner Naturgesetze gewonnen werden.‘
… dann ist mir nicht klar, ob sie wirklich meinen, dass es sich bei den Modellen ausschließlich um die konsequente Anwendung der bekanntnen Naturgesetze wären und sonst nichts. Denn wenn dies möglich wäre, würde es notwendig nur ein Klimamodell geben. Die Existenz unterschiedlicher Modelle zeigt aber, dass die modellierer Freiheitsgrade hatten, die eben nicht durch Naturgesetze determiniert sind. Wem dieser Sachverhalt offensichtlich ist, wird meinen, Sie wollten hier jemanden hinters Licht führen.
Das es sich natürlich auch um Naturgesetze und Gleichung handelt, die zum Einsatz kommen, wirkt so, dass sie stets betonen, dass ein Auto aus Blech besteht. Wer ihnen dann sagt, dass dies zwar eine notwendige, aber nicht hinreichende Bedingung sei, möchte nicht weitere Referate über die Rolle des Blechs im Automobilbau hören.
Hinsichtlich der Bedeutung von induktiven und deduktiven Arbeiten empfehle ich zur Einführung http://tinyurl.com/j2kb4lo und http://tinyurl.com/hbw5fvm – dann ersparen sie sich auch das Kopfschütteln bei ihren Lesern.
Aus jedem Ihrer Sätze geht hervor, dass Sie weder von Wissenschaftstheorie noch von Erkenntnistheorie auch nur eine ungefähre Ahnung haben.
Was soll hier irgend jemand mit einem argumentfreiem Satz wie diesem anfangen? Wollen sie eine Demonstration geben, was ad hominem Attacken sind? Dies gibt es ansonsten bereits zur genüge und sind darum vollständig überflüssig.
————- #97: Franz Zuber sagt:
Sie sitzen einem grotesken fundamentalem Irrtum auf: Wenn Sie nämlich sagen, Sie hätten etwas gelesen (G&T, Wissenschaftstheorie, Erkenntnistheorie) glauben Sie sofort, auch wirklich verstanden zu haben, was Sie da gelesen haben. Das stimmt aber natürlich nicht.
Sind sie sich sicher, dass Sie nicht von sich selbst reden? Das Fehlen jeglicher fachlichen Aussage belegt diese Vermutung.
————- #97: Franz Zuber sagt:
Zur Ihrer eigenen Überprüfung Ihrer Inkompetenz stelle ich Ihnen nachfolgend …
Sie erwarten doch wohl nicht wirklich, dass ich nach dieser dummdreisten Rüpelei auf derartiges Ansinnen eingehe?
Bitte an den Admin: M.E. genügt dieser Beitrag nicht den Nutzungsbedingungen der Kommentarfunktion. Ich wäre Ihnen dankbar, wenn sie diesen löschen würden.
3. Forts.:
„Denn auch wenn man einige Einflüsse auf das Klimasystem identifiziert, fällt sowohl dessen Quantifizierung in der Wechselwirkung schwer, als auch die Frage, ob man überhaupt alle Parameter hinreichend kennt.“
Gut, Sie postulieren da unbekannte Wechselwirkungen und die damit einhergehende Unsicherheit. Aber ich schrieb davon, daß die bekannten und im Modell berücksichtigen Gesetzmäßigkeiten die langfristige Klimadynamik bereits so genau reproduziert, daß man ihren Einfluß nicht ignorieren darf.
„Ferner stellt sich die Frage ob bei zyklischen und quasizyklichen Einflüssen diese adäquat darstellen lassen. Aber durch die gegenseitigen Rückkopplung wird die Simulation eigentlich nicht mehr sicher lösbar. Und genau das zeigen auch die Modellierungsergebnisse.“
Sie haben da eine völlig falsche Vorstellung von den Modellen. Ich schrieb doch schon, daß die interne Dynamik berücksichtigt wird [c)], damit zeigen GCMs grundsätzlich auch die Quasi-zyklen. Aber offensichtlich nicht hinreichend genau, sei es an der grid-Größe, unzureichender sub-grid Parametrisierung oder zu ungenauer oder wenig Kalibrierdaten aus Beobachtungen.
Aber dieser Mangel dann doch nicht dafür blind manchen, daß die langfristigen Klimaschwankungen durch externe Klimafaktoren so genau reproduziert werden, daß man deren Einfluß nicht ignorieren kann!
Ich schrieb:
„Von nichts anderem. Wenn sich also das Modellklima 2015 wesentlich von dem in 1780 unterscheidet, so liegt das nur an der Wirkung von b) und c)!“
Ihre Antwort:
„Das kann zu recht und begründet bezweifelt werden. „
Nein, in diesem Fall ist das so, weil die GCM-Modelle so aufgebaut ist, siehe oben. Es gibt im Modell ja nur a), b) und c) und kein d) etc. Was nicht drin ist, kann nichts verursachen.
„Woher will man in einem rückgekoppelten System wissen, dass es gerade trendneutrales Rauschen überhaupt gibt? Das man dieses ohne detaillierte Modellierung den Quantifizieren kann und dass es nicht weitere, bislang unbekannte Faktoren gibt?“
Ok, Sie spekulieren, ob nicht die internen Wechselwirkungen auch auf längerfristiger Zeitskala Amplituden liefern können, die vergleichbar sind mit dem Signal durch die externen Faktoren. Aber dann müßten ja gleich zwei Dinge im Modell falsch sein: einmal unterschätzt das Modell die internen Schwankungen c) und zweites überschätzt den Einfluß der externen b). Aber nun erklären Sie mir mal, wie das gehen soll, die Gleichungen für beide sind doch die gleichen?
„Viel mehr glaube ich, dass GCMs wegen einer Vielzahl von Unbekannten und Irrtümern nicht ihrem Ziel gerecht werden können.“
Was wäre denn das Ziel? Vielleicht wollen Sie etwas mit den Klimamodellen etwas erreichen, was überhaupt nicht möglich ist.
„Sicher kann man diesen auch diskutieren und ihn ablehnen, aber es fällt schwer, diese Erklärung wirksam zu wiederlegen. Vielmehr erscheint sie auch aufgrund der Beobachtungen sehr viel plausibler als die Standard-These des CO2 als Main-Forcing.“
Warum? Der Punkt ist doch daß man die Physik des CO2 nicht einfach ignorieren darf. Das wäre eine unzulässige Manipulation. Denn man weiß, daß der Anstieg des CO2 zu einer quantitativen Auswirkung in der Energiebilanz führt, die nicht vernachlässigbar ist. Hingeben gibt es bis heute keine anerkannte Kausalkette zwischen Sonnenzyklus und interner Klimavariabilität. Nennen Sie mir einen Grund warum ich als Naturwissenschaftler bekannte naturgesetzliche Zusammenhänge verleugnen sollte und durch spekulative Zusammenhänge, deren Kausalkette weder qualitativ noch quantitativ geschlossen ist, eintauschen, das ist doch Wahnwitz.
Zudem: Sehen Sie in den unbekannten solaren Zusammenhängen mit dem Klima nun ein Revolution in der Physik anbahnen? Ich nicht, denn noch gründet man diese Zusammenhänge -spekulativ, wenig stringent und lückenhaft- auf die „konventionelle“ Physik von heute.
„Was darin als extern oder intern bezeichnet werden soll, ist vor allem die Frage, wo man die Modellgrenzen zieht.“
Nun, wie gesagt, die externen Klimafaktoren wie die Beispiel oben werden durch die Messdaten derselben vorgeschrieben und sind nicht Teil des Modelloutputs. Man kann das Modell natürlich auch erweitern. Aber es macht wohl kaum Sinn, Vulkanausbrüche oder den anthropogenen CO2 Ausstoß in Modellen ausrechnen zu lassen. Das wäre schwieriger als das Klima zu modellieren und auch nicht zielgebend.
„Dass sie diese Größen [Wolkenbedeckung, Wasserdampf-Anteil] als bloßes Rauschen herunter spielen wollen“
Na, das haben Sie wieder nicht verstanden. Ich habe die interne Klimavariabilität als Rauschen bezeichnet. Aber assoziieren Sie damit keine falschen Vorstellungen, insbesondere wäre die Vorstellung falsch, man würde dies „künstlich“, also statistisch dazumischen. Auch die interne Klimavariabiliät ergibt sich aus dem Lösen deterministischer Gleichungen.
Wolkenbedeckung und Wasserdampf sind hingegen Parameter, die Modelloutput sind!
„Nochmals: Die Gleichungen, die entwickelt wurden, liefern offensichtlich stark disperse Ergebnisse, die angesichts der nachgewiesen Schwankungsbreite überwiegend falsch sein müssen.“
Sie schulden mir immer noch den Beweis. Sehe ich anhand der Literatur nicht. Das Klimasystem mit den Merkmalen wird in groben Zügen reproduziert. Diese sind zumindest so gut, daß man damit die Auswirkung grober Einflüsse (z.B. CO2-Verdopplung) so genau ermitteln kann, daß man davor warnt.
„Sowohl, als auch: Das Verhältnis von verfügbaren Daten, bekannten Zusammenhängen und Erkenntnis relevanter Faktoren zu unbekannten Sachverhalten ist bei Weitem (!) so, dass eine realitätsnahe Klimamodellierung gar nicht möglich ist.“
Was ist denn eine „realitätsnahe Klimamodellierung“? Klar, genauer ist immer schöner, aber deswegen ist das Erreichte ja nicht zu verwerfen.
2.Forts.:
„Also: Was einst als festes Naturgesetz galt, hat sich stark als Sonderfall erwiesen.“
Klar, aber nach wie vor so genau wie damals schon. Die RT und QM verfeinern ja nur die Genauigkeit, die klassische Mechanik ist ein Spezialfall davon, der für gröbere Genauigkeiten aber keine anderen Ergebnisse liefert. Das ist der Punkt: Es ist völlig überzogen, z.B. Gebäudestatik nach RT und QM zu berechnen, weil schon mit klassischer Mechanik eine völlig ausreichende Genauigkeit geliefert wird.
„Bei der Klimaforschung erwarte ich aber keine neuen Naturgesetze, sondern nehme zu Kenntnis, dass sich eine Vielzahl von Effekten in einem lokal sehr unterschiedlichen Bedingungkontext zu unüberschaubaren Wirkungen zusammenfügt.“
Ist mir zu spekulativ und emotional. Ein konkretes Beispiel bitte, mit Zahlen am besten. Die Kunst bei jedweder Modellbildung ist natürlich, daß man vorher quantitativ abschätzt, welche Effekte überhaupt zu welchen Auswirkungen führen können, und dann ins Modell die wichtigsten aufnimmt. Darauf basiert ja auch die Praxis, daß man Gebäudestatik nicht mit der RT macht, sondern nach klassischer Mechanik.
„Klimamodellierer gehen dagegen von der Annahme aus, dass sie mikroskopisch chaotische Systeme statistisch hinreichend präzise beschreiben können – einschließlich aller Rückkopplungen. Ich halte das gerade bei Durchsicht der bisherigen Ergebnisse für eine methodische Vermessenheit.“
Nennen Sie mal ein konkretes „mikroskopisch chaotische System“, welches in GCMs „statistisch“ beschrieben wird?
Ich nenne Ihnen mal eines, was Ihrer Beschreibung m.E. gerecht wird (wobei ich nicht genau weiß, ob Sie das wirklich so meinen, das müssen Sie mir wirklich erläutern): Ein 1 ccm Gasvolumen aus Atomen. Die Bewegung jedes einzelnen Atoms ist nicht erfassbar, die Bewegungen sind chaotisch. Die Physiker haben jedoch für die meisten Anwendungsfälle kein Problem damit, das Atomensemble nur statistisch zu beschreiben, indem sie dem Ensemble Temperatur, Druck, Dichte zuschreiben. Das ist für die meisten Anwendungen völlig ausreichend.
Auch da gilt wieder: wenn die Genauigkeit ausreichend ist, kann man komplexe Prozesse vereinfacht abbilden, auch z.B. statistisch. Das muß man im Einzelfall prüfen.
„Sie bleiben bei Ihrem Fehler, mit dem sie qualitative Unterscheide erfinden, wo keine sind. Auch Gauß hätte beim Kenntnisstand der Epizyklen-Modellierer wohl kaum eine andere Lösung gefunden.“
Wenn ich Sie da richtig verstehe, glauben Sie, daß man bei der Klimamodellierung heute quasi nur mit einer modernen Form von „Epizykeln“ arbeitet, aber eigentlich eine wissenschaftliche Revolution dafür bräuchte, die neue Naturgesetze bringe. So wie die Epizykeln durch die Newtonsche Mechanik abgelöst wurden, die kausal und universeller war als die schnöden Fits durch Epizykel. Ist das der Kern Ihres Vergleiches?
Ich schrieb:
„Wie ich bereits mehrmals erklärt habe, gibt man dem Modell externe Klimafaktoren in ihrer beobachteten Zeitabhängigkeit vor (Solare Einstrahlung, Treibhausgasverlauf, Aerosoleintrag durch Vulkane und anthropogen, etc.). Die Klimaschwankungen auf der Zeitskala einiger Jahrzehnte werden damit alleine schon reproduziert.“
Sie antworten:
„Eigentlich nicht.“
Aber Sie sehen doch die Ergebnisse der Modelle im Vergleich zu den Messungen der letzten 130 Jahre. Den Temperaturverlauf kann man physikalisch nicht ohne die externen Klimafaktoren hinreichend gut reproduzieren. Worauf gründet sich Ihre Aussage?
1. zunächst einmal will ich Ihnen sagen, daß mir die Diskussion mit Ihnen gefällt, denn Sie brechen nicht in emotionale Ausbrüche aus und bleiben sachlich. Da das hier leider selten ist, was andernorts selbstverständlich ist, sollte dies mal betont werden.
Zum Thema „betriebsblind“, Sie schrieben „Eine Rückbesinnung auf die Grundkonzepte findet selten bis nie statt.“. Können Sie mal mal erklären, welche die „Grundkonzepte“ und die fehlende „Rückbesinnung“ bei den GCMs sind? Und nennen Sie doch bitte die „grundlegende Irrtümer“, die man bei Modellen auf Basis physikalischer Gleichungen Ihrer Meinung nach annimmt. Ihr Vorwurf, daß die Modellierer „den Wald vor Bäumen nicht mehr sehen“ würden, müssen Sie mir nochmal erläutern. Auch diesen Vorwurf werte ich als Vorwurf, die Leute seien zu blöde. Also zu blöde, selber zu sehen, was sie eigentlich machen und was sie daraus schließen dürfen. Bitte erläutern Sie Ihren Vorwurf mal an konkreten Beispielen. Über praktizierte wissenschaftliche Methoden, um eine konkrete wissenschaftliche Frage zu behandeln, kann man freilich erkenntnistheoretische Debatten starten. Aber solange diese Debatten keine konkreten Erkenntnisse zur behandelten wissenschaftlichen Frage liefern, sind sie für die Frage wissenschaftlich nicht zielführend.
Sie können gerne an den anerkannten Verfahren, physikalische Modelle zu entwickeln, um komplexe Prozesse zu verstehen, herummäkeln. Aber solange Sie kein revolutionierend besseres Verfahren zur Erklärung komplexer Prozesse vorstellen, wird man daran festhalten. Physikalische Modelle nutzt man in den Ingenieurswissenschaften wie auch sonst in der Physik, die GCMs in den Klimawissenschaften sind methodisch in einer Linie mit diesen, und zu den Naturgesetzen, die wir heute kennen, gibt es keine Alternative Beschreibungen der Natur, die treffender wären.
Wenn Sie eine fundamentalistische Diskussion über die heutigen naturwissenschaftlichen Methoden eröffnen wollen, so tun Sie das. Aber bitte seien Sie dann auch so
konsequent und greifen es allgemein an, denn diese Diskussion würde nicht nur die Klimamodelle infrage stellen, sondern jedes physikalische Modell: von Modellen zur
Berechnung von Elektronenniveaus in Atomen bis hin zur Statik von Gebäuden.
„Hier unterliegen sie einem methodischen Fehler.“ Welcher da wäre?
Ich habe gar nicht in induktiv und deduktive unterschieden. Wenn Ihnen das aber wichtig ist, erkläre ich es Ihnen: Physikalische Modelle sind offensichtlich deduktiv, die GCMs eingeschlossen, denn sie basieren auf bekannten und erprobten Gesetzen. Es gibt wohl keinen Naturwissenschaftler, der glaubt, man bräuchte zum Verständnis des Klimas bislang unentdeckte physikalische Gesetze. Sie schrieben:
„Das, was wir heute Naturgesetze nennen, war keineswegs stets Wissen der Menschheit. … Das mechanische Weltbild wurde mit der Entwicklung der modernen Naturwissenschaften allmählich zum Standard. Laplace drückte es am markantesten aus.
Dieses Weltbild bekam aber mit den Relativitätstheorien bereits erhebliche Risse und wurde durch die Quantenmechanik weitgehend auf den Kopf gestellt. Einige Menschen wollen dies auch nach hundert Jahren nicht zu Kenntnis nehmen.“
Das ist populistischer Quark, so einfach war die Wissenschaftshistorie nicht. Das „mechanistische Weltbild“ war ein „Ideal“ Anfang es 19. Jahrhdts.: Man hatte die Hoffnung, daß sich auch irdische Bewegungsprozesse so elegant und exakt wie die Planetenbewegung vorhersagen lassen. Aber zur gleichen Zeit, wurden dieselben Forscher
mit neuen Entdeckungen wie der Elektrizität konfrontiert. Das mechanistische Weltbild war niemals in Stein gemeißelt. Was da „Risse“ durch RT und QM bekam wäre dann auch nicht das mechanistische Weltbild, sondern die klassische Physik knapp 100 Jahre später.
Die „Risse“ haben auch nur die zu beklagen, die unbedingt die Physik abschließen wollen und ein Weltbild damit gründen wollen. Für die Physik sind solche „philosophischen Überbauten“ völlig irrelevant, denn die Laplacesche Himmelsmechanik ist heute so gültig und genau wie sie immer schon war. Durch die Relativitätstheorien (RT) und Quantenmechanik (QM) hat sich daran nichts geändert, diese beiden Theorien beschreiben ja Effekte, mit denen man zu Laplace Zeiten gar
nicht durch empirische Beobachtungen konfrontiert war.
Aufhören, bitte!!! Das ist ja nicht auszuhalten, was Sie da im Zwiegespräch mit „NicoBäcker“ so an Halbgarem (z.B: „Bei der Klimaforschung erwarte ich aber keine neuen Naturgesetze, sondern nehme zu Kenntnis, dass sich eine Vielzahl von Effekten in einem lokal sehr unterschiedlichen Bedingungkontext zu unüberschaubaren Wirkungen zusammenfügt. Klimamodellierer gehen dagegen von der Annahme aus, dass sie mikroskopisch chaotische Systeme statistisch hinreichend präzise beschreiben können – einschließlich aller Rückkopplungen. Ich halte das gerade bei Durchsicht der bisherigen Ergebnisse für eine methodische Vermessenheit.“) und Banalem (z.B.: „Was einst als festes Naturgesetz galt, hat sich stark als Sonderfall erwiesen. Es ist keineswegs klar, dass unsere aktuelle Naturerkenntnis und der vermuteten Naturgesetze bereits absolut zu verstehen sind. Vielmehr sind auch fundamentale Änderungen in der Zukunft zu erwarten, die wir heute gar nicht kennen können.) so herauslassen.
Aus jedem Ihrer Sätze geht hervor, dass Sie weder von Wissenschaftstheorie noch von Erkenntnistheorie auch nur eine ungefähre Ahnung haben. Sie sitzen einem grotesken fundamentalem Irrtum auf: Wenn Sie nämlich sagen, Sie hätten etwas gelesen (G&T, Wissenschaftstheorie, Erkenntnistheorie) glauben Sie sofort, auch wirklich verstanden zu haben, was Sie da gelesen haben. Das stimmt aber natürlich nicht. Genausogut nämlich können Sie sich eine chinesische Zeitung kaufen und konzentriert auf die chinesichen Schriftzeichen starren und nachher sagen, Sie hätten das gelesen. Nur verstanden haben Sie dann trotzdem nichts.
Zur Ihrer eigenen Überprüfung Ihrer Inkompetenz stelle ich Ihnen nachfolgend ein Statement von Schellnhuber vor und fordere Sie auf, mir angelegentlich mitteilen zu wollen, warum der Ausspruch von Schellnhuber wissenschaftstheoretisch gesehen einen veritablen Bruch mit echter Wissenschaft darstellt, der belegt, dass auch Schellnhuber von Wissenschaftstheorie offensichtlich wenig Ahnung hat:
In einem Interview mit der Zeit sagte nämlich Schellnhuber:
„… Heute haben wir in der wissenschaftlichen Analyse drei grosse Domänen: das Experiment, die Theorie und die Simulation. Simulation baut jedoch massiv auf Theorie and Praxis auf: Wir brauchen nämlich möglichst gute Daten, um die Anfangs- und Randbedingungen zu bestimmen, und eine Theorie, um die Prozesse zu erklären und zu beschreiben. Die Simulation wird in Zukunft aber wohl die Vorhersage-Lufthoheit erringen …“
Erkennen Sie, was da nicht stimmt bei Schellnhuber?
Sie verwenden –wie üblich– viele Worte um nichts Auszusagen. Ich beziehe mich hier mal nur auf Ihren ersten Absatz.
„Als erstes nochmal zur Betriebsblindheit. Bei mir als Naturwissenschaftler wirkt Ihr Vorwurf der Betriebsblindheit gleichwertig mit Blödheit. Aus dem einfachen Grund: Naturwissenschaftler wollen wissen, wie die Natur funktioniert. Der Sinn des Klimamodellierens ist es, das Klima quantitativ zu verstehen und wesentliche Charakteristika des Klimasystems mit Hilfe von mathematischen Gleichungen beschreiben zu können. Dies gilt für jedes Modell in der Physik. Wenn Sie einem Modellierer Betriebsblindheit vorwerfen, so werfen Sie ihm vor, daß er sein Ziel, nämlich die Natur zu verstehen, nicht mehr verfolgen würde. Was Sie also vorwerfen, wäre eine Unfähigkeit zur Selbstreflexion des eigenen Tuns. Ein solcher Naturwissenschaftler wäre schon blöde.“
Es gibt ja bekanntlich zum Einen die ergebnissoffene Forschung, und zum Anderen die zielorientierte Forschung. Bei der ersten Variante würde ich nicht unbedingt Blödheit unterstellen, wenn sich ein Wissenschaftler in eine Idee verrannt hat und nicht mehr so ohne weiteres aus seiner persönlichen Sackgasse herausfindet. Bei der zweiten Variante ist es schon etwas differenzierter, insbesondere wenn Bezahlung, Folgeaufträge, Ruhm und Ehre von der Zielerreichung abhängig sind. Blöd ist dabei eigentlich nur, wenn man dabei nur den kurzfristigen Erfolg vor Augen hat. Ansonsten bedarf es hier schon einer gewissen Raf(f)inesse um die gewünschten Ergebnisse zu liefern und den Geldhahn weiter sprudeln zu lassen.
Ein schönes Beispiel liefert geradezu die Klima-Alarm-Forschung. Mit der Annahme daß die berechnete CO2-Klimasensivität der Realität entspricht, kann man keinen Blumentopf gewinnen. Dazu mußte man dann schon eine gehörige Portion positive Rückkopplungen (er)finden um dem Ziel nahe zu kommen. Mit der Bandbreite der Phantasie ist es dann auch möglich, die Szenarien mediengerecht aufzupeppen. Gegrüßet sei PIK und Co.
Herr Baecker ich kann Sie und Ihre Kollegen nicht davon entbinden, die politisch forcierte Volksverar…e mit zu unterbinden. Wenn Sie die Ehre der Wissenschaft retten wollen sind Sie geradezu verpflichtet, mit uns den medialen Blödsinn aufzudecken und anzuprangern. Aber nach Ihrem permanenten Geschreibsel zu Urteilen sehen Sie dafür keine Notwendigkeit.
Hier die versprochene Fortsetzung:
————– #92: NicoBaecker sagt:
Meine Argumentation zielt aber an der Reproduzierbarkeit der“ großen“ Klimaschwankungen ab, die also mehrere Jahrzehnte überdecken und höhere Amplitude haben. Diese sind nicht mit der internen Klimadynamik alleine erklärbar, sondern werden maßgeblich durch den Zeitverlauf der externen Klimafaktoren bestimmt.
Eine These, die zu belegen wäre. Sie kennen die Artikel auf EIKE wahrscheinlich gut genug. Hier möchte ich auf den Ansatz von Prof. Dr. Carl-Otto Weiss und Lüdecke verweisen. Hier wird von einer ähnlichen Vermutung ausgegangen, dass es eben unterschiedliche längerfristige Zyklen als Einfluss gäbe. Im Gegensatz zu den meisten Arbeiten hinsichtlich GCMs geht man aber von starken Einflüssen der Sonnenzyklen, im Besonderen der „De Vries / Suess Zyklus“, aus.
Sicher kann man diesen auch diskutieren und ihn ablehnen, aber es fällt schwer, diese Erklärung wirksam zu wiederlegen. Vielmehr erscheint sie auch aufgrund der Beobachtungen sehr viel plausibler als die Standard-These des CO2 als Main-Forcing.
Was darin als extern oder intern bezeichnet werden soll, ist vor allem die Frage, wo man die Modellgrenzen zieht.
————– #92: NicoBaecker sagt:
Wenn ich Ihnen oben schrieb, daß GCMs heute nicht gefittet oder getuned werden, so bezog ich mich darauf, daß dieses tuning/fitting keinen maßgeblichen Einfluß auf den groben Klimaverlauf haben. Das von Ihnen zitierte tuning bezieht sich darauf, daß man die Modelle eben auch zum Reproduzieren der kürzerfristigen Schwankungen tunen will. Sie sehen ja an den Werten, daß diese nicht physiklisch unsinnige Werte annehmen, sondern nur im Bereich der Meßgenauigkeit getuned werden und um subgrid-Prozesse, die also physiklaisch nicht aufgelöst sind, zu simulieren.
Genannt wurde vor allem die Wolkenbedeckung. Sie ist ein maßgeblicher Einflussfaktor. Sowohl deren Ursache ist nicht hinreichend bekannt, noch sind die historischen Aufzeichnungen hinsichtlich der Statistik geeignet, Ihre Behauptungen zu unterstützen. Denn die Wolkenbedeckung nimmt mit steigender Durchschnittstemperatur nicht zu, ebenso wenig wie der Wasserdampf-Anteil in der Atmosphäre.
Dass sie diese Größen als bloßes Rauschen herunter spielen wollen, ist aber vermutlich weit mehr dem Bias geschuldet als den belegbaren Sachverhalten. Angesichts massiver Unsicherheiten der Zusammenhänge kann die Ignoranz vor Allem mit einer Betriebsblindheit erklärt werden.
————– #92: NicoBaecker sagt:
Um nochmal zu Ihrem Beispiel Ptolemäus zurückzukommen: Das GCM-tuning ist nicht vergleichbar mit eim Epizykelmodell. Das Epizykel-Modell war völlig ohne Naturgesetze. GCMs basieren jedoch auf verifizierten physikalischen Gleichungen.
Nochmals: Die Gleichungen, die entwickelt wurden, liefern offensichtlich stark disperse Ergebnisse, die angesichts der nachgewiesen Schwankungsbreite überwiegend falsch sein müssen. Der Anspruch, das reale Klimasystem damit hinreichend zu beschreiben, kann damit als widerlegt angesehen werden.
————– #92: NicoBaecker sagt:
Meine Frage an Sie ist nun: Was kritisieren Sie nun an den GCMs eigentlich? Die Mängel beim Reproduzieren der internen Klimaschwankungen (des Klimarauschens) wie AMO, NAO auf der Zeitskala von mehreren Jahren oder die Reproduzierbarkeit der Klimaentwicklung aufgrund der externen Klimafaktoren auf der Zeitskala von mehreren Jahrzehnten?
Sowohl, als auch: Das Verhältnis von verfügbaren Daten, bekannten Zusammenhängen und Erkenntnis relevanter Faktoren zu unbekannten Sachverhalten ist bei Weitem (!) so, dass eine realitätsnahe Klimamodellierung gar nicht möglich ist. Die Ergebnisse bleiben somit in einer Modellwelt, die nicht zu Recht beanspruchen kann, zutreffende Aussagen über die Realität zu machen
Hier die versprochene Fortsetzung:
————– #92: NicoBaecker sagt:
Dennoch kann das physikalische Modell Vereinfachungen haben (z.B. nicht die Gravitationswirkung aller Kleinkörper im Asteroidengürtel berücksichtigen), dies kann dazu führen, daß Sie „tunen“ müssen, indem Sie z.B. die Gravitationswirkung der vielen kleinen Körper des Asteroidengürtel als ein „Rauschen“ berücksichtigen, welches mit viele weniger Parametern beschrieben werden kann als alle Kleinkörper zusammen. Aber diesen „subgrid“-Prozess als vereinfachter Ersatz der Summe der Gravitationswirkung der vielen kleinen Körper müssen Sie erstmal entwickeln, denn der stellt ja nur eine Vereinfachung der Realität dar und ersetzt diese nur. D.h. Sie müssen diesen Ersatzprozess anhand der wirklichen Wirkung der Kleinkörper tunen.
Modellvereinfachungen sind erkenntnistheoretisch erforderlich und nicht grundsätzlich abzulehnen. Dennoch muss bei jeder derartigen Vereinfachung geprüft werden, ob diese überhaupt zulässig ist – Im Besonderen zur Beantwortung der Fragestellungen. Nicht jede beliebige Vereinfachung ist auch zulässig.
————– #92: NicoBaecker sagt:
C.F. Gauß hat so etwas übrigens schon um die 1809 für die Gravitationswirkung der Kleinkörper gemacht. Dieses „tunen“ ändert aber nichts daran, daß das Modell auf physikalischen Gleichungen basiert. Also verwechseln Sie das nicht: Diese Art „tunen“ ist nicht gleichzusetzen mit dem Fitten, was Ptolemäus mit Hilfe der Epizykel gemacht hat!
Sie bleiben bei Ihrem Fehler, mit dem sie qualitative Unterscheide erfinden, wo keine sind. Auch Gauß hätte beim Kenntnisstand der Epizyklen-Modellierer wohl kaum eine andere Lösung gefunden.
————– #92: NicoBaecker sagt:
Sie schrieben:
„Die Hybris ist darin, dass man die erheblichen Schwankungen innerhalb des Klimas als trendneutrales Rauschen auffasst oder ob man damit eigentlich einräumt, etwas erklären zu wollen, was man aber grundsätzlich nur unzureichend versteht.“
Das ist allgemein aber nicht der Fall. Wie ich bereits mehrmals erklärt habe, gibt man dem Modell externe Klimafaktoren in ihrer beobachteten Zeitabhängigkeit vor (Solare Einstrahlung, Treibhausgasverlauf, Aerosoleintrag durch Vulkane und anthropogen, etc.). Die Klimaschwankungen auf der Zeitskala einiger Jahrzehnte werden damit alleine schon reproduziert.
Eigentlich nicht. Denn auch wenn man einige Einflüsse auf das Klimasystem identifiziert, fällt sowohl dessen Quantifizierung in der Wechselwirkung schwer, als auch die Frage, ob man überhaupt alle Parameter hinreichend kennt. Selbst ohne komplexe Wechselwirkungen würde man ein lineares Gleichungssystem aufbauen können, bei dem die Parameter nur unzureichend bekannt sind.
Ferner stellt sich die Frage ob bei zyklischen und quasizyklichen Einflüssen diese adäquat darstellen lassen. Aber durch die gegenseitigen Rückkopplung wird die Simulation eigentlich nicht mehr sicher lösbar. Und genau das zeigen auch die Modellierungsergebnisse.
————– #92: NicoBaecker sagt:
Überlagert sind diesen „großen“ Schwankungen in der Größenordnung einiger Zehntel Grad in der globalen Mitteltemperatur kürzerfristige Schwankungen mit geringerer Amplitude. Diese sind den internen Klimaschwankungen geschuldet, also dem vom Klimasystem selbst produzierten Schwankungen wie NAO, ENSO etc.
Die bekannten ozeanischen Zyklen, nicht nur ENSO, sondern auch AMO und PDO, müssten im GCM Modell berücksichtigt werden. Dies ist aber offensichtlich heute nicht der Fall: Siehe http://tinyurl.com/jxbglq8 und http://tinyurl.com/jxbglq8
Unter diesen Umständen ist der Verweis auf ein ‚Rauschen‘, dass man vernachlässigen könne, eher ein Bankrotterklärung des Ansatzes.
————– #92: NicoBaecker sagt:
Das Ziel der Modelle ist nun, die Beobachtungen mit Hilfe eines Sets von physikalischen Gleichungen (also allgemein gültigen Naturgesetzen, die auch für andere Vorgänge gelten, angewandt auf Klimasystem) zu reproduzieren. Man startet also zu einem Zeitpunkt, nehmen wir z.B. 1780, mit einem Anfangszustand des Klimas und berechnet schrittweise seine zeitlichen Änderungen, die sich aufgrund des Zeitverlaufs der externen Klimafaktoren und aufgrund der innewohnenden Dynamik des Gleichungssets ergibt. Dadurch bekommt man einen Zeitverlauf vom Klimazustand.
Das Modellklima zum Zeitpunkt nehmen wir z.B. 2015 nach 1780 ist also nur abhängig von:
a) Dem Anfangszustand 1780
b) Dem Zeitverlauf der externen Klimafaktoren zwischen 1780 und 2015
c) Der internen Dynamik
Das Konzept ist klar, aber es behandelt eben nicht die alles entscheidende Frage, ob man die Klimafktoren und damit das Modell überhaupt hinreichend beschreiben kann.
Gut wird dieses Problem von einer Peanuts-Folge illustriert:
Lucy himmelte Schröder an, aber fragte:
‚Sag mal, wie kannst du alle die schwierigen Stücke denn spielen, wo doch die schwarzen Tasten auf deinem Kinderklavier nur aufgemalt sind?‘
Seine Antwort: ‚Üben, üben, und nochmals üben!‘
Eine andere gute Illustration das Problems ist die Frage, wie man den Elefanten in den Kühlschrank bekommt …
————– #92: NicoBaecker sagt:
Von nichts anderem. Wenn sich also das Modellklima 2015 wesentlich von dem in 1780 unterscheidet, so liegt das nur an der Wirkung von b) und c)!
Das kann zu recht und begründet bezweifelt werden. Woher will man in einem rückgekoppelten System wissen, dass es gerade trendneutrales Rauschen überhaupt gibt? Das man dieses ohne detaillierte Modellierung den Quantifizieren kann und dass es nicht weitere, bislang unbekannte Faktoren gibt?
————– #92: NicoBaecker sagt:
Wenn ich Sie nun nach Ihren letzten beiden Kommentaren richtig verstehe, kritisieren Sie vor allem die unzureichende Modellierung der kürzerfristigen Klimaschwankungen, also die Reproduzierbarkeit von NAO, AMO, ENSO etc.. Das ist richtig. Ich habe schon öfter erwähnt, daß die GCMs von heute da noch Probleme haben.
Meine Kritik bezieht sich nicht nur auf dieses Detail, denn ich glaube, das die Behebung dieses Problems nicht mal die Spitze des Eisberges darstellt.
Viel mehr glaube ich, dass GCMs wegen einer Vielzahl von Unbekannten und Irrtümern nicht ihrem Ziel gerecht werden können.
Fortsetzung folgt …
Ich will ihnen für die Mühe danken, mit der sie diesen Text zu umfangreich erstaellt haben. Er bedarf der Würdigung und der Kritik. Damit der Post nicht zu lange wird und damit zu recht den Zorn des Admins auf sich zieht, werde ich ihn in mehreren Teilen beantworten
#92: NicoBaecker sagt:
Als erstes nochmal zur Betriebsblindheit. Bei mir als Naturwissenschaftler wirkt Ihr Vorwurf der Betriebsblindheit gleichwertig mit Blödheit. Aus dem einfachen Grund …
… Sie verstehen nicht, was Betriebsblindheit ist. Dies ist, dass man nicht stets nach den Grundlagen des eigenen Handelns fragt. Zu Recht will man nicht immer wieder bei ‚Adam und Eva‘ anfangen. Man kümmert sich um konkrete Probleme und verteidigt das erreichte. Eine Rückbesinnung auf die Grundkonzepte findet selten bis nie statt. Darum ist es in dieser Betriebsblindheit so schwer, grundlegende Irrtümer zu entdecken.
Um so komplexe ein Fachgebiet oder Bereich wird, um so mehr klebt man am Detail. Psychologisch erklärbar, vielleicht auch entschuldbar, aber nicht minder fatal. Dies findet man häufig in dieser Zeit – in unterschiedlichen Wissenschaften, aber auch in allen anderen Bereichen des Lebens.
Blödheit oder Dummheit ist dagegen etwas völlig anderes. Hier kümmert man sich nicht angemessen um Fragestellungen.
Dass Sie so empfindlich auf die Gefahr der Betriebsblindheit reagieren und pauschal Menschen davon frei sprechen wollen, illustriert um so mehr, wie verführerisch diese Denkblockaden sind.
————– #92: NicoBaecker sagt:
Naturwissenschaftler wollen wissen, wie die Natur funktioniert. Der Sinn des Klimamodellierens ist es, das Klima quantitativ zu verstehen und wesentliche Charakteristika des Klimasystems mit Hilfe von mathematischen Gleichungen beschreiben zu können.
Danke fürs Paraphrasieren meines Beitrags. So weit haben wir also keinen Dissens. Die Frage stellt sich dann, ob es hier überhaupt eine fruchtbare Lösung gibt, und ob die gewählten Methoden überhaupt die Chance auf Erfolg haben können. Denn auch wenn das Anliegen nachvollziehbar ist, folgt daraus nicht, das dieses auch sachgerecht umgesetzt wird.
————– #92: NicoBaecker sagt:
Dies gilt für jedes Modell in der Physik.
Offensichtlich nicht, denn viele Physiker beschäftigen sich nicht mit dem Klimasystem.
————– #92: NicoBaecker sagt:
Wenn Sie einem Modellierer Betriebsblindheit vorwerfen, so werfen Sie ihm vor, daß er sein Ziel, nämlich die Natur zu verstehen, nicht mehr verfolgen würde.
Keineswegs, sondern sein Ziel aus den Augen verloren zu haben. Man sagt auch: er sieht den Wald vor lauter Bäumen nicht.
————– #92: NicoBaecker sagt:
Was Sie also vorwerfen, wäre eine Unfähigkeit zur Selbstreflexion des eigenen Tuns. Ein solcher Naturwissenschaftler wäre schon blöde.
Ob ein Mensch fähig zur Selbstreflektion ist, ist eine andere Frage als ob er diese auch praktiziert. Jemand, der einen Ansatz vertritt und erhebliche Lebenszeit da hinein investiert hat, wird in der Regel diese Anstrengungen auch verteidigen und sich gegen Kritik wehren. Es ist normal, dass man scheinbare Widersprüche auch durch Hilfskonstrukte beseitigt. Manchmal auch zu Recht, aber der psychische Impuls ist zunächst völlig unabhängig vom Sachverhalt.
————– #92: NicoBaecker sagt:
Zudem ist dieser Vorwurf dann auch noch etwas vermessen, wenn er von jemanden kommt, der nach eigenen Angaben sowieso keine Ahnung von Denken und Forschung der Physik hat und daher die Ergebnisse dieser Forschung in diesem Rahmen gar nicht bewerten kann.
Von was reden sie da? Phantasieren sie sich etwas herbei? Ich hatte Physik im Nebenfach meines akademischen Abschlusses. Ein wissenschaftliches Studium sollte immer zum Ziel haben, auch fachfremde Arbeiten und Ergebnisse nachvollziehen zu können und sich fehlendes Wissen selbst anzueignen. Was glauben sie denn, sollte ein akademisches Studium bewirken? Reine Fachidioten heranzuzüchten? Ferner habe mich mit Wissenschaftstheorie beschäftigt, aber bezeichne mich als Laie, weil ich keine aktive Forschung zu Klimafragen betreibe.
Allerdings bilde ich mir sehr wohl ein, hinsichtlich der erkenntnistheoretischen Grundlagen Ihnen einiges vorraus zu haben. Nicht wegen meines Dünkels, sondern wegen der offensichtlichen Qualität der Argumente. Denn sie beanspruchen geradezu als Selbstverständlichkeit, dass jeder selbstkritisch reflektiert. Die allgemeine Erfahrung sollte sie gelehrt heben, dass dieser Anspruch in der Realität allzu oft nicht erfüllt ist und auch keineswegs selbstverständlich ist.
————– #92: NicoBaecker sagt:
Zu den Modellen:
Sie müssen nun zwei Arten von Modellen unterscheiden: Heutige Modelle in der Physik basieren immer aus Sätzen von physikalischen Gleichungen, die entweder direkt verwendet oder durch Approximationen allgemeiner Naturgesetze gewonnen werden.
Hier unterliegen sie einem methodischen Fehler. Zwar gibt es durchaus induktive Methoden, die von bekannten Zusammenhängen auf konsequente weitere Zusammenhänge schließen lassen. Auch können wilde Hypothesen sich in derartige Formeln gießen lassen, wobei keineswegs klar sein muss, ob derartige Hypothesen die Realität zutreffend beschreiben. Die Überprüfung liefert die Empirie und damit die Testbarkeit derartiger Aussagen.
Deduktive Methoden leiten dagegen von der Beobachtung ab und versuchen diese zu beschreiben und zu erklären. Dabei ist die phänomenologische Offenheit auch dazu geeignet, auch bislang unbekannte Zusammenhänge und Sachverhalte zu erkennen. Deduktives Arbeiten hat sich damit langfristig als das Kernprinzip moderner Wissenschaften erwiesen. Denn es wäre fatal, wenn der Erkenntnishorizont durch das bisher Bekannte beschränkt bliebe.
————– #92: NicoBaecker sagt:
Der Unterschied zwischen Modellen auf Basis physikalischer Gleichungen und einem deskriptiven Modell wie dem Epizykel-Modell ist fundamental. Das Epizykel-Modell ist nur ein Fit an die Beobachtungen. Ein physikalisches Modell der Planetenbewegungen basiert hingeben auf den Gesetzen der Mechanik, welche für die Planeten angewandt werden. Beim Epizykel-Modell wissen Sie nicht, ob es die Positionen auch noch außerhalb des Beobachungszeitraums hinreichend gut beschreibt. Ein physikalisches Modell hingegen kann dies, weil Sie wissen, daß die Naturgesetze auch außerhalb des Beobachtungszeitraumes gültig sind.
Leider haben sie meine bisherigen Hinweise auf Erkenntnisprozesse nicht zu Kenntnis genommen und verbleiben in einem konservativistisch-anachronistischen Argumentationsmodus. Das, was wir heute Naturgesetze nennen, war keineswegs stets Wissen der Menschheit. Um diese zu erkennen benötigte es völlig anderer Methoden als die Anwendung von Lehrbuchwissen. Das mechanische Weltbild wurde mit der Entwicklung der modernen Naturwissenschaften allmählich zum Standard. Laplace drückte es am markantesten aus. Dieses Weltbild bekam aber mit den Relativitätstheorien bereits erhebliche Risse und wurde durch die Quantenmechanik weitgehend auf den Kopf gestellt. Einige Menschen wollen dies auch nach hundert Jahren nicht zu Kenntnis nehmen.
Also: Was einst als festes Naturgesetz galt, hat sich stark als Sonderfall erwiesen. Es ist keineswegs klar, dass unsere aktuelle Naturerkenntnis und der vermuteten Naturgesetze bereits absolut zu verstehen sind. Vielmehr sind auch fundamentale Änderungen in der Zukunft zu erwarten, die wir heute gar nicht kennen können. Das Epizyklen-Modell stellte zum Kenntnisstand seiner Zeit eine herausragende Errungenschaft dar. Schließlich vermuteten deren Vertreter, dass die die Natur der Kosmologie ergründet haben. Erst sehr viel später erkannte man den Irrtum.
Bei der Klimaforschung erwarte ich aber keine neuen Naturgesetze, sondern nehme zu Kenntnis, dass sich eine Vielzahl von Effekten in einem lokal sehr unterschiedlichen Bedingungkontext zu unüberschaubaren Wirkungen zusammenfügt. Klimamodellierer gehen dagegen von der Annahme aus, dass sie mikroskopisch chaotische Systeme statistisch hinreichend präzise beschreiben können – einschließlich aller Rückkopplungen. Ich halte das gerade bei Durchsicht der bisherigen Ergebnisse für eine methodische Vermessenheit.
Fortsetzung folgt …
ich muß vielleicht etwas grundlegender ansetzen.
Als erstes nochmal zur Betriebsblindheit. Bei mir als Naturwissenschaftler wirkt Ihr Vorwurf der Betriebsblindheit gleichwertig mit Blödheit. Aus dem einfachen Grund: Naturwissenschaftler wollen wissen, wie die Natur funktioniert. Der Sinn des Klimamodellierens ist es, das Klima quantitativ zu verstehen und wesentliche Charakteristika des Klimasystems mit Hilfe von mathematischen Gleichungen beschreiben zu können. Dies gilt für jedes Modell in der Physik. Wenn Sie einem Modellierer Betriebsblindheit vorwerfen, so werfen Sie ihm vor, daß er sein Ziel, nämlich die Natur zu verstehen, nicht mehr verfolgen würde. Was Sie also vorwerfen, wäre eine Unfähigkeit zur Selbstreflexion des eigenen Tuns. Ein solcher Naturwissenschaftler wäre schon blöde. Zudem ist dieser Vorwurf dann auch noch etwas vermessen, wenn er von jemanden kommt, der nach eigenen Angaben sowieso keine Ahnung von Denken und Forschung der Physik hat und daher die Ergebnisse dieser Forschung in diesem Rahmen gar nicht bewerten kann.
Zu den Modellen:
Sie müssen nun zwei Arten von Modellen unterscheiden: Heutige Modelle in der Physik basieren immer aus Sätzen von physikalischen Gleichungen, die entweder direkt verwendet oder durch Approximationen allgemeiner Naturgesetze gewonnen werden.
Der Unterschied zwischen Modellen auf Basis physikalischer Gleichungen und einem deskriptiven Modell wie dem Epizykel-Modell ist fundamental. Das Epizykel-Modell ist nur ein Fit an die Beobachtungen. Ein physikalisches Modell der Planetenbewegungen basiert hingeben auf den Gesetzen der Mechanik, welche für die Planeten angewandt werden. Beim Epizykel-Modell wissen Sie nicht, ob es die Positionen auch noch außerhalb des Beobachungszeitraums hinreichend gut beschreibt. Ein physikalisches Modell hingegen kann dies, weil Sie wissen, daß die Naturgesetze auch außerhalb des Beobachtungszeitraumes gültig sind. Dennoch kann das physikalische Modell Vereinfachungen haben (z.B. nicht die Gravitationswirkung aller Kleinkörper im Asteroidengürtel berücksichtigen), dies kann dazu führen, daß Sie „tunen“ müssen, indem Sie z.B. die Gravitationswirkung der vielen kleinen Körper des Asteroidengürtel als ein „Rauschen“ berücksichtigen, welches mit viele weniger Parametern beschrieben werden kann als alle Kleinkörper zusammen. Aber diesen „subgrid“-Prozess als vereinfachter Ersatz der Summe der Gravitationswirkung der vielen kleinen Körper müssen Sie erstmal entwickeln, denn der stellt ja nur eine Vereinfachung der Realität dar und ersetzt diese nur. D.h. Sie müssen diesen Ersatzprozess anhand der wirklichen Wirkung der Kleinkörper tunen. C.F. Gauß hat so etwas übrigens schon um die 1809 für die Gravitationswirkung der Kleinkörper gemacht. Dieses „tunen“ ändert aber nichts daran, daß das Modell auf physikalischen Gleichungen basiert. Also verwechseln Sie das nicht: Diese Art „tunen“ ist nicht gleichzusetzen mit dem Fitten, was Ptolemäus mit Hilfe der Epizykel gemacht hat!
Sie schrieben:
„Die Hybris ist darin, dass man die erheblichen Schwankungen innerhalb des Klimas als trendneutrales Rauschen auffasst oder ob man damit eigentlich einräumt, etwas erklären zu wollen, was man aber grundsätzlich nur unzureichend versteht.“
Das ist allgemein aber nicht der Fall. Wie ich bereits mehrmals erklärt habe, gibt man dem Modell externe Klimafaktoren in ihrer beobachteten Zeitabhängigkeit vor (Solare Einstrahlung, Treibhausgasverlauf, Aerosoleintrag durch Vulkane und anthropogen, etc.). Die Klimaschwankungen auf der Zeitskala einiger Jahrzehnte werden damit alleine schon reproduziert. Überlagert sind diesen „großen“ Schwankungen in der Größenordnung einiger Zehntel Grad in der globalen Mitteltemperatur kürzerfristige Schwankungen mit geringerer Amplitude. Diese sind den internen Klimaschwankungen geschuldet, also dem vom Klimasystem selbst produzierten Schwankungen wie NAO, ENSO etc.
Das Ziel der Modelle ist nun, die Beobachtungen mit Hilfe eines Sets von physikalischen Gleichungen (also allgemein gültigen Naturgesetzen, die auch für andere Vorgänge gelten, angewandt auf Klimasystem) zu reproduzieren. Man startet also zu einem Zeitpunkt, nehmen wir z.B. 1780, mit einem Anfangszustand des Klimas und berechnet schrittweise seine zeitlichen Änderungen, die sich aufgrund des Zeitverlaufs der externen Klimafaktoren und aufgrund der innewohnenden Dynamik des Gleichungssets ergibt. Dadurch bekommt man einen Zeitverlauf vom Klimazustand.
Das Modellklima zum Zeitpunkt nehmen wir z.B. 2015 nach 1780 ist also nur abhängig von:
a) Dem Anfangszustand 1780
b) Dem Zeitverlauf der externen Klimafaktoren zwischen 1780 und 2015
c) Der internen Dynamik
Von nichts anderem. Wenn sich also das Modellklima 2015 wesentlich von dem in 1780 unterscheidet, so liegt das nur an der Wirkung von b) und c)!
Wenn ich Sie nun nach Ihren letzten beiden Kommentaren richtig verstehe, kritisieren Sie vor allem die unzureichende Modellierung der kürzerfristigen Klimaschwankungen, also die Reproduzierbarkeit von NAO, AMO, ENSO etc.. Das ist richtig. Ich habe schon öfter erwähnt, daß die GCMs von heute da noch Probleme haben.
Meine Argumentation zielt aber an der Reproduzierbarkeit der“ großen“ Klimaschwankungen ab, die also mehrere Jahrzehnte überdecken und höhere Amplitude haben. Diese sind nicht mit der internen Klimadynamik alleine erklärbar, sondern werden maßgeblich durch den Zeitverlauf der externen Klimafaktoren bestimmt.
Wenn ich Ihnen oben schrieb, daß GCMs heute nicht gefittet oder getuned werden, so bezog ich mich darauf, daß dieses tuning/fitting keinen maßgeblichen Einfluß auf den groben Klimaverlauf haben. Das von Ihnen zitierte tuning bezieht sich darauf, daß man die Modelle eben auch zum Reproduzieren der kürzerfristigen Schwankungen tunen will. Sie sehen ja an den Werten, daß diese nicht physiklisch unsinnige Werte annehmen, sondern nur im Bereich der Meßgenauigkeit getuned werden und um subgrid-Prozesse, die also physiklaisch nicht aufgelöst sind, zu simulieren.
Um nochmal zu Ihrem Beispiel Ptolemäus zurückzukommen: Das GCM-tuning ist nicht vergleichbar mit eim Epizykelmodell. Das Epizykel-Modell war völlig ohne Naturgesetze. GCMs basieren jedoch auf verifizierten physikalischen Gleichungen.
Meine Frage an Sie ist nun: Was kritisieren Sie nun an den GCMs eigentlich? Die Mängel beim Reproduzieren der internen Klimaschwankungen (des Klimarauschens) wie AMO, NAO auf der Zeitskala von mehreren Jahren oder die Reproduzierbarkeit der Klimaentwicklung aufgrund der externen Klimafaktoren auf der Zeitskala von mehreren Jahrzehnten?
Nehmen wir nochmal Ptolemäus, Ihr Vergleich gefäält mir: Im Prinzip kann an durch Hinzufügen von beliebig vielen exzentrischen Kreisen jede Planetenbahn anfitten – ohne zu wissen, warum sich der Planeten so bewegt.
Sehr geehrter Herr Baecker
Man nennt das Verfahren auch deduktiv: Aus den Beobachtungen leitet man Gesetzmäßigkeiten her, die eine Beschreibung der Welt erbringen. Von daher ist den Erfindern der Epizyklen auch der höchste Respekt zu erbringen. Sie konnten auch von Newton noch gar nichts wissen, denn der lebte sehr viel später. Auch kam Newton noch nicht auf die Schwerkraft wegen den Planetenbahnen, sondern aus sehr viel näherliegenden Beobachtungen. Erst aus der Kombination der Ergebnisse gewannen wir unser aktuelles Verständnis.
Nun wissen wir dadurch, dass die Epizyklen kein zutreffendes Bild der Planetenbahnen zeigt. Was haben also jene Astronomen falsch gemacht?
Sie sind von falschen Grundannahmen ausgegangen. Diese konnten sie aber gar nicht besser wissen, denn die Naturerkenntnis dieser Zeit machte die Entwicklung zutreffender Vorstellungen sehr schwierig, bzw. schloss sie aus.
——— #88: NicoBaecker sagt:
Das Gravitationsgesetz Newtons vom Zweikörpersystem Sonne Planet liefert hingegen eine Ellipse, ….
Ihre Argumentation ist völlig anachronistisch. Es ging nie darum zwischen beiden Erklärungsvarianten zu wählen, das durch Kopernikus uns Galilei das Epizyklenmodell falsifiziert wurde.
——— #88: NicoBaecker sagt:
Das ist der große Unterschied zwischen Fit und Physik!
Sie konstruieren einen Gegensatz, wo keiner ist.
Das Sie offensichtliche Verständnisprobleme in der Analyse haben haben sie auch entsprechende Defizite in der Wissenschaftstheorie.
——— #88: NicoBaecker sagt:
Und Klimamodelle machen Physik, nicht Fit!
Abgesehen davon, dass es den Gegensatz nicht gibt, ist es gleich mehrfach falsch.
1. Meine Quelle http://tinyurl.com/qhmkuxz belegte, dass es sehr wohl um tuning = fitting geht.
2. Wenn es um Physik ginge, dürften die Modellergebnisse nicht derartig abweichend sein. Denn gemäß IPCC, die sich ausschließlich auf GCMs stützt, liegt die CO2-Klimasensitivität zwischen 1,5 und 4,5 Grad, ohne sich auf einen wahrscheinlichsten Wert festzulegen. Das ist Faktor 3! Physik kann nicht derartig disperse Ergebnisse produzieren, sondern belegt, dass die meisten Ergebnisse falsch sein müssen. Wenn man noch nicht mal unterscheiden kann, welche der Ergebnisse, bzw. der Modellkonstruktionen und Eingangsparameter, falsch sind, hat man sich von einer Physik, die die Realität beschreibt offensichtlich erheblich entfernt.
3. Daraus lassen sich keine seriösen Vorhersagen machen und darum kann man auch nicht mehr von Physik sprechen, sondern von GIGO-Modellen.
Angesichts dessen frage ich mich allen ernstes: Was treibt Menschen dazu, hier nicht die Reißleine zu ziehen und dieses fruchtlose Spiel zu beenden?
Ihr Vergleich von Klimamodellen mit der ptolomäischen Himmelsmechanik zeigt mir, daß Sie überhaupt nicht verstanden haben, was Klimamodelle sind …
Sehr geehrter Herr Backer
Vielmehr hinterlassen Sie den Eindruck einen Vergleich nicht zu verstehen. Damit sie es aber leichter haben, eine Erklärung:
Ein Modell dient üblicherweise dazu, ein Phänomen zu erklären. Ferner sollten Vorhersagen damit möglich sein. Das trifft sowohl für das Epizyklen-Modell zu, als auch für GCMs.
Des Beispiel mit dem Epizyklen-Modell hat nur den Vorteil, dass sie kleine unstimmigkeiten eben nicht mit einem Rauschen weg erklären wollet, sondern dieses hinsichtlich seiner Auflösung ausmodellierte.
Zugegeben, der Anspruch eines GCMs ist ungleich komplexer aber es ergeben sich noch ewitere Vergleichbarkeiten: Das Epzyklen-Modell unterschätzte die zentrale Rolle der Sonne im kosmologischen verlauf. Ebenso die GCMs – hier weist man der solaren Varianz nur eine untergeordnete Rolle zu. Und auch dort ist es vermutlich der Grund, warum das Modell nicht zur Wirklichkeit passt.
———- #87: NicoBaecker sagt:
Letztlich wäre Betriebsblindheit das gleiche wie Blödheit.
Sie haben reichlich harsche Urteile für Ihre Kollegen. Sind sei bereit dafür, ähnlich scharf beurteilt zu werden?
———- #87: NicoBaecker sagt:
Wollen Sie allen Ernstes behaupten, daß Klimamodellierer betriebsblind sind und die Realität ignorieren?
Genau diesen Eindruck hinterlassen die Versuche. Man weiß von fundamentalen Mängeln und macht trotzdem weiter, als ob nichts gewesen wäre.
———- #87: NicoBaecker sagt:
Ich habe Sie nach Belegen gefragt. Bitte belegen Sie dies!
Ignorant? Ich habe zu verschiedenen Aussgen jeweils die Belege beigebracht, siehe #81 z.B.:
http://tinyurl.com/qhmkuxz
———- #87: NicoBaecker sagt:
Wie ich schon mehrmals erklärt habe sind Klimamodelle in erster Linnie gar nicht darauf aus sind …
Um so schlimmer! Wenn sie etweas erklären wollen, was sich mit der Realität nicht deckt, haben sie den Anspruch der Naturwissenschaft verlassen und sind bei Fantasy und SciFi angekommen.
———- #87: NicoBaecker sagt:
– wie Ptolemäus die Planetenpositione ge“fittet“ hat – das Klima zu fitten, sondern – wie Newton – das Klima physikalisch-quantitativ zu beschreiben! Dies funktioniert eben nicht hinreichnd gut. Ein Fit wäre exakt möglich, ein Klimamodell auf Basis der Physik zeigt immer Abweichungen in einem komplexen System.
Das ist eine Bankrotterklärung, keine Wissenschaft. Denn ohne Fit fehlt eben der empirische Beleg. Das ist dann nicht mehr von virtuellen Welten zu unterscheiden.
Oder Nullsatz des Jahres:
„Ein komplexes GCM ist eine genauere Approximation an die Wirklichkeit als ein einfaches Modell.“
Das muß man dem Pseudonym NicoBaecker lassen: Er schafft es immer wieder seine Nullsatzhits zu toppen.
Ihr Vergleich von Klimamodellen mit der ptolomäischen Himmelsmechanik zeigt mir, daß Sie überhaupt nicht verstanden haben, was Klimamodelle sind, trotzdem ich dies bereits mehrmals hinreichnd oft erklärt habe. Verzeihen Sie mein vielleicht ungerechtfertigt Mißtrauen Ihnen gegenüber. Aber Ihr Unvermögen, den wahren wissenschaftlichen Anspruch zu begreifen und dann auch noch Betriebsblindheit ins Spiel zu bringen, kommt mir reichlich verwegen vor. So verwegen kann man nur sein, wenn man keine Ahnung von der Materie hat.
Letztlich wäre Betriebsblindheit das gleiche wie Blödheit. Ein betriebsblinder Wissenschaftler, der nicht mehr nach der Wissenschaftlichkeit seiner Methode fragt, ist blöde! Wollen Sie allen Ernstes behaupten, daß Klimamodellierer betriebsblind sind und die Realität ignorieren?
Ich habe Sie nach Belegen gefragt. Bitte belegen Sie dies!
Wie ich schon mehrmals erklärt habe sind Klimamodelle in erster Linnie gar nicht darauf aus sind – wie Ptolemäus die Planetenpositione ge“fittet“ hat – das Klima zu fitten, sondern – wie Newton – das Klima physikalisch-quantitativ zu beschreiben! Dies funktioniert eben nicht hinreichnd gut. Ein Fit wäre exakt möglich, ein Klimamodell auf Basis der Physik zeigt immer Abweichungen in einem komplexen System.
Zur Bewertung der Modellgenauigkeit müssen Sie freilich berücksichtigen, daß das Modell den grundsätzlichen Temperaturanstieg und andere features erklären kann.
Sehr geehrter Herr Baecker
Verwechseln Sie hier nicht Anspruch und Wirklichkeit?
Denn wenn die Unzulänglichkeiten bekannt sind, ist damit völlig unklar, ob das ‚Rauschen‘ die Abweichungen zutreffend erklärt, oder ob damit lediglich die realitätsfernen Ergebnisse gerechtfertigt werden.
——————— #84: NicoBaecker sagt:
Um die Modellleistung zu bewerten, müssen Sie die Reproduzierung dieses Signals mit der Abweichung vergleichen, was zumindest teilweise der noch unzureichenden Modellierbarkeit des internen Klimarauschens geschuldet ist.
Die Hybris ist darin, dass man die erheblichen Schwankungen innerhalb des Klimas als trendneutrales Rauschen auffasst oder ob man damit eigentlich einräumt, etwas erklären zu wollen, was man aber grundsätzlich nur unzureichend versteht.
„Bei Modellen, deren Komplexität zum Einen die unmittelbare Nachvollziehbarkeit nicht mehr zulässt,“
Wenn ich Sie richtig verstehe, meinen Sie damit moderne GCMs. Aber was sollte da nicht mehr unmittelbar nachvollziehbar sein? Nennen Sie ein paar konkrete Beispiele dafür, sonst ist Ihr Argument gegenstandslos.
Sehr geehrter Herr Baecker
Konkret: Der Anspruch von CMIP5 ist hier zu finden: http://tinyurl.com/zuah7mt
„CMIP5 will notably provide a multi-model context for 1) assessing the mechanisms responsible for model differences in poorly understood feedbacks associated with the carbon cycle and with clouds, 2) examining climate “predictability” and exploring the ability of models to predict climate on decadal time scales, and, more generally, 3) determining why similarly forced models produce a range of responses.“
Also geht es sehr wohl um die Vorhersagbarkeit und die Qualität von Vorhersagen, die eben von Dritten nicht mehr einfach nachfollzogen werden kann
————— #81: NicoBaecker sagt:
„und zum Anderen mit adjustierbaren Parametern arbeitet“
Nennen Sie auch hier konkret Parameter, die man adjustiert.
Die Adjustierung ist ja nur dann problematisch, wenn durch dadurch die Ergebnisse des Modells maßgeblich beeinflußt werden. Ist das denn überhaupt der Fall? Wenn Sie meinen „ja“, so belegen Sie das!
Ohne Frage müssen Parameter, deren Einfluss man nur abschätzen kann entsprechend getuned werden. Das ist isoliert betrachtet kein Killer-Argument. Allerdings stell sich die Frage zu Recht, in wie weit Modelle , die einerseits vereinfachen müssen, die zugrande liegenden Mechanismen überhaupt angemessen abbildet, und man durch das Tuning diesen problematischen Sachverhalt lediglich verschleiert. Man produziert durch das Tuning einen begrenzten Fit, der aber nichts mehr zum Realitätsbezug des Modells sagt.
Detaillierter Dikussionen finden Sie in http://tinyurl.com/qhmkuxz
X-3 „… and as a result, the process of selecting a model configuration is shrouded in mystery
… The need to tune models became apparent in the early days of coupled climate modeling, when the top of the atmosphere (TOA) radiative imbalance was so large that models would quickly drift away from the observed state.“
Konkrete Parameter werden des Witeren genannt:
„… by tuning cloud-related parameters at most climate modeling centers [e.g. Watanabe et
50 al., 2010; Donner et al., 2011; Gent et al., 2011; The HadGEM2 Development Team,
51 2011; Hazeleger et al., 2011], while others adjust the ocean surface albedo [Hourdin et
52 al., 2012] or scale the natural aerosol climatology to achieve radiation balance [Voldoire
53 et al., 2012].“
Reicht Ihnen das? Sollten wir dies Arbeit detaillierter diskutieren? Oder wollen Sie andere Quellen einbringen?
————— #81: NicoBaecker sagt:
Meines Wissens besteht heute keine Notwendigkeit, aufgrund numerischer Beschränkungen, Parameter künstlich zu adjustieren. Früher gab es mal die Flußkorrkturen, aber das war mal.
Man speist die Modelle heute mit gemessenen Parametern und nicht mit künstlichen.
Es ist jedoch an Ihnen, ihre Behauptung zu auch belegen. Ansonsten liegt es nahe, Ihre Argumente als Strohmann-Argumente zu werten.
Der Artikel belegt, dass heute keine Fluss-Korrekturen mehr vorgenommen werden, nennt aber andere Bereiche.
Angesichts der gelieferten Belege gebe ich Ihnen die Bitte zurück, Ihren Punkt zu belegen.
————— #81: NicoBaecker sagt:
“ – und dennoch keine hinreichende empirischen Ergebnisse bringt,“
Die GCMs reproduzieren jedenfalls das empirische Klima ungleich genauer als Hardes Modell. Und dies liegt daran, daß diese eine höhere räumliche Auflösung und detailreichere Physik haben.
Da es nicht das Ziel Hardes ist, das Klima in seiner Gänze zu simulieren, zeigt ihr Kommentar lediglich das mangelnde Verständnis in dem Ansatz. Darüber hinaus ist das Grid Size für eine physikalisch angemessene Moddellierung noch immer wesentlich zu grob: Siehe http://tinyurl.com/hagsq22
„Typical AGCM resolutions are between 1 and 5 degrees in latitude or longitude: HadCM3, for example, uses 3.75 in longitude and 2.5 degrees in latitude, giving a grid of 96 by 73 points (96 x 72 for some variables); and has 19 vertical levels. This results in approximately 500,000 „basic“ variables, since each grid point has four variables (u,v, T, Q), though a full count would give more (clouds; soil levels). HadGEM1 uses a grid of 1.875 degrees in longitude and 1.25 in latitude in the atmosphere; HiGEM, a high-resolution variant, uses 1.25 x 0.83 degrees respectively.“
Nur zur Erläuterung: ein Breitengrad ist 111,2 km, ein Längengrad variert, ist damit aber zumeist vergleichbar. Wenn man sich mit Atmosphärenphysik beschäftigt, wird schnell klar, dass sich diese über einem See, einem Wald, Wiesen, Gebirge, bebautem Land usw. erheblich unterscheidet. Wenn wir schon Grid-Zellen mit 100 km Kantenlängen als hochauflösend bezeichnen, wird klar, dass diese nur sehr grobe Vereinfachungen physikalischer Vorgänge leisten können.
————— #81: NicoBaecker sagt:
„Modelle sind darum Modelle, weil sie abstrakt vereinfachen.“
Ein komplexes GCM ist eine genauere Approximation an die Wirklichkeit als ein einfaches Modell. Sie müssen bedenken, daß man Modelle auch z.B. deswegen macht, um Lücken in der empirischen Datenaufnahme schließen zu können und so überhaupt die Erforschung von manchen Phänomenen damit erst ermöglichen. Dies trifft vor allem da zu, wo Daten empirsch nicht oder nur aufwendig erhoben werden können, z.B. über Vorgänge im Weltall oder anderen Himmelskörpern, Erdinneres, Wetterzustand der Atmosphäre, Bauwerke, Herstellungsprozesse, chemische Vorgänge,…
Das bleibt klar. Wenn aber diese notwendige Vereinfachung so weit vergröbert, dass sie das zu Modellierende nicht mehr hinreichend abbilden kann – gemessen an seinem Anspruch – dann ist ein Bisschen mehr an Genaueigkeit eben noch immer unzureichend.
————— #81: NicoBaecker sagt:
„Z.B. ist mir das Modtran Modell recht einleuchtend. Die Frage allerdings bleibt: Was will das Modell aussagen und wird es diesem Ansatz gerecht?“
Die Grenzen von MODTRAN sind ja offenkundig (fixe Atmosphäre, spektrale Auflösung, Stoffverteilung). Denn das Modell ist ja einfach.
Und das ist auch völlig ok. Gerade weil Modtran lediglich den Strahlungstransport modelliert, kann es auch zu plausiblen – weil begrenzten – Aussagen kommen.
Komplexe Modelle mit sehr viel größerem Anspruch leiden unter dem Mangel an Transparenz.
————— #81: NicoBaecker sagt:
Bei komplexen Modellen kennt man die Grenzen auch, nur muß man sich damit eben auch detaillierter mit dem Modell beschäftigen. Es ist eine Frage des Aufwands. Wenn Sie den nicht spendieren wollen, so erlaubt Ihnen das jedoch keine haltlose Unterstellungen. Sie sollen mehr Neutralität zeigen und nicht einfach irgendwas ohne Belege behaupten, was Ihnen in den Kram paßt.
Also, ohne Belege bleiben Ihre Aussagen nur faule Unterstellungen, mehr nicht!
Gerade die Beschäftigung mit dem Modellansatz zeigt, dass der Anspruch so nicht erfüllt werden kann. Denn wenn die Physik vom Ansatz her zu grob und viele Faktoren eben unklar sind: Z.B. Wolkenbildung, Wolkenwirkung sind und auch die Forcings wie Wasserdampf in einer sehr unterschiedlichen Spannbreite unterstellt werden können, wird erkennbar, dass derartige Modelle eben nicht die Wirklichkeit abbilden, sondern eine rein virtuelle Welt beschreiben, deren Bezug zur Realität fragwürdig bleibt.
zwischen vor und nach 1990 besteht kein Unterschied in der Modellbeschreibung; das Modell wird schließlich immer am Anfang der Zeitreihe (üblicherweise im 19 Jahrdt) gestartet und berechnet dann das Klima in der Zeit vorwärts, das ist also keine „Fortschreibung“. Ich sehe auch keinen grundsätzliche Unterschied in den Abweichungen nach 1990, denn es gab vor 1990 auch Phasen mit ähnlich großen Abweichungen. Zur Bewertung der Modellgenauigkeit müssen Sie freilich berücksichtigen, daß das Modell den grundsätzlichen Temperaturanstieg und andere features erklären kann. Um die Modellleistung zu bewerten, müssen Sie die Reproduzierung dieses Signals mit der Abweichung vergleichen, was zumindest teilweise der noch unzureichenden Modellierbarkeit des internen Klimarauschens geschuldet ist.
Damit bezeichnen Sie komplexe Klimamodelle, auf die sich heutige Klimaforscher beziehen, als GIGO Modelle (GIGO = „garbage in – garbage out“. Das heißt ja wohl nichts anderes, daß Sie von diesen anerkannten wissenschaftlichen Methoden nichts halten und glauben, die Wissenschaftler wären zu blöde oder würden vorsätzlich mit ihren Modellen Unsinn produzieren.
Sehr geehrter Herr Baecker
Es ist nicht meine Diktion, andern Menschen Blödheit oder vorsätzlichen Irrtum zu unterstellen. Die einfachste Erkläre wäre der Irrtum und die Fehleinschätzung. Dies kann ich auch begründen:
Zu Beginn ist es durchaus nachvollziehbar, das bislang erworbene Wissen über atmosphäriche Vorgänge in ein entsprechendes Modell zu packen – auch wenn man sich darüber im Klaren ist, dass eben jenes Wissen bei weiten unzureichend ist, um sämtliche Klimaphänomene hinreichend zu beschreiben. Ich stelle mir allerdings vor, dass im Zuge der intensiven Beschäftigung mit dem Thema eine gewisse Betriebsblindheit einsetzte wie ich sie in ähnlicher Weise baei anderen Vorhaben schon oft beobachtete. In diesem Sinne geht immer mehr die Bodenhaftung verloren. Nachdem man sich Jahre mit einem Thema beschäftigte, mit Hilfsannahmen den Erfolg zu sichern suchte, ist es menschlich nur allzu verständlich, dass man die notwendige Distanz zu einer sachgerechten Beurteilung verloren hat.
Erst die Distanz ermöglicht ein Urteil, das dann hinterfragt, in wie weit das entwickelte Modell noch Bezug zur Realität hat.
Für mich war sehr aufschlussreich, dass man schon lange vor der Erfindung des Fernrohres sehr genaue Analysen zum Stand der Gestirne kannt. Das führte zur Entwicklung der Epizyklen zur Ptolemäschen Kosmsologie. Diese lieferten erstaunlich präzise Voraussagen zum Stand zukünftiger Positionen. Eine Präzisionen, die jene Klimamodelle nicht erreichten. Das Verfahren und Konzept der Epizyklen war anspruchsvoll und keineswegs trivial – dennoch erwies es sich nach jahrhunderte langem erfolgreichem Einsatz als physikalisch unzutreffend.
Warum sollte jenen Klimamodellen ein anderes Schicksal zuteil werden? Immerhin haben wir trotz umfassender Tuning-Parameter keinen überzeugenden Fit zu gemessenen Ergebnissen. Mir ist darum in keiner Weise klar, woher der Respekt vor derartigen Verfahren und Ergebnisse kommt. Die Menge des Aufwandes ist kein Qualitätsmerkmal, aber erklärt, werum man tendenziell an Irrtümern festhält.
—————- #80: NicoBaecker sagt:
Die Grenzen der Modellierbarkeit und die Schwächen komplexer Klimamodelle sind auch den Klimaforschern bekannt. Die daraus resultierenden Beschränkungen werden doch gerade in den Spannbreiten der Resulte wie der Klimasensitivität berücksichtigt.
Wenn dem so wäre, verstehe ich immer weniger, warum man diesen Elaboraten so viel Vertrauen schenkt.
—————- #80: NicoBaecker sagt:
Daß ein GCM das Modell von Harde um Längen an Genauigkeit der Klimareproduzierbarkeit schlägt, werden Sie doch hoffentlich nicht abstreiten? Der Physikteil von Hardes Modell ist nur eine winzige Teilmenge der eines GCMs.
Ihnen ist offensichtlich nicht klar, dass der Anspruch des Modells von Harde ein völlig anderer ist. Die Qualität eines Modells muss an dessen Selbstanspruch gemessen werden. Harde wollte nie ein globales Klimamodell erstellen, sondern wollte in bewusster Reduktion der Annahmen eine Grundlage zur Abschätzung der Klimasensitivität schaffen. Darin konnte er die Transparenz der Annahmen aufrecht erhalten und damit dem Modell Plausibilität verleihen, die in den CMIP5 Modellen verloren gegangen ist.
„Die GCMs reproduzieren jedenfalls das empirische Klima ungleich genauer als Hardes Modell. Und dies liegt daran, daß diese eine höhere räumliche Auflösung und detailreichere Physik haben.“
Diesen Eindruck habe ich nicht. Nach meinem laienhaften Verständnis können die GCM Modelle zwar die globale Temperaturen zwischen 1880 bis 1990 einigermaßen beschreiben, die Fortschreibung ab 1990 zeigt aber deutliche Diskrepanzen. (Marotzke schreibt diese der natürlichen Variabilität des Klimas zu.) Dies ist der Ausgangspunkt, weshalb viele Laien die GCM-Modelle anzweifeln.
Mit dem Mittelwert einer globalen Temperatur kann ich nicht viel anfangen. Mich interessiert ein Vergleich von Messung und Simulation auf lokaler Basis, also „gridweise“. Einen solchen Vergleich von GCM-Modellen mit Temperatur-Messungen habe ich bisher nur auf dem KNMI climate explorer gefunden. Die Daten sind aber schlecht zugänglich. Ich vermute dass bei einem lokalen Vergleich noch größere Unterschiede bestehen.
Deshalb mache ich ortsaufgelöst Modellrechnungen entsprechend zu den Arbeiten von Herrn Harde. Ich gebe zu dass das Modell viel einfacher ist als ein GCM-Modell. Dies hat mir aber bisher schon viel zum Verständnis von Klimafragen geholfen.
„Bei Modellen, deren Komplexität zum Einen die unmittelbare Nachvollziehbarkeit nicht mehr zulässt,“
Wenn ich Sie richtig verstehe, meinen Sie damit moderne GCMs. Aber was sollte da nicht mehr unmittelbar nachvollziehbar sein? Nennen Sie ein paar konkrete Beispiele dafür, sonst ist Ihr Argument gegenstandslos.
„und zum Anderen mit adjustierbaren Parametern arbeitet“
Nennen Sie auch hier konkret Parameter, die man adjustiert.
Die Adjustierung ist ja nur dann problematisch, wenn durch dadurch die Ergebnisse des Modells maßgeblich beeinflußt werden. Ist das denn überhaupt der Fall? Wenn Sie meinen „ja“, so belegen Sie das!
Meines Wissens besteht heute keine Notwendigkeit, aufgrund numerischer Beschränkungen, Parameter künstlich zu adjustieren. Früher gab es mal die Flußkorrkturen, aber das war mal.
Man speist die Modelle heute mit gemessenen Parametern und nicht mit künstlichen.
Es ist jedoch an Ihnen, ihre Behauptung zu auch belegen. Ansonsten liegt es nahe, Ihre Argumente als Strohmann-Argumente zu werten.
“ – und dennoch keine hinreichende empirischen Ergebnisse bringt,“
Die GCMs reproduzieren jedenfalls das empirische Klima ungleich genauer als Hardes Modell. Und dies liegt daran, daß diese eine höhere räumliche Auflösung und detailreichere Physik haben.
„Modelle sind darum Modelle, weil sie abstrakt vereinfachen.“
Ein komplexes GCM ist eine genauere Approximation an die Wirklichkeit als ein einfaches Modell. Sie müssen bedenken, daß man Modelle auch z.B. deswegen macht, um Lücken in der empirischen Datenaufnahme schließen zu können und so überhaupt die Erforschung von manchen Phänomenen damit erst ermöglichen. Dies trifft vor allem da zu, wo Daten empirsch nicht oder nur aufwendig erhoben werden können, z.B. über Vorgänge im Weltall oder anderen Himmelskörpern, Erdinneres, Wetterzustand der Atmosphäre, Bauwerke, Herstellungsprozesse, chemische Vorgänge,…
„Z.B. ist mir das Modtran Modell recht einleuchtend. Die Frage allerdings bleibt: Was will das Modell aussagen und wird es diesem Ansatz gerecht?“
Die Grenzen von MODTRAN sind ja offenkundig (fixe Atmosphäre, spektrale Auflösung, Stoffverteilung). Denn das Modell ist ja einfach.
Bei komplexen Modellen kennt man die Grenzen auch, nur muß man sich damit eben auch detaillierter mit dem Modell beschäftigen. Es ist eine Frage des Aufwands. Wenn Sie den nicht spendieren wollen, so erlaubt Ihnen das jedoch keine haltlose Unterstellungen. Sie sollen mehr Neutralität zeigen und nicht einfach irgendwas ohne Belege behaupten, was Ihnen in den Kram paßt.
Also, ohne Belege bleiben Ihre Aussagen nur faule Unterstellungen, mehr nicht!
„Ich weiß zwar nicht im Mindesten, worauf Sie sich beziehen, aber Sie müssen mich völlig falsch verstanden haben.“
Ich bezog mich auf diese Stelle in Ihrem #74:
„Ein Bisschen schwach,mit GIGO Modellen zu kommen.“
Das war Ihr Kommentar auf meine Aussage
#66: NicoBaecker sagt:
„Ich kann Sie letzlich nur an die Modellergebnisse verweisen,“
Damit bezeichnen Sie komplexe Klimamodelle, auf die sich heutige Klimaforscher beziehen, als GIGO Modelle (GIGO = „garbage in – garbage out“. Das heißt ja wohl nichts anderes, daß Sie von diesen anerkannten wissenschaftlichen Methoden nichts halten und glauben, die Wissenschaftler wären zu blöde oder würden vorsätzlich mit ihren Modellen Unsinn produzieren.
Die Grenzen der Modellierbarkeit und die Schwächen komplexer Klimamodelle sind auch den Klimaforschern bekannt. Die daraus resultierenden Beschränkungen werden doch gerade in den Spannbreiten der Resulte wie der Klimasensitivität berücksichtigt.
Daß ein GCM das Modell von Harde um Längen an Genauigkeit der Klimareproduzierbarkeit schlägt, werden Sie doch hoffentlich nicht abstreiten? Der Physikteil von Hardes Modell ist nur eine winzige Teilmenge der eines GCMs.
Lieber Herr Denzer, #77
ich empfehle Ihnen, sich zum Verständnis der Physik des Treibhauseffektes den Unterschied zwischen Energie (bzw. Wärme) und Leistung klar zu machen. Ihr Argument scheitert, weil Sie den Faktor Zeit nicht bedacht haben.
ich finde Ihre selbstgefällige Interpretation von anerkannten wissenschaftlichen Methoden erschreckend. Insbesondere der indirekte Vorwurf, die Wissenschaft würde es sich einfach machen.
Sehr geehrter Herr Baecker
Ich weiß zwar nicht im Mindesten, worauf Sie sich beziehen, aber Sie müssen mich völlig falsch verstanden haben.
Allerdings weiß ich von Menschen, die es sich nicht nur tatsächlich zu einfach machen, sondern auch, dass sie die Wissenschaft selbst repräsentieren. Dazu ist allerdings nur zu sagen, dass jene, die es sich zu einfach machen, aus Prinzip nicht die Wissenschaft repräsentieren können.
———— #76: NicoBaecker sagt:
Es sollte schließlich jedem, der sich mit dem Klimawandel beschäftigt bekannt sein, daß man Klimamodelle eben wegen der Komplexität des Klimas macht.
Modelle zu konstruieren ist fraglos eine Notwendigkeit in der Wissenschaft. Nur ist deswegen nicht jedes Modell auch automatisch sachgerecht und zutreffend. Bei Modellen, deren Komplexität zum Einen die unmittelbare Nachvollziehbarkeit nicht mehr zulässt, und zum Anderen mit adjustierbaren Parametern arbeitet – und dennoch keine hinreichende empirischen Ergebnisse bringt, können und müssen allerdings als GIGO-Modelle bezeichnet werden.
Modelle sind darum Modelle, weil sie abstrakt vereinfachen. Z.B. ist mir das Modtran Modell recht einleuchtend. Die Frage allerdings bleibt: Was will das Modell aussagen und wird es diesem Ansatz gerecht?
Auch das Modell von Prof. Harde erscheint mir plausibel. Er will auch keine komplette Klimasimulation erreichen sondern lediglich die CO2 Sensitivität abschätzen.
Bei CMIP5 scheint mir dagegen Anspruch und Methode unzureichend. Die Vorstellung, eine halbwegs vollständige Klimasimulation durchzuführen erfordert ein ganz anderes Kaliber eines Modells. Denn die Gitternetz-Zonen sind nicht nur zu groß bemessen, sondern auch das jeweilige Zusammenspiel der Effekte ist nur unzureichend verstanden und wird darum nicht sachgerecht abgebildet. Darum bleiben es GIGO-Modelle.
Wie Sie zu der Aussage kommen, daß die Wissenschaft nur einfache Antworten hätte, ist mir ein Rätsel. Sie sollten einfach mal in die Wissenschaft einsteigen, statt darüber ohne eine Spur von Faktenwissen zu lamentieren.
„„„Und ich „muss“ nicht zustimmen, dass der Wasserdampfgehalt der Luft mit zunehmender Temperatur zunimmt, das habe ich selbst gesagt.“
Gut, dann haben Sie also die physikalischen Grundlagen des Wasserdampf-feedbacks also verstanden?“
Oje, Wasser(dampf) ist keineswegs in Luft löslich.
Wieso der Partialdruck des Wasserdampfes in der Luft mit zunehmender Temperatur steigen soll, wird wohl auf ewig das eheimnis des Pseudonyms bleiben.
Herr Baecker (oder ist NicoBaecker der richtige Familienname ?), ich wollte zwar vor Weihnachten keine Kommentare mehr schreiben aber halte es für notwendig, Ihnen etwas Grundlegendes zu erklären, damit Sie erkennen,
dass die AGW Theorie auf tönernen Füssen steht
1) Wärme ist die Gesamtheit der kinetischen Energie von Atomen oder Molekülen in einem Volumen und je schneller sich diese bewegen, um so wärmer ist es. Temperatur ist die mittlere kinetische Energie der Atome oder Moleküle
2) Elektromagnetische Strahlung kann nur eine Temperaturerhöhung bewirken, wenn diese in Wechselwirkung z.B. mit den Luftmolekülen eintritt, wobei diese Wechselwirkung zu einer erhöhten Geschwindigkeit der Moleküle führen muss. tritt z.B. UV-Strahlung mit Sauerstoff in der Stratosphäre in Wechselwirkung, so spaltet die UV-Strahlung ein Sauerstoffmolekül in 2 Sauerstoffatome – dies führt aber nicht zu einer höheren Geschwindigkeit der Moleküle bzw. Atome.
3) Infrarotstrahlung regt bei Molekülen wie H2O; CO2; CH4 usw. die Schwingungen der Atome in diesen Molekülen an, die dann durch Stossaktivierung Stickstoffmoleküle und Sauerstoffmoleküle beschleunigen. Wenn Sie einem Skifahrer einen Tritt in den Hintern geben, erhöht sich dessen Geschwindigkeit auch.
4) Wenn wir nun über 1 % CO2 in der Atmosphäre
hätten, wäre eine durch Stossaktivierung bewirkte Temperaturerhöhung plausibel, aber nicht bei 0,04 % CO2 in der Luft, wo 1 CO2 Molekül von über 5000 O2 und N2 Molekülen umgeben ist. Um bei meinem saloppen Beispiel zu bleiben : Können Sie sich vorstellen, dass Sie 5000 Skifahrern gleichzeitig in den Hintern treten, damit diese sich schneller bewegen – mal ehrlich Hand aufs Herz ?!
Langer Rede kurzer Sinn. Sie müssen erst einmal begreifen, was Wärme ist, bevor Sie Daten interpretieren !!!! Herrn Martin Landvoigt möchte ich bei dieser Gelegenheit für einen gelungene Abkürzung danken, wenn man jemanden höflich klar machen möchte, wie wenig man von seinen Aussagen hält. GIGO = Garbage in, garbage out.
„ich bringe da gar nichts durcheinander. Wenn man den „Selbstverbrennungs“-Thesen von Prof. Schellnhuber Glauben schenkt, dann warnt er doch genau vor so etwas, oder wollen Sie das abstreiten?“
Ich kenne das Buch nicht und ich werde es auch nicht lesen. Den Titel nimmt ja wohl keiner ernstlich wörtlich. Ich finde den Titel ziemlich daneben und despektierlich.
„Oder wie definieren Sie „runaway-Effekt“? In der Elektronik spricht man von Mitkopplung bzw. im umgekehrten Fall von Gegenkopplung, das sind vom Prinzip her sehr klar definierte Zusammenhänge. Halten Sie mal ein Mikro vor den Lautsprecher des betreffenden Verstärkers, und Sie werden verstehen, was ich meine.“
Ja, das ist ein runaway-Effekt. In den kommen wir hoffentlich nicht beim anthropogenen CO2. Dies kann passieren, wenn der Ozean keine Senke wie jetzt, sondern eine CO2-Quelle wird. Mag sein, daß einige Projektionne da reinlaufen.
„„Und ich „muss“ nicht zustimmen, dass der Wasserdampfgehalt der Luft mit zunehmender Temperatur zunimmt, das habe ich selbst gesagt.“
Gut, dann haben Sie also die physikalischen Grundlagen des Wasserdampf-feedbacks also verstanden?
„Sie schreiben: „Sie müssen sich aber klarmachen, dass positives Feedback nicht gleich Instabilität bedeutet, das ist ein Irrtum, wie man sich leicht klarmachen kann.“…..und dann kommt leider gar nichts zum „Klarmachen“.“
Nun, ich dachte, das bringen Sie von selbst hin. Wenn Sie sich in Regelungstechnik auskennen, so sollte es Ihnen doch wohl klar sein, wie sich die Verstärkung durch feedback ergibt. Eine Gleichung wie y = y0/(1-f), f feedback sollte Ihnen da schon einmal begegnet sein. Solange f zwischen 0 und 1 liegt ist y endlich, die Verstärkung G = y/y0 ist endlich und größer 1. Die Instabilität bekommen Sie für f gleich 1.
„ An dieser Stelle hätte ich etwas Substanzielleres erwartet, ehrlich.“
Haben Sie jetzt bekommen. Bei Fragen konkret fragen.
„Treibt CO2 die Temperatur, wie Sie es behaupten“
Behaupte ich allgemein ja gar nicht. Das CO2 ist grundsätzlich externer und interner Klimafaktor. Extern ist das anthropogene, intern das aus dem Ozean und der anthropogen nicht beeinflußten Biosphäre. Externe Klimafaktoren werden vom Klima selber nicht beeinflußt und dies trifft beim anthropogenen weitgehend zu. Denn das Handeln des Menschen läßt sich ja schlecht durch Naturgesetze determinieren.
„oder treibt die Temperatur (u.a. über die CO2-Ausgasung der Ozeane) das CO2“
Ja, die Prozesse gibt es auch, wie gesagt, das trifft ein, wenn die Ozeane zur Quelle werden. Aber davon sind wir noch entfernt.
„, wie es diverse intelligente Menschen annehmen?“ Danke, diese wissen aber auch, daß der Anstieg aufgrund des CO2 aus fossilen Quellen zustandekommt.
„Wäre die von Ihnen behauptete CO2-Wasserdampf-Mitkopplung tatsächlich so wie von Ihnen unterstellt wirksam, hätte das Klima der Erde im Laufe ihrer Geschichte ständig gegen katastrophale Endpunkte laufen müssen.“
Nein, aus meinen in #71 genannten Gründen, solange die Auslenkung nicht zu stark ist gibt es noch neue Gleichgewichtsbedingungen.
„Und dass Sie so tun, als sei der menschliche Einfluss auf das Klima auch nur näherungsweise mit Ereignissen wie dem Einschlag des „Dinokiller-Kometen“ vergleichbar, lässt mich ehrlich gesagt nur noch staunen. Glauben Sie das wirklich?“
Nein, das habe ich offensichtlich nicht geschrieben! Klar ist, daß der Kometeneinschlag ein Kippunkt war. Wie ich Sie verstanden habe, waren sie da in #70 noch anderer Meinung:
„Ein wunderschöner Automatismus, der uns seit Milliarden von Jahren ein erstaunlich gleichmäßiges und dem Leben zuträgliches Klima gesichert hat. Selbst monströse Störeinflüsse wie der „Dino-Killer“-Komet vor 65 Mio. Jahren oder die ungeheuren Vulkanausbrüche des Sibirischen Trapps konnten das System nicht destabilisieren.“
Was denn nun?
Aber dieser war natürlich von seiner Art völlig anderes als die möglichen anthropogen ausgelösten Kippunkte. Ich verstehe nicht, wieso Sie das in einen Topf werfen. Ich jedenfalls habe dies offenkundig nicht getan.
„Nun, Hr. Baecker, ich hatte das doch schon genau erklärt: „Ich gehe davon aus, dass dies auf Wasser, genauer gesagt auf Wolkeneffekte zurückzuführen ist: Bei höheren Temperaturen enthält die Atmosphäre mehr Wasser, es bilden sich damit mehr Wolken und damit steigt der Anteil reflektierten Sonnenlichts“.
Ok, aber das Wolken-feedback ist komplizierter als Ihr Modell.
„Zudem stelle ich fest, dass Sie sich mit der Anerkennung eines separaten Wolkeneffekts in Gegensatz zur Lehrmeinung begeben, wie sie z.B. von Ihren AGW-Kumpels bei Wikipedia vertreten wird: Dort wird über Wasserdampf-Rückkopplung und die „sogenannte Eis-Albedo-Rückkopplung“ gesprochen, welche angeblich „die stärkste im Klimageschehen wirkende Rückkopplung“ sein soll. Wolkenrückkopplung wird elegant verschwiegen, warum wohl?“
Keine Ahnung. Gucken Sie einfach mal im IPCC report oder bei Soden & Held nach. Da wird zwischen Wasserdampf- und Wolkenfeedback differenziert. Aber beides ist eng miteinander verknüpft, sodass manche Autoren keine Differenzierung machen. Ebenso das lapse rate feedback. Die Physik macht da keine Grenzlinie, sondern gibt nur ein Gesamtergebnis raus. Man muß durch Modellmodulationen die Effekte in ihre Einzelwirkung ermitteln.
„Zur Klarstellung meinerseits: Der Einfluss der Wolken auf das Reflexionsvermögen (Albedo) der Erde ist deutlich größer als derjenige des Eises. Schauen Sie einfach mal auf Satellitenfotos der Erde und vergleichen Sie Wolkenflächen und Eisflächen. Zudem sind Wolken da, wo die Sonne auch kräftig scheint, während das Eis an den Polen konzentriert ist, wo das halbe Jahr gar kein Sonnenschein hinkommt und auch im Sommer der Einfallswinkel sehr klein ist. Wolken schatten mehr als 50 % der Erdoberfläche ab und reflektieren 20 % der Sonneneinstrahlung direkt zurück ins All. So zumindest Johannes Quaas, Professor für Meteorologie an der Universität Leipzig und Leiter der Arbeitsgruppe „Wolken und globales Klima“. Ihm wird der Satz zugeschrieben. „Wenn der Abschattungseffekt nur um drei Prozent sinken würde, würde dies schon den Treibhauseffekt, den das vom Menschen erzeugte CO2 in der Atmosphäre auslöst, glatt verdoppeln“.
Das stimmt. Das Wolkenfeedback ist wichtig. Das scheinen wir uns einig zu sein. Aber das ist eigentlich alles wissenschaftlich bekannt. Dann hat sich Ihre Recherche ja schon bezahlt gemacht.
Lieber Herr Landvoigt,
ich finde Ihre selbstgefällige Interpretation von anerkannten wissenschaftlichen Methoden erschreckend. Insbesondere der indirekte Vorwurf, die Wissenschaft würde es sich einfach machen. Es sollte schließlich jedem, der sich mit dem Klimawandel beschäftigt bekannt sein, daß man Klimamodelle eben wegen der Komplexität des Klimas macht. Wie Sie zu der Aussage kommen, daß die Wissenschaft nur einfache Antworten hätte, ist mir ein Rätsel. Sie sollten einfach mal in die Wissenschaft einsteigen, statt darüber ohne eine Spur von Faktenwissen zu lamentieren.
Sie scheinen einfache Antworten auf komplee Vorgänge in der Natur haben zu wollen. Da sind Sie bei den Naturwissenschaften falsch. Die Natur erfüllt Ihren Wunsch auf Schichtheit nicht, und das macht sie so interessant.
Sehr geehrter Herr Baecker,
haben Sie die Seiten gewechselt? Denn es ist dach das seit je her vorgetragene Argument, der AGW-Skeptiker, dass man eben nicht das Klima hinreichend versteht, um jedwede Prognose mit annehmbarer Sicherheit zu formulieren. Genau darum ist die AGW-These, die genau behauptet, man könnte nicht nur die Zukunft bestimmen, sondern auch an den Stellrädchen der Maschine drehen.
Nun bestätigen sie die Komplexität, und das einfache Aussagen gar nicht möglich seien. Haben sie mit ihren alten Freunden, die aber geneau das behaupten, nun gebrochen oder ihnen mangelnde Wissenchaftlichkeit vorgeworfen?
Es ist ihnen sicher nicht entgangen, dass Herr Müller sie nur beim Wort genommen hat, und die Konsequenzen folgerichtig einforderte.
———— #68: NicoBaecker sagt:
Gehen Sie lieber in die Kirche, da bekommen Sie Antworten, die Ihnen gefallen werden: schön einfach und passend auch auf Fragen, die man gar nicht fragte.
Abgesehen davon, dass in der Kirche heutzutage allzu oft AGW gepredigt wird, ist es nicht nett, sich über jene lustig zu machen, die genau das glauben und den Worten der AGW-Propheten folgen.
Die Kirchen räumen aber zumindest ein, dass sie das Wagnis des Glaubens eingehen, auch wenn sie eben nicht über hinreichend belegbares Faktenwissen verfügen. Diese Ehrlichkeit macht mir die Kirche, selbst wenn sie dem Klimawahn folgt, noch sympathischer als Jene, die sich auf die Wissenschaft berufen und behaupten, dass die kommende Klima-Katastrophe eben keine Frage des persönlichen Glaubens sei.
„Wenn Sie mich auffordern, nicht zu interpretieren, müssen sie präzisere Angaben machen. “
Nein, nachfragen!! Interpretieren ist zu irrtumsträchtig.
Sehr geehrter Herr Baecker
Genau darum habe ich nachgefragt. Ihre Antwort wirkte eher wie eine Flucht.
———— #66: NicoBaecker sagt:
Gucken Sie sich doch an, was zu für einen Zwergenaufstand Sie aufgrund Ihrer Interpretation von meines „bodennah“ veranstaltet haben.
Ich hab Sie erwischt, und jetzt wollen Sie sich raus winden. Wen wollen Sie da noch überzeugen? Sich selbst?
———— #66: NicoBaecker sagt:
Ich kann Sie letzlich nur an die Modellergebnisse verweisen,
Ein Bisschen schwach,mit GIGO Modellen zu kommen.
———— #66: NicoBaecker sagt:
…. da finden Sie es dann numerisch, denn deskriptiv, wie wir das hier machen, ist es immer eine begrenzt genaue Zusammenfassung quantitativer Ergebnisse.
Empirisch belegt?
———— #66: NicoBaecker sagt:
„Ich habe sie aber korrekt verstanden, dass sie von keiner Veränderung des mittleren Temperaturgradienten, der lapse rate ausgehen?“
Genau genommen gibt es neben dem grundlegenden Effekt der effktiven Anhebung der Tropopause auch ein lapse rate-feedback. Aber das könne Sie fürs grundlegende erständnis erstmal vernachlässigen.
Ihres eigenen Verständnisses oder des Verständnisses ihres Textes?
———— #66: NicoBaecker sagt:
„Ich habe sie aber korrekt verstanden, dass sie von keiner Veränderung des mittleren Temperaturgradienten, der lapse rate ausgehen?“
Fürs grundlgende Verständnis können Sie erstmal näherungsweise davon ausgehen. Die Modelle geben natürlich komplexere Ergebnisse raus, dazu gehört auch eine Änderung der lapse rate, besonders in den Tropen.
Dann müsste es empirische Belege dafür geben. Wo sind die?
nochmal zu den Fakten (nachzulesen in der wissenschaftlichen Literatur):
Sie bringen einiges durcheinander. Der runaway-Greenhouse Effekt wird beim Anstieg der Sonneneinstrahlung aufgrund der sonnenentwicklung in ein paar Mrd. Jahren erwartet.
Ich habe dies nur als Beispiel für das Wasserdampf-feedback aufgeführt. Die entsprechenden Veröffentlichungen gibt es seit den 70igern, z.B. Rasool, Bergh, Schneider.
Sie werden zustimmen müssen, daß der Wasserdampf-Gehalt mit zunehmender Temperatur zunimmt, daher ist dieses feedback grundsätzlich im Klimasystem implementiert. Das sieht man am einfachsten an der Dampfdruckkurve.
Der Auslöser fürs feedback kann ganz verschieden sein, siehe Beispielliste oben. Sie müssen sich aber klarmachen, daß positives Feedback nicht gleich Instabilität bedeutet, das ist ein Irrtum, wie man sich leicht anhand der Verstärkergleichung klarmachen kann.
In der Erdvergangenheit gab es schon einige Mal Kippunkte im Klima, die von der gleichen Größenordnung und Dauer waren wie sie im Zusammenhang mit dem anthropogene Klimawandel diskutiert werden, also einige tausende Jahre Dauer und Klimaänderungen, die zu Umwälzungen im Ökosystem führen. Der Einschlag des eines Kometen vor 65 Mio. Jahren war definitiv ein Kippunkt. Schließlich sind seine Auswirkungen noch als geologischer Facieswechsel überliefert und begründete das Ende des Mesozoikums.
Neben dem Wasserdampf-feedback gibt es auch ein Wolkenfeedback. Ich hatte diese beiden schon oben getrennt genannt. Sie müssen die beiden Effekte auseinander halten.
Lieber Hr. Baecker,
ich bringe da gar nichts durcheinander. Wenn man den „Selbstverbrennungs“-Thesen von Prof. Schellnhuber Glauben schenkt, dann warnt er doch genau vor so etwas, oder wollen Sie das abstreiten? Oder wie definieren Sie „runaway-Effekt“? In der Elektronik spricht man von Mitkopplung bzw. im umgekehrten Fall von Gegenkopplung, das sind vom Prinzip her sehr klar definierte Zusammenhänge. Halten Sie mal ein Mikro vor den Lautsprecher des betreffenden Verstärkers, und Sie werden verstehen, was ich meine.
Und ich „muss“ nicht zustimmen, dass der Wasserdampfgehalt der Luft mit zunehmender Temperatur zunimmt, das habe ich selbst gesagt. Ich kenne da jemanden, der sich mit der Sensorik und Steuerungstechnik von z.B. Anlagen beschäftigt, welche die Luftfeuchtigkeit im Gebäude begrenzen. Von solchen Leuten kann man eine Menge lernen.
Sie schreiben: „Sie müssen sich aber klarmachen, dass positives Feedback nicht gleich Instabilität bedeutet, das ist ein Irrtum, wie man sich leicht klarmachen kann.“…..und dann kommt leider gar nichts zum „Klarmachen“. An dieser Stelle hätte ich etwas Substanzielleres erwartet, ehrlich.
Und dass das Klima als Multi-Parameter-System von mehr als nur einem Faktor abhängt, ist selbstverständlich. Aber welches sind die Treiber, und welches sind die Folger? Treibt CO2 die Temperatur, wie Sie es behaupten, oder treibt die Temperatur (u.a. über die CO2-Ausgasung der Ozeane) das CO2, wie es diverse intelligente Menschen annehmen? Wäre die von Ihnen behauptete CO2-Wasserdampf-Mitkopplung tatsächlich so wie von Ihnen unterstellt wirksam, hätte das Klima der Erde im Laufe ihrer Geschichte ständig gegen katastrophale Endpunkte laufen müssen. Schließlich gab es genügend massive Störimpulse. Bei Regelungstechnikern stoßen Sie mit Ihren Thesen garantiert auf Unglauben.
Und dass Sie so tun, als sei der menschliche Einfluss auf das Klima auch nur näherungsweise mit Ereignissen wie dem Einschlag des „Dinokiller-Kometen“ vergleichbar, lässt mich ehrlich gesagt nur noch staunen. Glauben Sie das wirklich?
Naja, und dann schreiben Sie auch noch folgende ermahnende Worte: „Neben dem Wasserdampf-feedback gibt es auch ein Wolkenfeedback. Ich hatte diese beiden schon oben getrennt genannt. Sie müssen die beiden Effekte auseinander halten.“
Nun, Hr. Baecker, ich hatte das doch schon genau erklärt: „Ich gehe davon aus, dass dies auf Wasser, genauer gesagt auf Wolkeneffekte zurückzuführen ist: Bei höheren Temperaturen enthält die Atmosphäre mehr Wasser, es bilden sich damit mehr Wolken und damit steigt der Anteil reflektierten Sonnenlichts“.
Zudem stelle ich fest, dass Sie sich mit der Anerkennung eines separaten Wolkeneffekts in Gegensatz zur Lehrmeinung begeben, wie sie z.B. von Ihren AGW-Kumpels bei Wikipedia vertreten wird: Dort wird über Wasserdampf-Rückkopplung und die „sogenannte Eis-Albedo-Rückkopplung“ gesprochen, welche angeblich „die stärkste im Klimageschehen wirkende Rückkopplung“ sein soll. Wolkenrückkopplung wird elegant verschwiegen, warum wohl?
Zur Klarstellung meinerseits: Der Einfluss der Wolken auf das Reflexionsvermögen (Albedo) der Erde ist deutlich größer als derjenige des Eises. Schauen Sie einfach mal auf Satellitenfotos der Erde und vergleichen Sie Wolkenflächen und Eisflächen. Zudem sind Wolken da, wo die Sonne auch kräftig scheint, während das Eis an den Polen konzentriert ist, wo das halbe Jahr gar kein Sonnenschein hinkommt und auch im Sommer der Einfallswinkel sehr klein ist. Wolken schatten mehr als 50 % der Erdoberfläche ab und reflektieren 20 % der Sonneneinstrahlung direkt zurück ins All. So zumindest Johannes Quaas, Professor für Meteorologie an der Universität Leipzig und Leiter der Arbeitsgruppe „Wolken und globales Klima“. Ihm wird der Satz zugeschrieben. „Wenn der Abschattungseffekt nur um drei Prozent sinken würde, würde dies schon den Treibhauseffekt, den das vom Menschen erzeugte CO2 in der Atmosphäre auslöst, glatt verdoppeln“.
Oder eben mehr als aufheben, je nach Vorzeichen.
Mfg
Ich habe gerade einen gepfefferten Text verfasst:
zu @71 Nico Baecker.
Zum Thema „Kippunkte“ aus mathematischer und chaostheoretischer Sicht.
Irgendwie komisch, sein Kommentar ist verschwunden!?!
Habt Ihr ein Serverproblem?
MfG
Christian Ohle
Lieber Hr. Baecker,
in einem Punkt gebe ich Ihnen Recht: „Wenn die Sonne in ein paar Milliarden Jahren….“
Gratulation, damit dürften Sie den Reichweitenrekord bei Klimavorhersagen locker für sich in Anspruch nehmen können. Das haben weder Schellnhuber noch Rahmstorf und Konsorten bisher geschafft, da ging es fast immer nur um Jahrzehnte und ab und an mal um Jahrhunderte.
Die Sonne wird uns irgendwann in ganz, ganz ferner Zukunft verbrennen, das erkenne ich als aktuell geltenden Stand der Wissenschaft an.
Aber das hat nichts mit dem aktuellen Klimageschehen zu tun. Der von Ihnen aufgeführte „runaway-greenhouse effect“ ist nach meiner Einschätzung eine Fehlinterpretation. Sonst wäre das schon längst der Fall gewesen, schließlich hatten wir schon Klimaperioden auf der Erde, in denen es deutlich wärmer war als heute, und nichts ist passiert, trotz damals auch noch zusätzlich höherem CO2-Gehalt. Es gibt offensichtlich in der Atmosphäre eine negative Rückkopplung, die verhindert, dass das System in irgendeiner Richtung „davonläuft“. Damit ist auch das „Kipppunkt“-Gerede von Schellnhuber hinfällig.
Ich gehe davon aus, dass dies auf Wasser, genauer gesagt auf Wolkeneffekte zurückzuführen ist: Bei höheren Temperaturen enthält die Atmosphäre mehr Wasser, es bilden sich damit mehr Wolken und damit steigt der Anteil reflektierten Sonnenlichts. Umgekehrt lässt dieser „Sonnenschutz“ nach, wenn es kühler wird und das Wasser abregnet. Ein wunderschöner Automatismus, der uns seit Milliarden von Jahren ein erstaunlich gleichmäßiges und dem Leben zuträgliches Klima gesichert hat. Selbst monströse Störeinflüsse wie der „Dino-Killer“-Komet vor 65 Mio. Jahren oder die ungeheuren Vulkanausbrüche des Sibirischen Trapps konnten das System nicht destabilisieren. Wenn dann Mr. Schellnhuber angesichts von ein paar ppm CO2 von „Selbstverbrennung“ schwafelt, kann ich mich leiser Zweifel („Leugner, Leugner, auf den Scheiterhaufen mit ihm“) nicht erwehren.
Ich komme zu gegebener Zeit auf das Thema zurück. Ich versuche zu verstehen, wieso intelligente Menschen – und damit schliesse ich auch Sie ein – eine auf solch wackligen Annahmen und Voraussetzungen beruhende Vorstellung wie die „AGW“-Theorie ernsthaft vertreten können.
Mfg
ein Beispiel („Butter“) für Wasserdampf-Feedback mit externe Auslösefaktor ist der sog. „runaway-greenhouse effect“: in ein paar Mrd. Jahren nimmt die Solarkonstante aufgrund zunehmender Abstrahlleistung der Sonne zu (Sternentwicklung). Der damit verknüpfte Temperaturanstieg läßt mehr Wasser verdampfen, der Treibhauseffekt verstärkt sich. Im Unterschied zu anthropogenen CO2-Anstieg wird der solare Anstieg ausreichen, daß der durchs Wasserdampf-feedback verstärkte Treibhauseffekt zur vollständigen Verdunstung der Ozeane führt.
„Statt konkreter Erläuterungen kommen Sie mit Sprüchen wie „Den aktuellen Kenntnisstand dazu finden Sie im aktuellen IPCC report.“
Wie gesagt, Sie müssen zunächst einmal eine Frage stellen. Wenn Sie nicht mal wissen, was Sie wissen wollen, ist es ja ganz schlecht für Sie.
„“Die Ungenauigkeit führt zur bekannten Modellstreuung“. Das ist keine seriöse Art der Diskussion.“
Das ist auch nichts zum Diskutieren, sondern ein Ergenis.
„statt klipp und klar zu sagen, was Sie meinen und wo genau es in diesem riesigen Papierberg des IPPC steht.“
Sie scheinen einfache Antworten auf komplee Vorgänge in der Natur haben zu wollen. Da sind Sie bei den Naturwissenschaften falsch. Die Natur erfüllt Ihren Wunsch auf Schichtheit nicht, und das macht sie so interessant. Gehen Sie lieber in die Kirche, da bekommen Sie Antworten, die Ihnen gefallen werden: schön einfach und passend auch auf Fragen, die man gar nicht fragte.
„Und wissenschaftlich ist Ihre Vorgehensweise erst recht nicht.“
Aber die Ergebnisse sind es. Ich kann Ihnen schlecht etwas erklären, wenn Sie nicht wissen, was Sie wissen wollen. Formulieren Sie mal eine wissenschaftliche Frage.
„Und was Sie zum Thema externem Klimafaktor und Feedback in # 61 erzählen, kaufe ich Ihnen auch nicht ab.“
Ist aber eigentlich für einen Geowissenschaftler, Meteorologien oder Physiker völlig offensichtlich. Sie werden mir kaum einen sachgerechten Grund nennen können, warum dies nicht offensichtlich ist.
Sie wollen doch bitte nicht vertreten, daß das Klimasystem nur bei Änderung des CO2-Gehalts feedbacks zeigt und bei Änderungen von anderen externen Klimafaktoren nicht.
„Die tatsächlichen CO2-Kreisläufe auf dieser schönen Erde sind nicht ausreichend bekannt und werden u.a. in Grafiken des IPCC nicht korrekt wiedergegeben.“
Den CO2-Kreislauf haben wir offensichtlich gar nicht zu Diskussion gehabt. Es ging um die feedbacks des Wasserdampfs. Wenn Sie das schon durcheinanderwerfen, müssen Sie sich die Sachlage auf noch elemtarerer Ebene klarmachen und noch kleinere Brötchen backen.
„die man beim IPCC als felsenfest gegeben annimmt und zur Grundlage der Modellierungen macht, eben alles andere als eindeutig und klar definiert sind.“
Herr Mueller, Sie wissen doch gar nicht, was man weiß. Was wollen Sie denn darüber schon urteilen? Wenn man keine Ahnung hat, so muß einem jede naturwissenschaftliche Binsenweisheit als „felsenfest“ erscheinen.
Und war 2014 tatsächlich das wärmste Jahr, zwar laut NASA-Forscher Gavin Schmidt ohne Zweifel. Nur hatte er vergessen zu erwähnen, nur mit einer Wahrscheinlichkeit von 38 %.
Sehr geehrter Herr Haase
M.E. geht die Wahrscheinlichkeit eher gegen 0 % – vor allem, wenn man die satellitengestützten Messungen heran zieht. http://tinyurl.com/q4de624
2014 war ein unauffälliges Jahr. Der Oktober 2015 stand ganz im Zeichen des El Nino, aber der scheint auch wieder zu ende zu gehen.
————– #63: Uwe Haase sagt:
Wird denn so ein gigantischer Aufwand gegen die Fossilien Energien betrieben?
Ja ! Man denke nur an die kommende Abschaltung der Braunkohlkraftwerke und weitere Stillegungspläne für Steinkohlekraftwerke, obwohl wir noch nicht einmal einen Ersatz für die Grundlast haben. Die Werbe-Maschine gegen die Fossilen läuft auf allen Kanälen.
————– #63: Uwe Haase sagt:
Die Wirklichkeit sieht etwas anders aus. Wir starten, weil wir aus den Politik- und Sozialwissenschaften kommen, ein Projekt im Jahre 2016, das sich mit dem politischen und sozialen Folgen der Klimawandelthematik beschäftigt.
Das widerlegt die These nicht, sondern macht nur Hoffnung, dass die Volksverdummung nicht total wirkt.
„Wenn Sie mich auffordern, nicht zu interpretieren, müssen sie präzisere Angaben machen. “
Nein, nachfragen!! Interpretieren ist zu irrtumsträchtig. Gucken Sie sich doch an, was zu für einen Zwergenaufstand Sie aufgrund Ihrer Interpretation von meines „bodennah“ veranstaltet haben. Fragen Sie einfach nach, wenn Sie es genauer haben wollen. Ich kann Sie letzlich nur an die Modellergebnisse verweisen, da finden Sie es dann numerisch, denn deskriptiv, wie wir das hier machen, ist es immer eine begrenzt genaue Zusammenfassung quantitativer Ergebnisse.
„Ich habe sie aber korrekt verstanden, dass sie von keiner Veränderung des mittleren Temperaturgradienten, der lapse rate ausgehen?“
Genau genommen gibt es neben dem grundlegenden Effekt der effktiven Anhebung der Tropopause auch ein lapse rate-feedback. Aber das könne Sie fürs grundlegende erständnis erstmal vernachlässigen.
„Ich habe sie aber korrekt verstanden, dass sie von keiner Veränderung des mittleren Temperaturgradienten, der lapse rate ausgehen?“
Fürs grundlgende Verständnis können Sie erstmal näherungsweise davon ausgehen. Die Modelle geben natürlich komplexere Ergebnisse raus, dazu gehört auch eine Änderung der lapse rate, besonders in den Tropen.
„Oder ist das doch ihre These?“
Nein, ein Ergebnis.
Lieber Hr. Baecker,
mit ihrem schnodderigen Gehabe punkten Sie bei mir nicht im Geringsten. Statt konkreter Erläuterungen kommen Sie mit Sprüchen wie „Den aktuellen Kenntnisstand dazu finden Sie im aktuellen IPCC report. Die Ungenauigkeit führt zur bekannten Modellstreuung“. Das ist keine seriöse Art der Diskussion. Sie weichen aus und schmeißen mit großen Papiermengen um sich, statt klipp und klar zu sagen, was Sie meinen und wo genau es in diesem riesigen Papierberg des IPPC steht. Und wissenschaftlich ist Ihre Vorgehensweise erst recht nicht.
Und was Sie zum Thema externem Klimafaktor und Feedback in # 61 erzählen, kaufe ich Ihnen auch nicht ab. Die tatsächlichen CO2-Kreisläufe auf dieser schönen Erde sind nicht ausreichend bekannt und werden u.a. in Grafiken des IPCC nicht korrekt wiedergegeben. Davon können Sie sich gerne hier bei EIKE selbst überzeugen, siehe z.B. die beiden Aufsätze „Die Große Dekarbonisierungslüge“ (Teil 1 und Teil 2), die hier kürzlich erschienen sind. Stattdessen wird von Seiten der KLimaalarmisten – zu denen sie ja gehören – mit Annahmen und Tricks wie dem „Revelle-Faktor“ gearbeitet, um zu übertünchen, dass man gar nicht wirklich weiß, wie die konkreten Verhältnisse sind. Das, Herr Baecker, sollten Sie sich glasklar machen, verstehen Sie? Weil Sie genau daran scheitern werden, dass Dinge, die man beim IPCC als felsenfest gegeben annimmt und zur Grundlage der Modellierungen macht, eben alles andere als eindeutig und klar definiert sind. Seriöse Wissenschaftler würden das anerkennen und zugeben, dass ihre Berechnungen mit entsprechend (extrem) hohen Unsicherheiten behaftet sind.
Mfg
„“Durch den zunehmenden Treibhauseffekt wird es bodennah wärmer. Das sieht man am besten in zeitlichen Mittelwerten.“
Ihre Formulierung ist unklar.“
Meine Formulierung beschreibt die grundsätzlich Wirkung.
„Man könnte darunter verstehen, dass sich dadurch die lapse rate verändern würde, denn ansonsten würde die Betonung auf die Bodennähe wenig Sinn machen.“
Ihr „könnte Verstehen“ folgt aber nicht logisch aus meiner Aussage. Ihre spekulative Folgerung trifft zudem auch nicht zu.
„Meinten sie dagegen, dass die Lapse Rate sich nicht signifikant ändert, müsste eine Temperaturerhöhung in der gesamten Lower Troposphere nachweisbar sein, nicht nur bodennah.“
lower troposphere = bodennah. Bitte interpretieren Sie nicht.
Von der Sonne aus gesehen ist die Lower Troposphere tatsächlich bodennah. Aus menschlicher Perspektive würde man Bodennah eher mit bis zu 30 cm über dem Boden beschreiben, eher nicht in 2 m Höhe und mit Sicherheit nicht 2000 m Höhe.
Das aber ist der Messbereich der Satellitenmessungen der lower Troposphere, die von einigen abgelehnt werden als nicht-bodennah.
Wenn Sie mich auffordern, nicht zu interpretieren, müssen sie präzisere Angaben machen. Sonst macht ihre Aussage gar keinen Sinn.
Ich habe sie aber korrekt verstanden, dass sie von keiner Veränderung des mittleren Temperaturgradienten, der lapse rate ausgehen? Oder ist das doch ihre These?
http://tinyurl.com/lsr87rg
Das Britische MetOffice trickst an dieser Stelle weniger: „The HadCRUT4 dataset (compiled by the Met Office and the University of East Anglia’s Climatic Research Unit) shows last year was 0.56C (±0.1C) above the long-term (1961-1990) average. Nominally this ranks 2014 as the joint warmest year in the record, tied with 2010, but the uncertainty ranges mean it’s not possible to definitively say which of several recent years was the warmest.“ http://tinyurl.com/p69csnv
#52 Hallo Herr Michelbach,
vielen Dank für den Hinweis. Sind zwar regionale Daten, aber doch ein Indikator, die „Isolierung“ ist in einigen Regionen zumindest schwer nachweisbar.
#53 Hallo Herr Landvoigt,
Wird denn so ein gigantischer Aufwand gegen die Fossilien Energien betrieben? Die Wirklichkeit sieht etwas anders aus. Wir starten, weil wir aus den Politik- und Sozialwissenschaften kommen, ein Projekt im Jahre 2016, das sich mit dem politischen und sozialen Folgen der Klimawandelthematik beschäftigt. Wir haben bereits eine Einleitung veröffentlicht und u. a. ein Video von zwei 15. jährigen Schülerinnen zum Thema „Schattenseiten der Erneuerbaren Energien.“ Wir finden dieses Video sehr bemerkenswert, weil einer der Schülerinnen z. B. mir gegenüber meinte, in den Fächern Gesellschaftslehre und Erdkunde wird RTL-Wissen vermittelt. Selbst Jugendliche spüren, irgendetwas stimmt nicht. Statt Pluralismus wird in den Schulen parteipolitische Gleichmacherei betrieben. Aber diese Schüler sind die Wähler von Morgen, oder sollten es sein. Es könnte auch leicht sein, dass sie sich angewidert umdrehen.
http://tinyurl.com/z6cbekk
noch zu:
„Warum Sie dann meinen, CO2 sei der primäre Faktor und Wasserdampf samt Wolken sei lediglich ein „Folger“, erschliesst sich mir nicht. Das ist so, als ob Sie behaupteten, der kupierte Schwanz würde mit dem 35-Kilo-Boxerrüden wedeln.“
Wie gesagt, aufwendige Klimamodelle sind deswegen notwendig, um die Reaktion des Klimasystems auf einen externen Klimafaktor auszurechnen. Denn -wie gesagt- dieses feedback (d.h. die Reaktion des Klimasystems) ist im Gesamtergebnis nicht vernachlässigbar, sondern von der gleichen Größenordnung wie die primäre Urache. Solche feedabcks sind das Wasserdampf-feedback und das Wolken-feedback, und einige mehr (siehe IPCC und wissenschaftliche Primärliteratur).
Wie ebenfalls hinlänglich oft erklärt müssen die feedbacks bei jedem externe Klimafaktor eingerechnet werden. Das gilt nicht nur bei der extern verursachten CO2-Erhöhung, sondern auch bei einer Änderung der Erdbahnparameter, der Solarkonstante, Aerosolgehalt aufgrund von Vulkanismus, Änderung der Land-Meerverteilung (Albedo) etc…. Ohne Einbeziehung dieser feedbacks ließen sich auch die Eiszeiten nicht aufgrund der Änderung der solaren Einstrahlung alleine erklären.
Sie müssen sich den Unterschied zwischen exterem Klimafaktor und feedback(=interne Reaktion des Systems) glasklar machen. Verstehen Sie? Sonst werden Sie scheitern.
„Nun, Sie haben hiermit zugegeben, dass Wasserdampf und nicht CO2 das wichtigste Treibhausgas ist.“
Das ist offensichtlich, gucken Sie ins Lehrbuch.
„Was Sie jedoch aussen vorlassen ist die Tatsache, dass Wasserdampf auch Wolken bildet und dann mehr als 80 % der gesamten Strahlungsflüsse blockieren kann, und zwar in beide Richtungen. Das ist nach meinem Kenntnisstand in keinem Modell richtig berücksichtigt.“
Doch natürlich ist dies berücksichtigt, gerade für die komplexen Prozesse wie Wolkenbildung benötigt man doch gerade Klimamodelle. Das cloud-feedback ist neben dem vapor-feedback das wichtigste feedback und nur durch raumlich auflösende GCMs zu berechnen.
„Dabei handelt es sich hier um Effekte, die um Grössenordnungen über dem liegen, was man dem CO2 zuschreibt.“
Weder das cloud-feedback alleine noch das Gesamtfeedback ist Größenordnungen stärer. Wie ich aber bereits geschrieben habe, liegen die Effekte in der gleichen Größenornung und daher muß man sie berücksichtigen. Das geht wie gesagt nur durch GCMs und nicht durch naive Abschätzung.
„Nach meinem Kenntnisstand kann man die Bildung von Wolken nicht mit ausreichender Zuverlässigkeit modellieren.“
Den aktuellen Kenntnisstand dazu finden Sie im aktuellen IPCC report. Die Ungenauigkeit führt zur bekannten Modellstreuung.
„Warum Sie dann meinen, CO2 sei der primäre Faktor und Wasserdampf samt Wolken sei lediglich ein „Folger“, erschliesst sich mir nicht.“
Da enttäuschen Sie mich. Ich hätte schon gedacht, daß Ihnen der Unterschied zwischen Ursache und Folge klar ist.
Lieber Hr. Baecker,
Sie schreiben:“Natürlich ist Wasserdampf das wichtigste Treibhausgas, ebenso einsichtig ist, daß der Wasserdampfanteil in der Atmosphäre steigt, wenn die Temperatur zunimmt. Das Wasserdampf-feedback ist grundsätzlich also nicht abwegig. Der Punkt ist natürlich, wie man dies quantifiziert, man kann dies grob überschlagen, aber da wir hier nicht nur eine Größenordnung abschätzen wollen, sondern einen Fehler von weniger als einen Faktor 2, muß man schon möglichst viel der interagierenden Prozesse in der Atmosphäre mitberücksichtigen, denn diese können gegenläufige Effekte derselben Größenordnung induzieren und eine naive grobe Abschätzung konterkarieren, und dies geht am besten mit Klimamodellen. Wenn Sie also an der Quantifizierung der feedbacks interessiert sind, müssen Sie sich die implementierte Physik der Klimamodelle angucken. Denn darauf basieren die heutigen Angaben.“
Nun, Sie haben hiermit zugegeben, dass Wasserdampf und nicht CO2 das wichtigste Treibhausgas ist. Was Sie jedoch aussen vorlassen ist die Tatsache, dass Wasserdampf auch Wolken bildet und dann mehr als 80 % der gesamten Strahlungsflüsse blockieren kann, und zwar in beide Richtungen. Das ist nach meinem Kenntnisstand in keinem Modell richtig berücksichtigt. Dabei handelt es sich hier um Effekte, die um Grössenordnungen über dem liegen, was man dem CO2 zuschreibt.
Nach meinem Kenntnisstand kann man die Bildung von Wolken nicht mit ausreichender Zuverlässigkeit modellieren.
Warum Sie dann meinen, CO2 sei der primäre Faktor und Wasserdampf samt Wolken sei lediglich ein „Folger“, erschliesst sich mir nicht. Das ist so, als ob Sie behaupteten, der kupierte Schwanz würde mit dem 35-Kilo-Boxerrüden wedeln.
Mfg
„lower troposphere = bodennah.“
bodennah = near surface
Was darfs denn sein? Prandtl-Schicht, Ekman-Schicht oder vielleicht Peplosphäre?
Noch haben Sie freie Auswahl. Auch beim Parken auf dem Fischmarkt. Und der Klimawandel findet ja nicht statt, so daß das Auto ja nicht weggespült werden kann.
„“Durch den zunehmenden Treibhauseffekt wird es bodennah wärmer. Das sieht man am besten in zeitlichen Mittelwerten.“
Ihre Formulierung ist unklar.“
Meine Formulierung beschreibt die grundsätzlich Wirkung.
„Man könnte darunter verstehen, dass sich dadurch die lapse rate verändern würde, denn ansonsten würde die Betonung auf die Bodennähe wenig Sinn machen.“
Ihr „könnte Verstehen“ folgt aber nicht logisch aus meiner Aussage. Ihre spekulative Folgerung trifft zudem auch nicht zu.
„Meinten sie dagegen, dass die Lapse Rate sich nicht signifikant ändert, müsste eine Temperaturerhöhung in der gesamten Lower Troposphere nachweisbar sein, nicht nur bodennah.“
lower troposphere = bodennah. Bitte interpretieren Sie nicht.
Eine Verstärkung des atmosphärischen Treibhauseffektes bewirkt eben die Erwärmung der bodenanhen Schichten, hingegen die Abkühlung der oberen.
„ich hatte gefragt: „„Wasserdampf ist anerkanntermassen ein starkes Treibhausgas und in Verbindung mit Wolkenbildung ein entscheidender Faktor beim „Treibhauseffekt“. Da hätte ich von Ihnen doch etwas mehr erwartet als ein Schulterzucken.““
Wo ist da die Frage?
„Sie antworten: „Zweifellos, wenn Sie das auch interessiert, so müssen Sie auch recherchieren. Dieses Thema ist natürlich viel umfangreicher als Ihre Anfrage oben. Wenn Sie auch da was haben, so informieren Sie doch auch alle“
Nun Hr. Baecker, ich stelle hier zunächst fest, dass Sie kneifen. Nicht weil Sie es nicht könnten, sondern weil Sie genau wissen, dass ich genau die Schwachstelle Ihres gesamten Lügengebäudes angepickt habe.“
Herr Müller, mir gefällt Ihre unruhige und abfällige Art nicht. Haben Sie überhaupt aufrichtiges Interesse auf Antworten auf Sachfragen?
Wenn ja, so stellen Sie doch einfach Ihre Fragen, aber bitte konkret. Was Ihre obige Frage zur Korrelation zwischen Tagesminimum und Luftfeuchtigkeit und Bewölkung angeht, kann ich Ihnen wie gesagt nichts liefern, denn diese Frage halte ich für sehr speziell. Das müssen Sie selber recherchieren.
Ich habe Ihnen schon einmal gesagt, daß ich nicht Ihr Wikipedia bin. Ich kann Ihnen aber gerne mitteilen, was ich weiß. Aber dazu müßten Sie mich erstmal fragen.
Natürlich ist Wasserdampf das wichtigste Treibhausgas, ebenso einsichtig ist, daß der Wasserdampfanteil in der Atmosphäre steigt, wenn die Temperatur zunimmt. Das Wasserdampf-feedback ist grundsätzlich also nicht abwegig. Der Punkt ist natürlich, wie man dies quantifiziert, man kann dies grob überschlagen, aber da wir hier nicht nur eine Größenordnung abschätzen wollen, sondern einen Fehler von weniger als einen Faktor 2, muß man schon möglichst viel der interagierenden Prozesse in der Atmosphäre mitberücksichtigen, denn diese können gegenläufige Effekte derselben Größenordnung induzieren und eine naive grobe Abschätzung konterkarieren, und dies geht am besten mit Klimamodellen. Wenn Sie also an der Quantifizierung der feedbacks interessiert sind, müssen Sie sich die implementierte Physik der Klimamodelle angucken. Denn darauf basieren die heutigen Angaben.
„Formal korrekt, jedoch am Tjhema vorbei.“
Entschuldigung, ich stimme Ihnen ja zu, daß die Klimaphysik interessanter ist als die altbackene Physik der Strahlungsgesetze und das Infrarotspektrum von CO2 und wie der atmosphärische Treibhauseffekt funktioniert. Aber Sie haben kaum das Recht, Herrn Reiters Frage umzudeuten. Überlassen Sie es Herrn Reiter, seine Frage so zu stellen, wie er es meint.
Kommen wir zu Ihrer Frage, wie eine Störung in der Größenordnung Energiebilanz von ein paar Watt pro Quadratmeter über Jahre auf Klimasystem wirkt (dies ist die Größenordnung der Summe der Differenz heute gegen vor 250 Jahren).
Diese Frage habe ich schon oft genug erläutert, und Sie waren dabei. Aber ich kann Ihnen auch gerne mein Wissen dazu mitteilen.
Ich schrieb:
„Wie das feedback des Klimasystem auf ein initiales radiative forcing reagiert, ist ja eine Frage, die mit der Physik des CO2 nichts zu tun hat, sondern mit der Physik des Klimasystems. Falsch wäre dabei die Vorstellung, daß das Ergebnis dieses feedbacks eine Naturkonstante wäre.“
Sie interessiert dabei:
„Auch ist es nicht die Frage, ob es sich um eine Naturkonstante handelt, sondern um die praktischen Auswirkungen, die man als Grundlage für eine Berechnung, z.B. des 2-Grad-Ziels, in ansatz bringen muss. Ohne diese zuverlässig treffen zu können, machen fast alle AGW-Diskussionen gar keinen Sinn. Denn ist dies CO2-Sensitvität real tatsächlich mit < 0,5 Grad anzusetzen, ist der Klimaschutz ein absurdes Unterfangen, für das weder Geld nicht Zeit sinnvoll einzusetzen ist.“ Ja richtig. Wenn die Klimasensitivität für die Reaktion des Klimasystems auf der Zeitskala von 100 Jahren kleiner als 0,5 °C wäre, so könnten wir wohl nochmal so viel CO2 in die Luft abgeben wie in den letzten 250 Jahren. Aber die Wahrscheinlichkeit für diesen Fall ist nach heutigem Wissen sehhhr klein. Also warum Kamikaze spielen? Die nach heutigen wissenschaftlichen Maßstäben besten Einschätzungen der Klimasensitivität finden Sie im AR5.
Ich versuche mal plastisch die Atmosphäre mit einer Hausisolierung zu vergleichen. Es strömt Wärme ins Haus, aber durch „Verstärkung“ der Isolierung mittels CO2 und anderen Gasen wird immer mehr Wärme im Haus zurückbehalten. So müssten doch langsam die Temperaturschwankungen im Haus reduziert werden. Ich weiß, dies ist auf das Minimum an beteiligten Faktoren reduziert. Aber unabhängig wie warm es am Tag war, müsste es doch so an einer Langzeitstatistik erkennbar sein, ob der Abstand zwischen Höchst- und Tiefsttemperaturen sich verändert.
Sehr geehrter Herr Haase
Ich finde ihren Ansatz sehr gut und plausibel. Denn man könnte den CO2 Anteil tatsächlich mit einer Isolierung vergleichen.
Allerdings sind die Systeme so komplex, dass man deren Nachweis möglicherweise so nicht zwingend erbringen kann. Denn durch Wind, Wolkenbedeckung und Wasserdampfanteile wird im Modell ein starkes Rauschen erzeugt. Das heißt, der Signal-Rauschabstand ist sehr schlecht für einen Nachweis.
Im Gegenzug kann man dann allerdings fragen: Wenn die Wirkungen so schwer nachzuweisen sind, ist es dann sinnvoll wegen nicht nachweisbarer Effekte einen so gigantischen Aufwand zu betreiben?
Durch den zunehmenden Treibhauseffekt wird es bodennah wärmer. Das sieht man am besten in zeitlichen Mittelwerten.
Sehr geehrter Herr Baecker
Ihre Formulierung ist unklar. Man könnte darunter verstehen, dass sich dadurch die lapse rate verändern würde, denn ansonsten würde die Betonung auf die Bodennähe wenig Sinn machen. In diesem Fall sollten aber untersuchungen genau das belegen können.
Meinten sie dagegen, dass die Lapse Rate sich nicht signifikant ändert, müsste eine Temperaturerhöhung in der gesamten Lower Troposphere nachweisbar sein, nicht nur bodennah.
Guten Abend Herr Haase,
eigentlich wollte ich mich ja ausklinken, aber Ihre Anfrage bedarf eines Hinweises. Betrachten Sie doch noch einmal in Abbildung 6 die beiden mittleren Graphiken. Sie zeigen die Maxima- und Minima-Temperaturen für den Monat August. In Schwerin ist der Trend zwischen Maxima und Minima für den heißen Monat August „Null“, Stagnation über 65 Jahre, auf dem Telegraphenberg in Potsdam treiben die Extrema sogar auseinander. Von der von Ihnen angesprochenen „CO2-Kuscheldecke über den Wolken“ und der dadurch abgemilderten Temperaturspanne ist nichts zu sehen.
Das Problem ist, dass wie in den USA und sicher weltweit ein Großteil der Messstationen bisher keinen CO2-bedingten Temperaturanstieg gemessen hat, sondern nur einen durch die Ausdehnung von Siedlungen verursachten Wärmeinseleffekt.
#42:
Herr Urbahn, hat die Ergebnisse des Wetterstationenprojekts über die Plausibilität der Daten der USA von A. Watts angesprochen. Ich kenne diese Daten. Ein Teil der Temperaturkurven steigt an, eine anderer Teil stagniert, bei einer Dritten Gruppe nimmt die Temperatur sogar ab. Nur, weil der größte Teil der Stationen aus praktischen Gründen im Siedlungsbereich zu finden ist, bringen diese letzlich auch den Mittelwert insgesamt zum Ansteigen. Das ist tatsächlich das AGW, das vom Menschen verursachte Global Warming. Es ist eben nicht das CO2, wie uns immer suggeriert wird, sondern die Bautätigkeit von immer mehr Menschen.
Nur ein Hinweis zur Plausibilität der deutschen Daten:
http://tinyurl.com/z8a2yge
In der von mir weiter oben angesprochenen Sendung auf Arte über die „Gültigkeit“ der Daten wurde erklärt, dass etwa die Hälfte des globalen Temperaturanstiegs von 0,85 °C seit 1850 durch den Menschen verursacht wurde. Wenn Sie von dieser Hälfte wiederum die Hälfte für den Wärmeinseleffekt verbuchen, bleibt für das CO2 wahrlich nichts mehr übrig, denn es gibt ja noch andere Spurengase in der Atmosphäre und den Wasserdampf. Wo sollte man denn den geringen Einfluss des CO2 noch erkennen können?
Lieber Herr Landvoigt, #28
„Sie wechseln hier den Bezugsrahmen. Denn zuerst kritierten sie den mangelnden Fit von statistischen Beobachtungen anhand von Signifikanztests. Nun aber verzichten sie völlig auf den statistischen Beleg und den per Signifikanztest bestätigten Nachweis. „
Herr Reiter hat gefragt: „Koennten Sie bitte das physikalische Modell bezueglich CO2 darlegen und erlaeutern, wie es verifiziert werden kann? Danke.“
Darauf habe ich geantwortet. Wollen Sie das etwas bestreiten?
Formal korrekt, jedoch am Tjhema vorbei. Die kritiscge Frage war im Kontext auch nicht, dass es einen, wie auch immer quantifizierbaren Einfluss gibt, sondern wie dieser zu quantifizieren, bzw. wie sich dieser im Systemkontext auswirkt:
———- #23: J.Reiter sagt:
@ NicoBaecker #21:
„Spekulationen gibt es zahlreiche. Der Punkt ist, keine davon lieferte bislang ein komplettes physikalisches Modell von Ursache (Sonne) zur Wirkung (Klima). Wenn es nicht mal ein physikalisches Modell hat, kann man auch nichts verifizieren und in eine Klimamodell implementieren.“
Koennten Sie bitte das physikalische Modell bezueglich CO2 darlegen und erlaeutern, wie es verifiziert werden kann? Danke.
Daran wird erkennbar, dass Herr Reiter nach einer Darstellung fragte, die als Ersatz zum solaren Modell gültig habe. Dieses haben sie nicht erbracht.
Ich erinnere mich an Pippi Langstrumpf, die auf die Frage, was die Uhr sei, nicht mit der Uhrzeit, sondern mit der Beschreibung des Instruments antwortete. Man kann das als rhetorisch spitzfindig beschreiben.
————– #38: NicoBaecker sagt:
„Sie bleiben hier bei eine isolierten Effektbeschreibung.“
Die physikalische Wirkung des CO2 in der Atmosphäre ist naturgesetzlich und quantitativ geklärt, und keine Effekteschreibung, die man (noch) nicht erklären könnte.
Das Schlüsselwort ist hier ‚isoliert‘, d.h. ohne Systemkontext. Genau das versuchen sie zu verschleiern. Wie aus dem Lehrbuch der Eristik.
————– #38: NicoBaecker sagt:
„Die Erklärungen sind hier vor allem die Rückkopplungen, die für die entsprechenden Faktoren stehen sollen. Und da ist die Fachwelt wohl äußerst divergent. Darauf beziehen sich aber die kritischen Rückfragen.“
Wie das feedback des Klimasystem auf ein initiales radiative forcing reagiert, ist ja eine Frage, die mit der Physik des CO2 nichts zu tun hat, sondern mit der Physik des Klimasystems. Falsch wäre dabei die Vorstellung, daß das Ergebnis dieses feedbacks eine Naturkonstante wäre.
Auch ist es nicht die Frage, ob es sich um eine Naturkonstante handelt, sondern um die praktischen Auswirkungen, die man als Grundlage für eine Berechnung, z.B. des 2-Grad-Ziels, in ansatz bringen muss. Ohne diese zuverlässig treffen zu können, machen fast alle AGW-Diskussionen gar keinen Sinn. Denn ist dies CO2-Sensitvität real tatsächlich mit < 0,5 Grad anzusetzen, ist der Klimaschutz ein absurdes Unterfangen, für das weder Geld nicht Zeit sinnvoll einzusetzen ist.
In Deutschland zeigen alle Monate, alle Jahreszeiten und die Jahre selbst einen Erwärmungstrend im 30-jährigen Mittel.
Das Jahr 2014 wurde in Deutschland als das wärmste Jahr seit Beginn der regelmäßigen Aufzeichnungen gemessen.
2015 wird das zweitwärmste Jahr in Deutschland mit einem neuen wärmsten November und wahrscheinlich einem neuen wärmsten Dezember.
Das Jahr 2014 wurde global als das wärmste Jahr seit Beginn der regelmäßigen Aufzeichnungen gemessen.
2015 wird wieder ein Jahr, welches global deutlich wärmer als alle bisher gemessen Jahre ist.
Mit dieser Litanei kann man endlos weiter machen.
Das alles ignoriert Steven Michelbach und fabuliert von „einem klaren Trend“ zu kälteren Temperaturen, weil er ein paar lokale Datenreihen so zurecht gestutzt hat, dass sie in dieser Zeit einen sinken Temperaturtrend zeigt.
Wer soll ihm bei seinen Gedankengängen außerhalb der Leugnerszene folgen? Die Antwort laut: Niemand!
Werner Bjoern
Lieber Hr. Baecker,
ich hatte gefragt: „„Wasserdampf ist anerkanntermassen ein starkes Treibhausgas und in Verbindung mit Wolkenbildung ein entscheidender Faktor beim „Treibhauseffekt“. Da hätte ich von Ihnen doch etwas mehr erwartet als ein Schulterzucken.“
Sie antworten: „Zweifellos, wenn Sie das auch interessiert, so müssen Sie auch recherchieren. Dieses Thema ist natürlich viel umfangreicher als Ihre Anfrage oben. Wenn Sie auch da was haben, so informieren Sie doch auch alle“
Nun Hr. Baecker, ich stelle hier zunächst fest, dass Sie kneifen. Nicht weil Sie es nicht könnten, sondern weil Sie genau wissen, dass ich genau die Schwachstelle Ihres gesamten Lügengebäudes angepickt habe.
Ich habe ehrlich gesagt auch nichts anderes von Ihnen erwartet. Aber ich bin ein geduldiger Mensch. Ich werde, wie von Ihnen gewünscht, eben selbst etwas recherchieren und auf die Sache zurückkommen. Und ich bin mir ziemlich sicher, dass Ihnen das, was ich dann publizieren werde, nicht gefallen wird.
„Man kann alle Menschen eine kurze Zeit lang belügen. Und man kann manche Menschen lange Zeit belügen. Aber man kann nicht alle Menschen auf lange Zeit belüge“.
Mfg
„Ob meine Isolierung wirklich langsam wirkt. Wenn Sie da eine Statistik kennen, wäre ich dankbar für einen Link.“
Endlich wird hier ein praktisches Problem diskutiert. Wie ich Sie verstanden habe, haben Sie die Isolierung Ihres Hauses verbessert und wollen überprüfen, welche Verbesserung dies konkret gebracht hat. Ich habe ein ähnliches Problem so gelöst. Wir haben eine Gasheizung. Ich habe in der Heizperiode den Zählerstand täglich abgelesen und mir die täglichen Mittelwerte von Außentemperatur, Sonnenscheindauer und Windstärke vom nächsten Wetteramt besorgt. Da ich dies vor und nach den Isolierungsmaßnahmen durchgeführt habe hatte ich eine rasche Erfolgskontrolle.
hatte ich überlesen, vielen Dank für diese Information. Habe diesen Fakt mal in mein Netzwerk kommuniziert und es kommt Erstauen auf. Ein Laie wie ich denkt da eher an eine längere Zeitspanne. Ein Bekannter kommentierte dies mit „… zu viel Spiritus beim Grill anzünden?“
#41 Hallo Herr Baecker,
deshalb frage ich ja! Ich versuche mal plastisch die Atmosphäre mit einer Hausisolierung zu vergleichen. Es strömt Wärme ins Haus, aber durch „Verstärkung“ der Isolierung mittels CO2 und anderen Gasen wird immer mehr Wärme im Haus zurückbehalten. So müssten doch langsam die Temperaturschwankungen im Haus reduziert werden. Ich weiß, dies ist auf das Minimum an beteiligten Faktoren reduziert. Aber unabhängig wie warm es am Tag war, müsste es doch so an einer Langzeitstatistik erkennbar sein, ob der Abstand zwischen Höchst- und Tiefsttemperaturen sich verändert. Ob meine Isolierung wirklich langsam wirkt. Wenn Sie da eine Statistik kennen, wäre ich dankbar für einen Link.
“ Das ist doch die Grundthese des postulierten zunehmenden Treibhauseffektes, immer weniger Energie kann den Planeten verlassen, also muss diese Spanne signifikant kleiner werden. “
Das ist ein ganz simples Badewannenprinzip. Der Wasserspiegel bei einem etwas zu knapp bemessenem Abfluß steigt so lange, bid der hydrostatische Druck den Wert erreicht hat, wo die Abflußmenge gleich der Zuflußmenge ist. Die Behauptung der „Treibhäusler“ besteht darin, daß postuliert wird, das CO2 würde den „Abluß“ verengen. und da die Strahlungsintensität mit einer Potenz von 4 zunimmt, müsse das zu höheren Temperaturen führen. Linear versteht sich.
Aber lassen wir das. Bisher ist dieser Beweis keinem gelungen. Wäre ja auch fatal, würden doch die Klimaforscher arbeitslos.
ich hatte dies in #8 schon so angedeutet. AFAIR ist der Max Wert aus den 5-min Mittelwert ermittelt. Ich bin mir da aber nicht ganz sicher.
MfG
A. Watts hat die Ergebnisse seines Wetterstationenprojekt auf der AGU-Tagung in San Fransico vorgestellt. von den 1218 Wetterstationen der kontinentalen USA gibt es eine Untergruppe von 410 Stationen „ungestörte“ Stationen (näheres dazu auf WUWT lesen). Verwendet man diese zur Berechnung des Anstiegs der „Durchschnittstemperatur“ pro Dekade seit 1979, dann ist dieser Anstieg um 1/3 geringer als der von der NOAA berechnete offizielle Anstieg, nämlich 0,2°C gegenüber 0,32°C.
Nebenbei auf der AGU-Tagung wurde auch gezeigt, daß in den USA in dem Zeitraum 2005 bis 2015 das NOX in der Luft zwischen 20% und 50% zurückgegangen ist, während es in Westeuropa gut 50% waren.
MfG
„Zu meiner Anfrage zu den nächtlichen Abkühlungstrends. Ich meinte nicht die nächtlichen Temperaturen, sondern die Spanne zwischen Höchst- und Tiefsttemperaturen.“
Ich glaube, dazu finden Sie auch in IPCC Bericht Zusammenfassungen.
„müsste doch ein zunehmender Treibhauseffekt diese Spanne verkleinern. Das ist doch die Grundthese des postulierten zunehmenden Treibhauseffektes, immer weniger Energie kann den Planeten verlassen, also muss diese Spanne signifikant kleiner werden. “
Ihre „Grundthese des postulierten zunehmenden Treibhauseffektes“ habe ich noch nie irgendwo behauptet gesehen. Woher haben Sie das?
Durch den zunehmenden Treibhauseffekt wird es bodennah wärmer. Das sieht man am besten in zeitlichen Mittelwerten.
„Mir war nicht bewusst, dass es sich bei diesen Werten nur um äußerst kurze Zeiträume handelt.“
Doch, der Extremwert muß nur einmal punktuell erreicht werden. Daher ist dies eigentlich auch nur eine Kuriosität, aus der man keine generellen Trends o.ä. ableiten kann.
Lieber Herr Mueller, #34
„Sie stellen hier im Forum doch immer Ihre Fachkompetenz gerade bezüglich der Klimawissenschaften heraus.“
Danke. Aber ich bin deswegen nicht Ihr Infokasten. Sie haben nach einer unter tausenden von Klimauntersuchung gefragt.
„Wieso verlangen Sie dann von Fragestellern, die nicht beruflich in diesem Bereich tätig sind, sie sollten selbst recherchieren und Ihnen ihre Ergebnisse mitteilen?“
Weil es Sie interessiert und mich nicht genug.
„Wasserdampf ist anerkanntermassen ein starkes Treibhausgas und in Verbindung mit Wolkenbildung ein entscheidender Faktor beim „Treibhauseffekt“. Da hätte ich von Ihnen doch etwas mehr erwartet als ein Schulterzucken.“
Zweifellos, wenn Sie das auch interessiert, so müssen Sie auch recherchieren. Dieses Thema ist natürlich viel umfangreicher als Ihre Anfrage oben. Wenn Sie auch da was haben, so informieren Sie doch auch alle.
Element;Messstation;Datum;Wert;Einheit;Geo-Breite (Grad);Geo-LÃ¤nge (Grad);HÃ¶he (m);SensorhÃ¶he (m);Erstellungsdatum;Copyright;
Lufttemperatur Tagesmaximum; Kitzingen; 2015-08-07; 40,3; Grad C;49,736; 10,178; 188; keine Daten vorhanden; 2015-12-18; Â© Deutscher Wetterdienst 2015;
„Bei wievielen Übereinstimmungen zwischen Maxima und Minima der des fünfjährigen gleitenden Mittels der Märztemperaturen mit dem Verlauf der Solarstrahlung wären es nicht mehr zufallsverteilte Quasi-Perioden, sondern eben doch von der Sonne gesteuerte Prozesse?“
Ja, gut Frage. Der Punkt ist, diese subjektive Einschätzung in ein objektives mathematisches Verfahren zu bringen: wie definieren Sie Maximum und welche zeitliche Verschiebung zählt noch als synchron, etc. . Zudem ist ja gar nicht klar, wie bislang unbekannte Beziehungen zwischen Sonnenaktivität und Temperatur sein sollten. Ihre Methode geht von einer linearen, synchronen aus, aber wer sagt, daß die response-Funktion die Delta-Funktion ist, denn das ist ja in der Natur selten der Fall?
Zudem ist es immer problematisch, wenn die Daten stark verrauscht sind. Die Korrelation zwischen Ihrer Jan-Feb-März-Temperatur auf der Zugspitze und dem CO2-Anstieg ist natürlich deswegen nicht zu sehen, weil die Wintertemperatur generell verrauschter ist als andere Saisonmittel. Zudem nimmt das Rauschen zu, je weniger Stationen Sie „bündeln“. Im Deutschlandmittel jedoch sieht man die Korrelation.
Lieber Herr Landvoigt, #28
„Sie wechseln hier den Bezugsrahmen. Denn zuerst kritierten sie den mangelnden Fit von statistischen Beobachtungen anhand von Signifikanztests. Nun aber verzichten sie völlig auf den statistischen Beleg und den per Signifikanztest bestätigten Nachweis. „
Herr Reiter hat gefragt: „Koennten Sie bitte das physikalische Modell bezueglich CO2 darlegen und erlaeutern, wie es verifiziert werden kann? Danke.“
Darauf habe ich geantwortet. Wollen Sie das etwas bestreiten?
„Sie bleiben hier bei eine isolierten Effektbeschreibung.“
Die physikalische Wirkung des CO2 in der Atmosphäre ist naturgesetzlich und quantitativ geklärt, und keine Effekteschreibung, die man (noch) nicht erklären könnte.
„Die Erklärungen sind hier vor allem die Rückkopplungen, die für die entsprechenden Faktoren stehen sollen. Und da ist die Fachwelt wohl äußerst divergent. Darauf beziehen sich aber die kritischen Rückfragen.“
Wie das feedback des Klimasystem auf ein initiales radiative forcing reagiert, ist ja eine Frage, die mit der Physik des CO2 nichts zu tun hat, sondern mit der Physik des Klimasystems. Falsch wäre dabei die Vorstellung, daß das Ergebnis dieses feedbacks eine Naturkonstante wäre.
Update DWD für Interessierte. Man hat meine Anfrage nach München verwiesen. Es wurde mir zwar jetzt mitgeteilt, dass die Wetterstation mit dem Rekordwert eine offizielle DWD-Station ist. Aber immer noch keinen Hinweis, wo ich die Daten sehen kann. Also erneut angefragt! Es geht mir nicht um die Tatsache, dass ich den Rekord als Mythos deuten möchte. Rekorde passieren, besonders wenn sich die Messreihen aus der sogenannten „Kleinen Eiszeit“ in ein Optimum hineinbewegen. Aber das der DWD sich so schwertut, eine direkte Quelle zu benennen, ist schon erstaunlich.
Des Weiteren habe ich bei meiner Suche beim DWD etwas entdeckt, was ich persönlich abscheulich finde. Da werden offensichtlich aufgrund des fehlenden klimatischen Vernichtungsschlages sämtliche Unfälle, die sich bei Wind, Nebel oder Regen ereignen, als Wetteropfer deklariert. Was natürlich den Lesern still und leise soufflieren soll, die Opfer sind schon da. Wie tief muss man eigentlich sinken, um Unfallopfer für seine Politik zu missbrauchen.
http://tinyurl.com/hcpw6y8
Zu meiner Anfrage zu den nächtlichen Abkühlungstrends. Ich meinte nicht die nächtlichen Temperaturen, sondern die Spanne zwischen Höchst- und Tiefsttemperaturen. Natürlich sind Wolkenbildung und Niederschlag wichtige Faktoren, aber langfristig müsste doch ein zunehmender Treibhauseffekt diese Spanne verkleinern. Das ist doch die Grundthese des postulierten zunehmenden Treibhauseffektes, immer weniger Energie kann den Planeten verlassen, also muss diese Spanne signifikant kleiner werden.
Ein extrem kluger Satz:
„Ja interessant, aber dazu weiß ich nichts. Recherchieren Sie selber und teilen Sie mir Ihre Ergebnisse mit.“
Sie sind eingeladen zum Fischmarktparken … ,-)
Da gibt es dann den Jahres-Trollo überreicht.
Sehr geehrter Herr Heinzow,
wir alle wissen, wie chaotisch das Wettergeschehen ist, entsprechend ist die Betrachtung, die Analyse eines „Klimas“ ebenso ein „Stochern“ in einem sehr diffusen Geschehen. Nicht, weil die einzelnen Bestandteile unbekannt sind, sondern weil die Wechselwirkungen in Raum und Zeit kaum überschaubar sind. Wir sind uns dieses Zustandes bewusst, für die CO2-Fraktion ist trotz des fehlenden, wissenschaftlich abgesicherten Beweises offensichtlich aber alles klar und in trockenen Tüchern, settled!
#21:
Sehr geehrter Herr Baecker,
vor wenigen Tagen sah ich eine Sendung auf Arte über die „Gültigkeit“ der Daten die dem AGW zugrundeliegen. (Auch hierüber könnte man einen eigenen Bericht schreiben…) Ich möchte nur ein Beispiel herausnehmen. Es wurde explizit darauf hingewiesen, wie komplex und eigentlich wenig sicher die Klimamodelle seien! Aber, so wurde erklärt, wenn man oft genug rechnet, dann würde sich die „Wahrheit“ schon herauskristalisieren. Dies wurde an folgendem Beispiel verdeutlicht: Ein Modell sei wie eine „Paint-Gun“. Irgendwo sei ein diffuses Ziel (Plexiglasscheibe mit Rautenöffnung). Wenn man nun oft, sehr oft genug darauf schießen würde, bekäme man Treffer auf der Scheibe oder eben nicht. Und dann würde man irgendwann die Raute sehen, das „Klimamuster des CO2“. „Klingt plausibel!“, dachte ich zunächst. Dann aber kam mir der Gedanke, ja, wenn das wahre Ziel aber eine andere Plexisglasscheibe, 10 m daneben wäre, mit einem „?“ statt einer Raute, dann wüsste man letztlich gar nichts zu dem gesuchten Problem.
Frage an Sie, könnte es sein, dass man die Klimamodelle so sehr auf das CO2 ausgerichtet und justiert hat, dass man andere Mechanismen, Ursachen damit gar nicht mehr erkennen kann? Man hätte dann die Spurensuche und so auch die Waffe auf einen speziellen Feind ausgerichtet. Dieser wird dann verhaftet, aber der Täter war eben doch ein anderer. Besser wäre es möglicherweise die effektivere Spurensuche der Kriminalistik heranzuziehen, als sich zu früh auf einen „Täter“ zu fixieren. Sonst geht vielleicht doch trotz teurer Hochleistungscomputer der „Paint-Ball“-Schuß auf das CO2 nach hinten los.
Da ich durchaus mit Simaulationsmodellen in einer anderen Sparte gearbeitet habe, könnte ich mir vorstellen, das man das Modul „CO2“ (Black-Box) durch ein Modul „Dynamische Sonne“ ersetzen könnte. Beide Male würde man nur ein Temperatursignal in die weiteren Modelberechnungen übergeben. Die restlichen Algorithmen könnten wohl beibehalten werden. Sie müssten dann nur der Klimasensitivität der Sonne eine realistische Größe geben…
Im Übrigen möchte ich mich hiermit aus der Diskussion zurückziehen. Es gibt noch viel zu tun. Ich danke für die Diskussion, vielleicht trifft man sich ja bei einem der nächsten Beiträge wieder.
Lieber Hr. Baecker,
Sie stellen hier im Forum doch immer Ihre Fachkompetenz gerade bezüglich der Klimawissenschaften heraus. Wieso verlangen Sie dann von Fragestellern, die nicht beruflich in diesem Bereich tätig sind, sie sollten selbst recherchieren und Ihnen ihre Ergebnisse mitteilen? Wasserdampf ist anerkanntermassen ein starkes Treibhausgas und in Verbindung mit Wolkenbildung ein entscheidender Faktor beim „Treibhauseffekt“. Da hätte ich von Ihnen doch etwas mehr erwartet als ein Schulterzucken.
Können Sie nicht oder wollen Sie nicht?
Mfg
„sehr interessant. Gibt es dazu auch Korrelationsbetrachtungen zu den Faktoren Luftfeuchtigkeit und Wolkentransparenz (getrennt nach Tag und Nacht)?“
Ja interessant, aber dazu weiß ich nichts. Recherchieren Sie selber und teilen Sie mir Ihre Ergebnisse mit.
„Bei wievielen Übereinstimmungen zwischen Maxima und Minima der des fünfjährigen gleitenden Mittels der Märztemperaturen mit dem Verlauf der Solarstrahlung wären es nicht mehr zufallsverteilte Quasi-Perioden, sondern eben doch von der Sonne gesteuerte Prozesse? “
Beachten Sie, daß Korrelationen notwendige Bedingungen für einen Kausalzusammenhang sind, aber niemals hinreichend.
„Es gibt generell – Ausnahmen gibt es natürlich- einen signifikanten Trend“
Ein Trend beweist nichts. Korrelationen sind notwendige Bedingungen für einen Kausalzusammenhang, aber niemals hinreichend.
Erstaunlich das das Pseudonym immer wieder mit demselben Unfug daherkommt.
“ wärmeren Tagesminimumtemperaturen“
Köstlich, einfach nur köstlich, denn sowas gehört in die Rubrik der Unsinnsbehauptungen wie: „er fuhr die schnellste Zeit“ oder der „weiße Rappe“.
Und so ein Pseudonym will Physiker sein.
Nun, das ist ja trivial. Das Modell für CO2 ist das gleiche wie für jedes andere Gas, welches Infrarotstrahlung absobiert. Man muß nur die Materialparameter entsprechend einsetzen.
Sehr geehrter Herr Baecker
Sie wechseln hier den Bezugsrahmen. Denn zuerst kritierten sie den mangelnden Fit von statistischen Beobachtungen anhand von Signifikanztests. Nun aber verzichten sie völlig auf den statistischen Beleg und den per Signifikanztest bestätigten Nachweis.
Sie bleiben hier bei eine isolierten Effektbeschreibung. Tatsächlich kann man das von Ihnen beschriebene z.B. auch mit Modtran nachvollziehen. Hier haben wir eine Brutto-Klimasensitivität von rund einem Grad.
Das aber würde keinen Alarmisten rscht geben, die von erheblich größeren effektiven Wirkungen ausgeht. Die Erklärungen sind hier vor allem die Rückkopplungen, die für die entsprechenden Faktoren stehen sollen. Und da ist die Fachwelt wohl äußerst divergent. Darauf beziehen sich aber die kritischen Rückfragen.
Gerade die Rückkopplungen werden bei einigen Autoren negativh eingeschätzt, z.B. Lindzen und Choi, Harde und einige andere. Hier geht man von rund 0,5 Grad aus. Das IPCC behauptet einen noch wahrscheinliches Minimum bei 1,5 Grad, auch wenn es sehr viele andere Studien gibt, die sich unterhalb dieser Marke bewegen. Ebenso geht das IPCC von einer noch wahrscheinlichen Obergrenze von 4,5 Grad aus.
Ein allgemein anerkannter empirischer Nachweis der Klimawirkung müsste hier einen Wahrscheinlichsten wert mit einer Fehlerrates liegen, der nicht mehr als 10 % schwanken dürfte, bzw. mittels statistischen Signifikanztest genau das gelegen sollte. Hier finde ich nichts in Ihrer Argumentation.
Immer wieder diese Null- und Unsinnssätze und Unsinnsbehauptungen, denn der Begriff „Klima“ ist untrennbar mit Variabilität verknüpft.
Stationarität gibt es in der Atmosphäre nicht.
Aber sowas interessiert ja die Pseudonyme nicht, die hier physikalischen Unsinn zu verbreiten suchen, um mit den sog. „erneuerbaren Energien“ und einer Hypothese, die seit fast einem halben Jahrhundert des Beweises bedarf, im Trüben das Geld der Bürger abzufischen.
Sehr geehrter Herr Baecker,
ja, genau diesen Punkt, ob das 5-jährige Mittel nicht gerade diese ca. 11-jährigen Zyklen erzeugt, habe ich hinterfragt. Allerdings habe ich kein Statistikstudium hinter mir, um die Frage mathematisch vertieft zu klären. Meine Überlegung dazu war: Wenn die Daten zufallsverteilt sind, dann könnte sich durchaus eine ca. 10-jährige Wellenstruktur ergeben. Allerdings würde ich bei einer zufallsverteilten Datenmenge dann erwarten, dass Maxima und Minimima eben auch zufallsverteilt sind. Ich möchte Sie hier bitten in meinem ersten Artikel zum solaren Paradoxon auf kaltesonne.de die Abbildungen 6, 7a und 7b genau nach dieser Fragestellung zu begutachten. Bei wievielen Übereinstimmungen zwischen Maxima und Minima der des fünfjährigen gleitenden Mittels der Märztemperaturen mit dem Verlauf der Solarstrahlung wären es nicht mehr zufallsverteilte Quasi-Perioden, sondern eben doch von der Sonne gesteuerte Prozesse?
Sie können mir da ja eventuell weiterhelfen. Ist das immer noch zufällig, wenn über einen Zeitraum von 250 Jahren 16 von 23 Zyklen zumindest nach meinem Eindruck gut übereinstimmen (da kann man sich natürlich im Detail darüber streiten)? Und das in einer Klimaphase, die von gravierenden Umwälzungen und großen Vulkanausbrüchen geprägt war. Ich sehe die Zyklen ja auch nicht als abschließenden Beweis, sondern als Fingerabdruck, als Indiz was tatsächlich hinter dem Klimawandel stecken könnte. Immerhin sind ja gleitende Mittel ein Instrument, um in dem Rauschen vermeintlich chaotischer Daten dennoch etwas zu erkennen. Fingerabdrücke vom CO2 auf Temperaturdaten sind mir in ähnlicher Form bisher noch nicht untergekommen. Das einzige, was man konkret weiß, ist, dass die Konzentration ansteigt.
„Koennten Sie bitte das physikalische Modell bezueglich CO2 darlegen und erlaeutern, wie es verifiziert werden kann? Danke.“
Nun, das ist ja trivial. Das Modell für CO2 ist das gleiche wie für jedes andere Gas, welches Infrarotstrahlung absobiert. Man muß nur die Materialparameter entsprechend einsetzen.
Damit folgt aus der Physik des Strahlungstransports, daß der Fluß der Infrarotstrahlung durch Treibhausgase so ist, daß die Intensität vom Boden ins Weltall abnimmt. Die Energiebilanz zwischen ein- und abgehender Strahlungsenergie ist bei einem stationären Klima gleich. Die einkommende Strahlung wird vor allem am Boden, die abgehende aber wegen der Absorption der Treibhausgase aber auf verschiedenen kälteren Atmsophärenschichten bewerkstelltigt, was gemaß den Strahlungsgesetzen und der konvektiven Schichtung in der Atmosphäre dazu führt, daß die Temperatur am Boden höher sein muß als ohne Treibhausgase.
Die hier relevanten physikalischen Gesetze sind alle verifiziert, die Absorptionkoeffizienten von CO2 sind gemessen.
Lieber Hr. Baecker,
sehr interessant. Gibt es dazu auch Korrelationsbetrachtungen zu den Faktoren Luftfeuchtigkeit und Wolkentransparenz (getrennt nach Tag und Nacht)?
Mfg
völlig richtig. Im letzten IPCC Report stehen zusammengefasst die Resultate. Es gibt generell – Ausnahmen gibt es natürlich- einen signifikanten Trend zu wärmeren Tagesminimumtemperaturen.
„Spekulationen gibt es zahlreiche. Der Punkt ist, keine davon lieferte bislang ein komplettes physikalisches Modell von Ursache (Sonne) zur Wirkung (Klima). Wenn es nicht mal ein physikalisches Modell hat, kann man auch nichts verifizieren und in eine Klimamodell implementieren.“
Koennten Sie bitte das physikalische Modell bezueglich CO2 darlegen und erlaeutern, wie es verifiziert werden kann? Danke.
MfG
J. Reiter
Kurzes Update für Interessierte, der DWD hat zwar geantwortet, aber nicht auf meine Anfrage. Ich habe eine erneute Anfrage gestartet. Aber eine andere Frage, vielleicht kann mir jemand beantworten, ob ich hier einen Gedankenfehler begehe. Wenn es einen zunehmenden Treibhauseffekt gibt, müsste sich doch der nächtliche Abkühlungstrend um diesem Faktor verringern. Was statistisch, wenn er wirklich wie behauptet bereits eine relevante Größenordnung besitzt, nachweisbar sein müsste. Anders formuliert, müsste man sich nicht die Temperaturen der Nächten anschauen, statt die am Tage, um den Faktor Sonne aus der Rechnung zu nehmen? Gedankenfehler oder lohnen sich hier statistische Auswertungen?
„Allerdings! Man findet sie nur über das fünfjährige gleitende Mittel.“
Eben. Haben Sie sich mal überlegt, ob man durch das Mitteln nicht Quasi-Perioden von knapp der doppelten Periode erzeugt?
In Reihen aus 5-Werte-übergreifenden Mitteln von Reihen aus normalverteilten Zufallszahlen findet man solche Quasi-Perioden auch (random walk). Werden also Zufallsgeneratoren auch durch die Sonne gesteuert?
„die Hinterlegung des Artikels wurde nur wegen einer Behauptung von Prof. Dr. Harald Lesch (Astrophysik) getätigt, und da es womöglich als Ergänzung sinnvoll ist. Die Aussage in der Sendung „Leschs Kosmos – Der Klima-Lügen-Check“ ist, dass die einwirkende Sonnenenergie nur um 0,2 Watt/Quadratmeter pro Sonnenzyklus (11 Jahre) auf der Erdoberfläche schwankt und folglich keine Auswirkungen mit sich bringt.“
Man muß bei der Wirkung der Sonne auf das Klima die Einflußfaktoren auseinanderhalten. Herr Michelbach kann dies leider auch nicht. Daher noch mal zur Klarstellung:
a) Klimafaktor TSI (=total solar irradiance = Gesamtstrahlungsleistung). Daß zur Funktionieren des Klimas Energie von der Sonne bezogen wird, ist wohl unbestritten. Da die TSI über den Zyklus 0,2% schwankt ist der damit verbundene Temperatureinfluß leicht abzuschätzen und beträgt ca. 0,1°C. Dies wird durch die Klimamodelle auch reproduziert. Eine Amplitude von 0,1°C mit einer Periode von ca. 11 Jahren ist allerdings in gemessenen Temperaturreihen empirisch schwer statistisch nachweisen. Aus statistischen Gründen gelingt dies am chancenreichsten im globalen Mittel.
b) Klimafaktoren X, Y, Z,…: zusätzlich zu dem physikalisch unmittelbar einleuchtendem Zusammenhang (Energie-Klima) wird seit Jahrzehnten über weitere physikalische Einflüsse, die von der Sonne aufs Klima wirken spekuliert. Grundsätzlich kann man diese Spekulationen in die verschiedenen „Ursachenvermittler“ unterteilen: Sonnenwindplasma, interplanetares Magnetfeld, solare UV-Strahlung.
Beispielsweise spekuliert man um Svensmark um einen solaren Einfluß aufgrund der solaren Modifikation (übers interplanetare Magnetfeld) in der kosmischen Strahlung und damit einer Modulation der Ionisationsraten und damit der Wolkenbildung. Der vom Autor zitierte Landscheidt spekulierte über eine Wirkung aufgrund der Modulation der Bewegung der Sonne um den Schwerpunkt des Sonnensystems.
Spekulationen gibt es zahlreiche. Der Punkt ist, keine davon lieferte bislang ein komplettes physikalisches Modell von Ursache (Sonne) zur Wirkung (Klima). Wenn es nicht mal ein physikalisches Modell hat, kann man auch nichts verifizieren und in eine Klimamodell implementieren.
Der viel ältere Ansatz geht hingeben umgekehrt: man spekuliert ganz ohne Physik und sucht empirisch nach Zusammenhängen zwischen Sonne und Klima in Daten. Dieser Ansatz war in den Anfängen der Klimaforschung vor über 100 Jahren sehr populär und Herr Michelbachs historische Auszüge zeigen dies ja auch sehr schön. Zur Frage nach den verursachenden physikalischen Abläufen kam es dabei jedoch nie. Denn dieser Ansatz scheiterte bereits an den verwendeten Methoden, wie die Klimatologen schwerlich spätestens in der zweiten Hälfte des 20. Jahrhunderts auch eingesehen hatten. Zu dem Zeitpunkt war die Statistik und mathematische Datenanalyse schon so weit ausgebildet, daß auch Nicht-Mathematiker sich nicht mehr davor verschließen konnten: viele von den Klimatologen vermeintlich gefundenen und leidenschaftlich diskutieren Zusammenhänge hielten einen Signifikanztest nicht stand: viele vermeintlich gefundene Zusammenhang konnten vom reinen Spiel des Zufalls nicht mit hoher Wahrscheinlichkeit unterschieden werden.
Ein bekannter Stolperstein ist das vermeidliche „Auffinden“ von Perioden durch die Bildung von übergreifenden Mitteln. Denn auch bei Zeitreihen aus Zufallszahlen kann man durch übergreifendes Mitteln Perioden erzeugen (z.B. random walk), wenn man nur lange genug würfelt. Die Tatsache, daß diese vermeintlichen Zusammenhänge zwischen Sonne und Klima häufig nur für bestimmte Zeiträume und in einzelnen Zeitreihen auftritt, legt es nahe, daß es sich hierbei nur um „hübsche Beispiele“ aus einem Pool zufallsgenerierter Muster handelt. Einer der ersten Klimatologen, der innerhalb der eigenen Reihen vor den Tücken der naiv (also ohne Signifikanzprüfung etc.) praktizierten Datenanalyse der klassischen Klimatologie (bis ca. 1930) warnten, war Franz Baur.
Sehr geehrter Herr Baecker,
grundsätzlich ist man nie davor gefeit, Daten nach seinem eigenen Gusto zu interpretieren. Jeder, der mit Daten umgeht, muss sich immer dieser Konsequenz bewusst sein. Wenn ich den falsch liege, möge man mich gerne korrigieren. Als jahrzehntelanger, streitbarer Naturschützer fühle ich mich sicher mehr als andere gerade beim Thema Klima dieser Verantwortung bewusst. Wie ich bei der hier vorgestellten „alten“, in meinen Augen heute noch gültigen Literatur, mit Überraschung erkannt habe, hat selbst Prof. Köppen schon vor 140 Jahren auf den Einfluss des 11-jährigen Sonnenzyklus hingewiesen; ich erwähnte das ja. Auf diese Arbeiten stieß ich erst vor wenigen Wochen. In meinem Beitrag auf kaltesonne.de zum „Solaren Paradoxon Deutschlands Teil I“ habe ich bereits vor einem halben Jahr von solaren Zyklen in deutschen Temperaturdatensatz hingewiesen. Wenn sie die Graphiken meines Berichtes nochmals genau anschauen, sehen Sie, dass diese Zyklen auch auf dem Hohenpeißenberg zu finden sind. Auch bei der DWD-Wetterstation bei uns im Ort findet man diese Zyklen und auch bei vielen anderen. Allerdings! Man findet sie nur über das fünfjährige gleitende Mittel. Aber studieren Sie doch bitte einmal den Teil I zum solaren Paradoxon…
#15
Sehr geehrter Herr Münch,
ich habe die Sendung von Professor Lesch auch gesehen. Wenn ich meine Enttäuschung über diese traurige Präsentation unwissenschaftlicher, vermeintlicher Fakten zum Ausdruck bringen wollte, müsste ich einen neuen Beitrag schreiben. Vielleicht so viel. Bis Mitte des vergangenen Jahrhunderts war die Wirkung des CO2 auf das Klima klein, eher „mickrig“. Nun hat sich die Konzentration erhöht, von 320 ppm (1960) auf 400 ppm. Ich rechne nun: „Klein“ x 1,25 = …………. ? Welche Wirkung hat nun CO2?
#16
Sehr geehrter Herr Tiburg,
entschuldigung, dann habe ich Sie falsch verstanden. So stimme ich mit Ihnen überein. Wer sich mit der Entwicklung alter Kulturen beschäftigt, weiß, dass klimatische Einflüsse häufig für das Entstehen und Vergehen von Kulturen verantwortlich waren. In warmen Klimaphasen ging es den meisten Menschen gut, in kalten herrschten oft Hunger, Not und Krieg!
#17
Sehr geehrter Herr Münch,
da stechen Sie bei mir ins Ameisennest. Ich habe mich lange Jahre mit der Thematik von Mittelwerten beschäftigt, allerdings in einer anderen Forschungsrichtung, und weiß, wie schwierig es ist, einen aussagekräftigen Mittelwert zu bekommen, oder ob es gar völlig unsinnig ist, diese Größe anzustreben. Auf der vergangenen 9. Klimakonferenz von EIKE zeigte uns Dr. Soon ein Beispiel für die Berechnung eines Mittelwertes. Er zeigte uns mehrere schöne Flugzeuge und fragte dann, wie wohl das „mittlere“ Flugzeug denn nun aussehen könnte. In der nächsten Folie zeigte er uns dann ein Objekt aus allen gemittelten Farbpixeln. Künstlerisch wertvoll und ein Raunen ging durchs Plenum…aber ein Flugzeug war es nun nicht mehr! So messen Wetterstationen zwar Temperatur, aber sie messen auch die Klimageschichte um die Stationen herum, ausgelöst durch alle möglichen Änderungen. Eine mittlere Temperatur für ein Gebiet oder global ist eben auch nur ein Mittel aus allen „individuellen“ Klimageschichten der einzelnen Stationen. Deswegen auch der Hinweis auf die Temperaturentwicklung auf dem Telegrafenberg, Potsdam. Und der Einfluss von 10 Raummetern Holz findet sich ebenso im globalen Mittel der „Erdtemperatur“ wieder… nachzulesen hier: http://www.eike-klima-energie.eu/climategate-anzeige/waermeinseleffekt-in-deutschen-wetterdaten/
Etwas verwundert bin ich schon, dass Geographen die Wintermonate als Jan-März definieren. Das wäre mir neu.
Wenn man „Klimawerte“ und deren Entwicklungen diskutieren möchte bietet es sich an zu schauen, was die Schweizer Meteorologie-Kollegen uns an Grafiken anbieten. Dort sieht man auch, dass seit 1998 das 20-jährige Mittel fällt. Nur wenn man sich den Verlauf der ganzen Kurve anschaut, dann wird es spannend, was die kommenden 10 Jahre angeht:
http://tinyurl.com/SwissClimatePage
Achja, Die Spline-Kurven werten die ansonsten guten Grafiken nicht wirklich auf: Prof, Dikau (Bonn) sagte einst: diese Splines sehen schön aus, haben aber keine sachliche Begründung in Klimareihen. Insbesondere in den Randbereichen geht deren Sinn in Unsinn über.
Der Vollständigkeit halber muss man noch klarstellen, dass es sich beim Sonnenzyklus von 11,07 Jahren lediglich um einen „Durchschnittswert“ handelt. Die Zyklen können zwischen 7 und 15 Jahren betragen – in einzelnen Fällen gegebenenfalls sogar stärker abweichen.
Die Durchschnittsdauer der Sonnenzyklen ist dementsprechend genauso wertvoll wie ein Mittelwert der globalen Temperatur.
Guten Tag Herr Steven Michelbach,
die Hinterlegung des Artikels wurde nur wegen einer Behauptung von Prof. Dr. Harald Lesch (Astrophysik) getätigt, und da es womöglich als Ergänzung sinnvoll ist. Die Aussage in der Sendung „Leschs Kosmos – Der Klima-Lügen-Check“ ist, dass die einwirkende Sonnenenergie nur um 0,2 Watt/Quadratmeter pro Sonnenzyklus (11 Jahre) auf der Erdoberfläche schwankt und folglich keine Auswirkungen mit sich bringt.
haben Sie sich schon einmal überlegt, ob Ihnen nicht ähnliche Fehlinterpretationen durch allzu willkürliches ‚data treatment‘ passiert ist wie es auch Zeitgenossen der von Ihnen zitierten Klimatologen damals schon passiert ist? Die gleitende Lustrenmittel zeigen zwar auch eine quasi-11-Jahresperiode, aber eigentlich passen nur die Hälfte der Maxima in Abb. 3 mit den Maxima in der Sonnenaktivitätszahl zeitlich überein. Wenn man mal hypothetisch diese synchronen Ereignisse als die ‚reinen‘ sonnengetriebenen Temperatursignale hernimmt, so müsste also zwischen Sonnenaktivitätsminimum und -maximum die dadurch induzierte Temperaturamplitude auf der Zugspitze im Jan-Mrz-Mittel knapp 1 °C ausmachen.
Warum ist eigentlich die Zugspitze im Winter so „sonnensensitiv“? Und warum nur die Zugspitze?
Kann es sein, daß Sie einfach nur ein zufälliges Muster gefunden haben, so wie auf dem Mars von tausenden von Hügeln einer ein Gesicht hat?
Admin
Hallo Markus,
richtig! Ausgehend vom einzelnen Tag, über den 11-jährigen und alle anderen solaren Zyklen bestimmt vorrangig die Sonne das Klimageschehen auf der Erde. Das sind die Fakten.
zu #2:
Sehr geehrter Herr Seilkopf,
Sie haben völlig recht. Es hilft aber nichts! Es gibt so viele, die von den computermodell-inszenierten Panikmeldungen in die Irre geführt werden. Deshalb ist es wichtig gerade am deutschen Temperaturdatensatz aufzuzeigen, wie langfristiger Klimawandel zu interpretieren ist, und was große Klimawissenschaftler über die tatsächlichen Ursachen dafür seit 140 Jahren herausgefunden haben. Dazu eine Stellungnahme der Deutschen Meteorologischen Gesellschaft (1999): „Es ist unstrittig, dass der anthropogene Treibhauseffekt noch nicht unzweifelhaft nachgewiesen werden konnte.“ Übersetzt in besser verständliches Deutsch:„Der wissenschaftlich korrekte Beweis, dass es einen menschengemachten Klimawandel gibt, ist bis heute nicht erbracht!“ Und Deutschland treibt eine Energiewende zum vermeintlichen Schutz des Klimas voran! Geht’s noch…?
zu #3:
Sehr geehrter Herr Endres,
es geht bei der Analyse von Klima nicht um die Ausprägung eines einzelnen Jahres, sondern um die Bewertung mehrerer Periode von jeweils 30 Jahren. Auf unserem Hausberg in Baden-Württemberg, dem Feldberg im Schwarzwald, sind die Wintertemperaturen seit 1987 um ca 2 °C abgesunken, trotz der milden Winter 2014 und 2015. Das wird auf dem Harz nicht anders sein, wenn man für die Winter der letzten 27 Jahre die Monate Januar bis März mittelt. Habe es gerade für Sie ausgewertet: Für Hatzgerode beträgt der Temperaturrückgang seit 1988 ca. 1°C, wobei es für 2005 und 2006 Datenlücken gibt. Für den Brocken gibt es leider zu viele Datenlücken…
zu #5:
Sehr geehrter Herr Tiburg,
wenn Sie tatsächlich meinen, dass wir durch die Energiewende Energie und CO2 eingespart hätten, dann sollten Sie hier auf EIKE noch einige Artikel aus der rechten, der energiewirtschaftlichen Spalte nachlesen. Dem ist nämlich nicht so, auch wenn uns das oft und vollmundig erzählt wird.
Zu #6:
Sehr geehrter Herr Haase,
ihre Station ist eine private Station in einem privaten Messnetz, die Station des DWD ist im Gewann Lerchenbühl, Hinweis hier:
http://www.infranken.de/regional/kitzingen/Heiss-heisser-Kitzingen;art218,730645
Re: 17 h: Kitzingen mit 39,2 Grad dabei.
Datum: 05. Juli 2015 17:29
http://tinyurl.com/pqyy3uf
Kitzingen nun 39,4. Es wird nicht mehr zum Rekord reichen.
Datum: 07. August 2015 16:06
http://tinyurl.com/pv8esfc
Wird der Wert irgendwie berechnet? Ich habe jetzt den DWD angeschrieben, mal schauen was die zum Thema antworten.
Typische Trollbehauptung:
„Ich könnte noch viele positive Ergebnisse aufzählen.“
Ich bin genauso wie Sie extrem vergnügungssüchtig: „#5
Dann zähl den Schwachsinn bitte mal auf!
Wir wollen hier schliesslich auch mal wieder prusten vor lachen…“
Wird der Troll aber nicht. Und parken auf dem Fischmarkt auch nicht.
Zu #5 Herrn Hase: Sie haben bei den 14:00h und 15:00h Werten wahrscheinlich die Stundenmittelwerte aus der WESTE-XL Reihe Die Tagesmaxima beruhen auf kürzeren Messintervallen.
Dann zähl den Schwachsinn bitte mal auf!
Wir wollen hier schliesslich auch mal wieder prusten vor lachen…
Aber bei dem medialen Trauerspiel (Propaganda zur KlimaLÜGENKonferenz) scheint eh Hopfen & Malz verloren. Die Idioten wollen es nicht anders als nach Strich und Faden betrogen zu werden, sie bejubeln es noch und/oder weinen vor Rührung. Wenn das Außerirdische mitbekommen, die sparen sich eine Kontaktaufnahme zu solchen Trotteln oder sie nutzen das für ihre Zwecke.
Diese Religion wird sehr bald ihre ….isten haben, manche sind schon auf gutem Weg!
Er ist notwendig und gleichzeitig aber auch unnötig. Warum?
Die Notwendigkeit erklärt sich von selber. Aufklärung über den CO2 Wahnsinn.
Doch unnötig ist der Artikel, weil es bekanntes Wissen ist, welches zig mal verifiziert wurde. Die Arbeit, um diesen Artikel zu schreiben, ist vertan und doch nicht vertan.
Der Wahnsinn zeigt, dass es nie und nimmer um einen Klimawandel und dei Anpassungen an selbigen gehen kann, sondern nur darum Gelder zu vernichten und Menschen zu schädigen.
Somit liegt eine klare Verletzung des Amtseides vor. Punkt. Doch oh weh, der hat ja keine Bedeutung. Somit kann und darf ein Politiker lügen bis die Balken krachen.
http://goo.gl/9Q3qkx
… mit Bildern: Eisbohrkern, Sonnenflecken
http://goo.gl/ShSNU8