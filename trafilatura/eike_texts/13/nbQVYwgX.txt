Als ein Experten-Begutachter des IPCC seit 18 Jahren, das heißt also von dessen ersten Anfängen an, habe ich tausende Kommentare zu allen Berichten geschrieben. Meine Kommentare zum 4. Zustandsbericht, alle 1898, können beim IPCC (2007) gefunden werden, und meine Meinungen über das IPCC stehen in Gray (2008b).
Seine jüngste Veröffentlichung trägt den Titel [übersetzt] „Der Betrug um die globale Erwärmung und der Superbetrug um den Klimawandel“ [The Global Warming Scam and the Climate Change Super Scam]. Sie ist gewissermaßen eine Fortsetzung seiner sehr effektiven ersten Kritik mit dem Titel „The Greenhouse Delusion: A Critique of “Climate Change 2001“. Wir wissen jetzt, dass der Bericht aus dem Jahr 2001 den Hockeyschläger enthielt sowie Phil Jones‘ globale Temperaturaufzeichnung, zwei Beweisstücke, die grundlegend für die Proklamation anthropogener Gründe für die globale Erwärmung sind. In der Zusammenfassung jenes Buches schreibt er:
● Es gibt gewaltige Unsicherheiten in den Modellergebnissen, die erkannt und unermesslich sind. Sie sind so gewaltig, dass Adjustierungen der Modellparameter Modellergebnisse erzeugen kann, die zu fast jedem Klima passen, einschließlich zu einem solchen ohne Erwärmung oder gar mit Abkühlung.
● Kein Modell hat jemals irgendeine zukünftige Klimasequenz erfolgreich vorhergesagt. Trotzdem werden Zukunfts-„Projektionen“ bis zu mehreren hundert Jahren im Voraus vom IPCC als plausible Zukunftstrends präsentiert, basierend auf umfangreich verzerrten „Storylines“ in Kombination mit nicht getesteten Modellen.
● Das IPCC hat eine Fülle wissenschaftlicher Informationen zum Thema Klima abgeliefert, aber nicht einen einzigen Beweis erbracht, dass ein zunehmender Kohlendioxidgehalt irgendwelche schädlichen Auswirkungen hat.
Auf Seite 58 des Buches beschreibt er eine der ernstesten Grenzen der Computermodelle:
Kein Computermodell ist jemals validiert worden. Ein früher Entwurf von Climate Change 95 enthielt ein Kapitel mit dem Titel „Climate Models – Validierung“ als eine Erwiderung auf meinen Kommentar, dass kein Modell jemals validiert worden ist. Sie änderten den Titel zu „Climate Model – Evaluation“ und ersetzten auch im Text das Wort „Validierung“ mit „Evaluation“. Damit beschrieben sie nichts weniger als was man tun müsste, um ein Modell zu validieren.
Ohne einen erfolgreichen Validierungsprozess sollte man kein Modell als fähig erachten, eine plausible Prophezeiung zukünftigen Verhaltens des Klimas zu liefern.
Was ist Validierung?
Die traditionelle Definition von Validierung involviert, dass man das Modell rückwärts laufen lässt, um bekannte Klimabedingungen abzubilden. Der allgemein angewendete Terminus lautete „Nachhersage“ [hindsight forecasting]. Da gibt es eine wesentliche Begrenzung wegen der Zeit, die ein Computer braucht, um die historischen Bedingungen zu rekonstruieren. Steve McIntyre hat bei ClimateAudit das Problem illustriert:
Caspar Amman sagte, dass GCMs etwa einen Tag Maschinenzeit brauchten, um 25 Jahre abzudecken. Auf dieser Grundlage ist es offensichtlich unmöglich, den Übergang vom Pliozän zum Pleistozän mit einem GCM zu modellieren (also die letzten 2 Millionen Jahre), da dieses dazu 219 Jahre Rechenzeit benötigen würde.
Auch sind Modelle nicht in der Lage, gegenwärtige oder historische Bedingungen zu simulieren, weil wir diesbezüglich keine genauen Kenntnisse oder Messungen haben. Das IPCC räumt dies in Kapitel 9 des Berichtes aus dem Jahr 2013 ein:
Obwohl von grundlegender Bedeutung hat die Evaluierung von Klimamodellen auf der Grundlage von Klimabeobachtungen der Vergangenheit einige wichtige Grenzen. Sie sind begrenzt hinsichtlich der Variablen und Phänomene, für die es Beobachtungen gibt.
Sorgfältige Validierung ist „unabdingbar“, aber ernsthaft begrenzt, weil wir nicht wissen, was historisch los war. Die Reduktion der Anzahl der Variablen umgeht die begrenzte Computer-Kapazität sowie fehlende Daten oder Wissen um die Mechanismen.
Allerdings erklären O’Keefe und Kueter:
Als Folge hiervon gibt es nur sehr wenige GCM-Projektionen in vollem Umfang. Die Modellierer haben eine Vielfalt von Vereinfachungs-Verfahren angewendet, um mehr Ergebnisse erzeugen zu können. Da die Genauigkeit vollumfänglicher GCM-Läufe unbekannt ist, ist es auch nicht möglich abzuschätzen, welche Auswirkung die Anwendung dieser Vereinfachungen auf die Qualität der Modellergebnisse hat.
Ein Problem ist, dass eine Variable, die derzeit als keine Konsequenzen zeitigend angesehen wird, unter anderen Bedingungen von grundlegender Bedeutung ist. Dies Problem tauchte in der Erdwissenschaft auf, wo bestimmte Mineralien, „trace minerals” genannt, als von geringer Bedeutung eingestuft worden waren und in Berechnungen der Bodenfruchtbarkeit keinen Eingang gefunden haben. In den siebziger Jahren lautete die Objektive, mittels massiven Einsatzes von Dünger die Ernteerträge zu verbessern. Anfang der achtziger Jahre gingen die Ernteerträge trotz hinzugefügter Dünger zurück. Offensichtlich konnten die Pflanzen Dünger-Mineralien ohne einige Spuren-Mineralien nicht aufnehmen. Im Beispiel Weizen war es Zink, welches der Katalysator war für die Absorption bedeutender chemischer Dünger.
In der Klimadebatte ist es inzwischen ein Muss, dass eine Person, die durch AGW-Befürworter angegriffen wird, mit der Wahrheit handelt. Es beweist, dass sie die Wahrheit kennen und aus eigenem Willen von diesen Wahrheiten Abstand nehmen aus politisch-ideologischen Gründen. Skepticalscience ist ein perfektes Beispiel, und dessen Versuch, die Validierung von Modellen zu rechtfertigen beginnt mit einem Angriff auf die Beobachtung von Freeman Dyson, dass:
„Modelle voller verformbarer Faktoren stecken, die an das bestehende Klima angepasst werden, so dass die Modelle mehr oder weniger mit den gemessenen Daten übereinstimmen. Aber es gibt keinen Grund zu glauben, dass die gleichen verformbaren Faktoren das richtige Verhalten in einer Welt mit unterschiedlicher chemischer Zusammensetzung spiegeln, zum Beispiel in einer Welt mit einem viel höheren CO2-Anteil in der Atmosphäre“.
Sie verwenden „Zuverlässigkeit“ anstelle von Validierung und verwenden den Terminus „Nachhersage“ [hindcasting], aber in einem anderen Zusammenhang:
„Falls ein Modell korrekt Trends prophezeien kann ab einem Anfangszeitpunkt irgendwann in der Vergangenheit, können wir erwarten, dass es mit hinreichender Sicherheit prophezeien kann, was in der Zukunft passieren könnte“.
Sie behaupten, dass mittels ihres Systems
die Modelle erfolgreich die globalen Temperaturen seit dem Jahr 1900 reproduzieren können, auf dem Festland, in der Luft und im Ozean.
Und:
Klimamodelle müssen getestet werden um herauszufinden, ob sie funktionieren. Wir können nicht 30 Jahre lang warten, um, zu sehen, ob ein Modell gut ist oder nicht; Modelle werden gegen die Vergangenheit getestet, gegen das, was wir wissen.
Es ist jetzt 25 Jahre her, dass das IPCC seine ersten Modell-Prophezeiungen (Projektionen) veröffentlicht hat, und schon damals wurde die Lüge in Abbildung 1 bloßgestellt:
Abbildung 1. Quelle: Präsentation von John Christy von der University of Alabama [UAH] vor dem Committee on Natural resources des Weißen Hauses am 15. Mai 2015.
Herumfummeln, um Zuverlässigkeit als Validierung zu maskieren
Versuche der Validierung während der 120 Jahre mit Instrumenten-Aufzeichnungen erwiesen sich ebenfalls als problematisch, aus den gleichen Gründen wie bei den historischen Aufzeichnungen. Eine wesentliche Herausforderung war die Abkühlungsphase von 1940 bis 1980, weil diese zusammenfällt mit der stärksten Zunahme der menschlichen CO2-Erzeugung. Dies widersprach der grundlegendsten Hypothese des IPCC, der zufolge eine CO2-Zunahme zu einer Temperaturzunahme führt. Freeman Dyson hat diese Praxis beschrieben als „verbiegen“ [tweaking] und darüber in vielen Beiträgen bei WUWT berichtet. Es ist die Praxis der Fabrikation von Beweisen, um die Lügen zu stützen, die die Computermodelle sind.
Sie trachteten nach einer Erklärung in Einklang mit ihrer Philosophie, dass jedwede Anomalie, oder jetzt Störung, dem Menschen geschuldet ist. Sie verbogen das Modell mit Sulfaten aus anthropogenen Quellen, eine Partikelsorte, die Sonnenlicht blockiert und Abkühlung erzeugt. Sie wendeten das an, bis der Modelloutput zur Temperaturkurve passte. Das Problem war, dass nach 1980 die Erwärmung wieder einsetzte, das Sulfatniveau aber gleich blieb. Alles, was sie tun, leidet unter der von T. H. Huxley ausgesprochenen Wahrheit: „Die Große Tragödie der Wissenschaft ist das Scheitern einer wunderschönen Hypothese an einem hässlichen Faktum“.
Gray erklärte:
Anstelle von Validierung und der traditionellen Anwendung mathematischer Statistik sind die Modelle „evaluiert“ ausschließlich durch die Ansicht derjenigen, die sie erdacht haben. Derartige Ansichten sind parteiisch und verzerrt. Außerdem sind sie nichts weiter als Raten.
Er verwies auch darauf, dass das IPCC im Bericht 2001 im Abschnitt mit der Überschrift Model Evaluation geschrieben hat:
Wir erkennen in vollem Umfang, dass viele Evaluierungs-Statements, die wir abgeben, einen bestimmten Grad subjektiver wissenschaftlicher Vorstellung enthalten sowie viel „allgemeines“ oder „persönliches“ Wissen. Beispiel: Die reine Auswahl von Modellvariablen und Modellierungsprozessen, die untersucht werden, basieren oftmals auf der subjektiven Beurteilung und Erfahrung des modellierenden Teams.
Im Physical Science Basis-Bericht 2013 wird eingeräumt, dass es keine Validierung gibt.
Kapitel 9 des IPCC-Berichtes aus dem Jahr 2013 trägt den Titel Evaluation of Climate Models.Sie machen ein paar Verbesserungen bei der Evaluierung geltend, aber es handelt sich immer noch nicht um Validierung.
Obwohl von grundlegender Bedeutung, hat die auf Klimabeobachtungen der Vergangenheit basierende Evaluation von Klimamodellen einige wichtige Begrenzungen. Sie ist begrenzt auf jene Variablen und Phänomene, für die es Beobachtungen gibt.
In vielen Fällen bilden das Fehlen oder die schlechte Qualität langfristiger Beobachtungen, seien es spezifische Variablen, ein wichtiger Prozess oder ein bestimmtes Gebiet (z. B. Polargebiete, obere Troposphäre/untere Stratosphäre {UTLS} und die Tiefsee) unverändert ein Hindernis. Außerdem sind die beobachteten Aufzeichnungen, mit denen die Modelle verglichen werden, ,mangelhaft‘, und zwar wegen Ungewissheiten bei den Beobachtungen und der Gegenwart interner Variabilität. Diese Begrenzungen können reduziert, aber nicht vollständig eliminiert werden, durch die Verwendung multipler unabhängiger Beobachtungen der gleichen Variable ebenso wie die Verwendung von Modell-Ensembles.
Das in dem Kapitel beschriebene Verfahren zur Modellevaluierung reflektiert die Notwendigkeit der Klimamodelle, dass diese das beobachtete Verhalten des Klimas in der Vergangenheit als eine notwendige Bedingung für Projektionen in die Zukunft in Betracht ziehen müssen. Dies ist jedoch keine Antwort auf die viel schwierigere Frage zu bestimmen, wie sehr ein Modell zu Beobachtungen passen muss, bevor Projektionen durchgeführt werden, die man zuverlässig erachten kann. Seit dem AR 4 gibt es ein paar Beispiele auftauchender Hindernisse, wobei Beobachtungen verwendet werden, um Multimodell-Ensemble-Projektionen zu behindern. Diese Beispiele, die später noch in Abschnitt 9.8.3 zur Sprache kommen, bleiben Bestandteil eines Gebietes aktiver und bislang nicht aufschlussreicher Forschungen.
Ihre Schlussfolgerung
Die heutigen Klimamodelle sind im Prinzip besser als ihre Vorgänger. Allerdings ist jede einzelne hinzugefügte weitere Komplexität auch eine Quelle neuer Fehler (z. B. mittels unsicherer Parameter), obwohl damit einige Aspekte des simulierten Klimas verbessert werden sollen. Außerdem können dadurch neue Wechselwirkungen zwischen Modellkomponenten auftreten, die vielleicht, wenn auch nur zeitweise, die Modellsimulation anderer Aspekte des Klimasystems entwerten können. Und es kommt noch hinzu, dass trotz der erzielten Fortschritte wissenschaftliche Unsicherheiten hinsichtlich der Details vieler Prozesse fortbestehen.
Diese Zitate stammen aus dem Physical Basis Science-Bericht, was bedeutet, dass die Medien und die Politiker sie nicht lesen. Alles, was sie zu Gesicht bekommen, ist ein kleiner Kasten (2.1) auf Seite 56 der Summary for Policymakers SPM. Man hat die Worte sorgfältig gewählt, um zu implizieren, dass alles besser ist als noch im Ar 4. Eröffnet wird das Ganze mit den Worten:
Verbesserungen der Klimamodelle seit dem AR 4 des IPCC sind offensichtlich bei den Simulationen der Temperatur im Maßstab von Kontinenten, hinsichtlich der großräumigen Niederschlagsmenge, des Monsuns, des Arktischen Meereises, dem ozeanischen Wärmegehalt, einigen Extremereignissen, dem Kohlenstoff-Zyklus, der atmosphärischen Zusammensetzung und Aerosole, den Auswirkungen stratosphärischen Ozons und der El Niño-Southern Oscillation.
Das Einzige, was sie einräumen, ist, dass
die Simulation großräumiger Verteilungen des Niederschlags sich seit dem AR 4 etwas verbessert hat, obwohl die Modelle auch weiterhin hinsichtlich des Niederschlags schlechter abschneiden als hinsichtlich der Temperatur. Das Vertrauen in die Repräsentation der Prozesse bzgl. Wolken und Aerosole bleibt gering.
Ironischerweise sind diese Bemerkungen Gegenstand der gleichen Herausforderung der Validierung, weil der Leser den Anfangszeitpunkt nicht kennt. Falls das Modell nicht funktioniert, ist der Terminus „etwas verbessert“ bedeutungslos.
All dies bestätigt die Gültigkeit der Kommentare von Dr. Gray, dass die Validierung für ein Klimamodell unabdingbar ist und dass
„kein Computer-Modell jemals validiert worden ist“.
Und: Ohne einen erfolgreichen Validierungsprozess sollte kein Modell als fähig erachtet werden, eine plausible Vorhersage des zukünftigen Verhaltens des Klimas zu liefern.
Link: http://wattsupwiththat.com/2015/08/08/validation-of-a-climate-model-is-mandatory-the-invaluable-work-of-dr-vincent-gray/
Übersetzt von Chris Frey EIKE
ich bin zu naturwissenschftliche Denkweise erzogen worden und besitze ausreichend Allgemeinwissen, um meistens Humbug sofort zu erkennen. Bin aber kein Naturwissenschaftler.
Was aber in unseren Medien als Naturwissenschaft und Naturwissenschaftler durchgeht, spottet jeder Beschreibung, manchmal denke ich, mann sollte die Verbrennung auf den Scheiterhaufen für Verbreitung von Unfug wieder einführen, ehrlich!
Schuld sind in 1. Linie: Ausbildung, in der naturwissenschaft immer mehr zurückgedrängt wird und erzogen wird, Wissen ist nichts, Selbstdarstellung alles. Und 2.: Medien, bei dennen die Wahrheit zugunsten der Quote zu 100% geopfert wird.
Das es einige wenige Naturwissenschaftler in D gibt, wird nicht bezweifelt. Beleidigen wollte ich diese in keinster Weise. Für mich ist aber nicht der Abschluß wichtig, sondern das Weltbild. Und das ist bei den meisten deutschen ganz und gar nicht naturwissenschaftlich.
Aber die Messung der IR-Fernthermometer-Temperaturdifferenzen Boden – Luft beweisen, dass die Atmosphäre ein Kühlhaus, kein Treibhaus ist. Solche Artikel über Klimamodelle und deren Validierung bzw. Nicht-Validierung sind sinnlos zur Beantwortung der CO2 Frage.
Wie häufig können ‚Experten‘ von Greenpeace in den ÖRF-Anstalten ein völlig falsches Bild technischer Sachverhalte darstellen. In Deutschland gibt es hunderte, vielleicht tausende Physiker, Techniker, Reaktorsicherheitsexperten usw., aber wenn in China ein Sack Reis umfällt (oder gab es in Fukushima eine Wasserstoffgasexplosion in einem Atomkraftwerk?) dann werden ‚Experten‘ von Greenpeace gefragt, die keinerlei formalen Abschluss nachweisen können. Letzten Endes beweist diese Selektion der Inkompetenz, dass eine bewusste Irreführung der Öffentlichkeit angestrebt wird, und dass insbesondere die öffentlich-rechtlichen Medien ihrem gesetzlichen Auftrag einer ausgewogenen Berichterstattung nicht nachkommen, also kriminell handeln.
Ein Nebeneffekt dieser ökosozialistischen Indoktrination ist es nun aber, dass seriöse Wissenschaft grundsätzlich verunglimpft werden muss. Ich habe selbst in den siebziger Jahren an den Universitäten die Diskussion über ‚wertfreie‘ Wissenschaft miterlebt. Was erwartet man denn von einer Ideologie, die Objektivismus als Schimpfwort ansieht? Diese Leute, die damals das Wort geführt haben, sitzen heute an den Schalthebeln der Medien-Macht, weil für solche Posten nicht eine formale wissenschaftliche Ausbildung, sondern Dummschwätzen und Seilschaften maßgeblich sind. Und wie damals seriöse Wissenschaft denunziert wurde, so wird sie heute möglichst ignoriert, und wo sie frech vorprescht und doch mal öffentlich auftritt diskreditiert, was meistens auf persönliche Beleidigungen hinausläuft. Somit ergibt sich das Bild, dass Naturwissenschaftler weltfremd, inkompetent oder korrupt sind.
Doch mit diesen Fähigkeiten würden keine Flugzeuge fliegen, kein Internet funktionieren und keine Kraftwerke – nicht mal Wind- oder PV-Anlagen – Strom liefern, von einer sicheren, ständig gesteigerten Nahrungsmittelproduktion trotz widriger Klimaverhältnisse ganz zu schweigen. Man sollte die pauschale Verurteilung besser vermeiden, da man sich letzten Endes selbst auf die Stufe einseitiger Propagandisten stellt.
Danke Herr Kowatsch,
Sie bestätigen meine Meinung, dass viele Bauern, die den Himmel und die Pflanzen mit offenen Augen beobachten, besser die Natur verstehen, als viele Akademiker, die einen Abschluß in einem naturwissenschaftlichen Fach haben.
Ich weigere mich, diese Titelträger als Naturwissenschaftler zu verstehen, allen voran unsere Kanzlerin. Obwohl ich keinen Zweifel habe, dass sie ihre Arbeit selbst geschrieben hat.
Was braucht man denn, um ein GCM zu starten und zum Laufen zu bringen? Bestimmt keine genauen Anfangswerte wie bei den Vorhersagemodellen.
Man braucht letztendlich nur das Forcing durch die Ozeane, was übrigens beim Hindcast des Dust Bowl funktionierte, um dieses kurzfristige lokale „Klimaphänomen“ simulieren zu können. Das Forcing durch das Land ist ja bekannt und gut vermessen seit ca 100 Jahren.
Probleme bereiten die Ozeanmodelle, denn das Forcing der Ozeane durch Sonne, Wind und Niederschläge (also die Atmosphäre) ist weitgehend unbekannt. Das liegt schlichtweg an zu wenig Meßdaten. Ozeanmessungen bis zum Meeresgrund in der nötigen Auflösung sind nicht bezahlbar. Selbst in der Atmosphäre schafft man das nicht mit den billigen Radiosonden.
Das Ergebnis ist, wie überraschend, gleich b), da es in der ausgesprochenen oder unausgesprochenen Zielstellung der Produktentwicklung liegt.
Frage: Wieso nimmt man die Computerspielerei so ernst? Fällt es Akademikern so schwer, zwischen reellen und virtuellen Welt zu differenzieren? Glaubt man, man packt das ganze menschliche Unwissen in einer Maschine, die nur exakt definierte menschliche Befehle ausführen kann und es kommen völlig neue, ungeahnte Erkenntnisse raus?