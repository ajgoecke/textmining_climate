Über den Vortragenden
Richard S. Lindzen war Alfred P. Sloan-Professor für Meteorologie beim Massachusetts Institute of Technology bis zu seinem Ruhestand im Jahre 2013. Er ist Autor von über 200 Fachartikeln zu den Themen Meteorologie und Klimatologie und ist ein Mitglied der US-National Academy of Sciences und in der akademischen Beratungsversammlung der GWPF (s. Liste der Mitglieder am Ende des Papiers.).
Prof. Lindszens Vortrag
Vor über einem halben Jahrhundert hat C.P. Snow (ein Romanschriftsteller und physikalischer Chemiker, der auch in verschiedenen wichtigen Positionen des Britischen Staatsdiensts und kurzzeitig in der UK-Regierung diente) ausgezeichnet die Verflechtungen ‚zweier Kulturen‘ untersucht:
„Viele Male war ich bei Zusammentreffen von Leuten anwesend, die –nach den
Maßstäben der traditionellen Kultur – für hochgradig gebildet gehalten
wurden und die mit beträchtlicher Begeisterung ihre Ungläubigkeit bezüglich
der Unwissenheit der Wissenschaftler zum Ausdruck brachten. Ein-oder auch
zweimal war ich provoziert und habe die Gesellschaft gefragt, wie viele von
ihnen das Zweite Gesetz der Thermodynamik (2. Hauptsatz) beschreiben könnten. Die
Reaktion war kalt: Sie war negativ. Obwohl ich nach etwas gefragt hatte, was
das wissenschaftliche Gegenstück zu der Frage war: „Haben Sie ein Stück von
Shakespeares Werk gelesen?“.
Ich glaube nun, falls ich eine noch einfachere Frage gestellt hätte – so etwa
„Was bedeutet Masse, oder Beschleunigung, was ist das wissenschaftliche
Äquivalent zu der Frage „Können Sie lesen?“ – nicht mehr als einer von zehn
dieser hochgradig Gebildeten hätte den Eindruck gehabt, dass ich die gleiche
Sprache wie sie benutze. So geht es dem das großartigen Gebäude der
modernen Physik und die Mehrheit der cleversten Leute in der westlichen
Welt hat etwa so viel Kenntnis davon wie es ihre Vorgänger im Neolithikum
hatten.“
Ich befürchte, dass sich seit Snows Einschätzung vor 60 Jahren nicht viel geändert hat. Während es einige weiterhin vertreten mögen, dass Unwissenheit in der Physik die politischen Fähigkeiten nicht beeinträchtigt, so beeinträchtigt das jedoch sehr wahrscheinlich die Fähigkeit von nicht-wissenschaftlichen Politikern, mit wissenschaftlich-basierten Themen umzugehen. Die Lücke im Verständnis ist somit eine Einladung zur vorsätzlichen, bösartigen Ausnutzung. Bei der demokratischen Notwendigkeit, dass Nicht-Wissenschaftler zu einer wissenschaftlichen Frage Stellung beziehen, ersetzen Glaube und Vertrauen unvermeidlich das Verständnis, wobei triviale und stark vereinfachte falsche Erzählungen die Nichtwissenschaftlern darin beruhigen, dass sie nicht ohne wissenschaftliches „Verständnis“ handeln. Das Thema der globalen Erwärmung bietet zahlreiche Beispiele für all‘ das.
Ich möchte diese Vorlesung mit dem Versuch beginnen, die Wissenschaftler im Auditorium sich mit der wirklichen Natur des Klimasystems auseinander zu setzen , und den motivierten Nicht-Wissenschaftlern unter den Zuhörern zu helfen, dass sie bei Snows „einer von zehn“ dabei sind, um sich über die trivialen Übervereinfachungen hinwegzusetzen.
Das Klimasystem
Die nachfolgende Beschreibung des Klimasystems enthält nichts, was im Geringsten kontrovers ist und ich erwarte, dass jedermann mit wissenschaftlichem Hintergrund ohne Weiteres dieser Beschreibung folgen kann. Auch werde ich trotz der Beobachtungen von Snow versuchen, diese Beschreibung auch für den Nicht-Wissenschaftler verständlich zu machen.
Das System, das wir betrachten, besteht aus zwei turbulenten Flüssigkeiten (die Atmosphäre und die Ozeane) , die sich miteinander in Wechselwirkung befinden. Mit „turbulent“ meine ich einfach, dass es durch irreguläre Kreisläufe gekennzeichnet ist – wie zum Beispiel in einem gurgelnden Bach oder in kochendem Wasser – allerdings hier im planetarischen Maßstab der Ozeane und der Atmosphäre. Der Gegensatz von turbulent wird laminar genannt, aber jede Flüssigkeit, die zu einer genügend schnellen Strömung gezwungen wird, die wird turbulent – und Turbulenz begrenzt deutlich eine Berechenbarkeit und Voraussage. Mit Wechselwirkung meine ich einfach, dass sie Druck und Stress aufeinander ausüben und dabei auch Wärme austauschen.
Diese Flüssigkeiten befinden sich auf einem rotierenden Planeten, der ungleichmäßig von der Sonne „beheizt“ wird. Die Bewegungen in der Atmosphäre (und in geringerem Ausmaß in den Ozeanen) werden durch diesen ungleichmäßigen Einfluss der Sonne verursacht. Die Sonne selbst kann die Erde durchaus unveränderlich bestrahlen, aber sie scheint senkrecht über den Tropen während sie die Polregionen nur streift. Was die Ozeane antreibt, ist weitaus komplexer und es enthält den Windeinfluss ebenso wie das Absinken von kaltem und von Salzwasser. Auch die Erdrotation hat ebenfalls viele Auswirkungen, aber hier sollten wir einfach zur Kenntnis nehmen, dass es zu einer Einstrahlung entlang der Breitengrade führt, die somit einer Kreisbahn folgt.
Die Ozeane haben Kreisläufe und Strömungen, die in Zeiträumen von Jahren bis zu Jahrtausenden ablaufen, und diese Systeme befördern Wärme hin zu der Oberfläche und weg von ihr. Auf Grund der Ausmaße und der Dichte der Ozeane sind diese Flussgeschwindigkeiten generell sehr viel kleiner als wie in der Atmosphäre und sind durch weitaus längere Zeitabläufe gekennzeichnet. Die Tatsache, dass diese Zirkulationen Wärme zur Oberfläche und von ihr fort transportieren, bedeutet, dass sich die Oberfläche selbst niemals in einem Gleichgewicht mit dem Weltraum befindet. Das heißt, es gibt niemals ein exaktes Gleichgewicht zwischen der von der Sonne eingehenden Wärme und der von der Erde abgegebenen Strahlung weil Wärme immer in den Ozeanen gespeichert und auch von ihnen abgegeben wird und sich deren Oberflächentemperatur daher immer etwas verändert.
Zusätzlich zu den Ozeanen steht die Atmosphäre in Wechselwirkung mit einer sehr unregelmäßigen Landoberfläche. Wenn Luft über eine Gebirgsregion strömt, ist dieser Fluss stark gestört. Deshalb spielt die Topographie bei der Beeinflussung des regionalen Klimas eine Hauptrolle. Diese gestörten Luftströmungen erzeugen sogar Strömungswellen, die das Klima in entfernten Regionen ändern können. Computersimulationen des Klimas können diese Auswirkungen nicht ausreichend beschreiben.
Ein entscheidender Bestandteil des atmosphärischen Teils ist Wasser – in seinen flüssigen, festen oder dampfförmigen Phasen – und die Phasenänderungen haben gewaltige Auswirkungen auf die Energieflüsse. Jede dieser beiden Komponenten hat auch bedeutende Auswirkungen auf die Abstrahlung. Sie alle wissen, dass es Wärme braucht, um Eis zu schmelzen, und weitere Wärme ist erforderlich, um aus Wasser Dampf zu erzeugen. Der Begriff Feuchtigkeitsgehalt bezieht sich auf die Wasserdampfmenge in der Atmosphäre. Die Flussrichtung der Wärme kehrt sich um, wenn die Phasenwechsel in Gegenrichtung erfolgen; das heißt, wenn Wasserdampf zu Wasser kondensiert und wenn Wasser gefriert. Das Freiwerden von Wärme bei der Wasserdampf-Kondensation erzeugt Gewitterwolken (bekannt als Cumulonimbus), und die in einer Gewitterwolke existierende Energie ist mit der einer H-Bombe vergleichbar. Ich sage das, um zu veranschaulichen, dass diese Energieumwandlungen sehr beträchtlich sind. Wolken bestehen aus Wasser in Form von kleinen Tröpfchen und Eis in Form kleiner Kristalle. Normalerweise werden diese Tröpfchen und Eiskristalle durch aufsteigende Luftströmungen in der Schwebe gehalten, aber wenn sie genügend groß angewachsen sind, fallen sie durch die aufsteigende Luft als Regen oder Schnee. Nicht nur sind die bei den Phasen-Umwandlungen beteiligten Energien bedeutend, ebenso ist es eine Tatsache, dass sowohl Wasserdampf als auch Wolken (Eis und Wasser enthaltend) die Abstrahlung stark beeinflussen.
Obwohl ich noch nicht den Treibhauseffekt erwähnt habe, bin ich sicher, dass Sie alle gehört haben, dass Kohlendioxid ein Treibhausgas ist und dass dies seine Erwärmungswirkung erklärt. Sie sollten deshalb verstehen, dass die zwei wichtigsten Treibhaus-Substanzen bei weitem Wasserdampf und Wolken sind.
Wolken sind außerdem bedeutende Reflektoren für das Sonnenlicht.
Das Einheitsmaß für die Beschreibung von Energieflüssen ist Watt pro Quadratmeter. Der Energievorrat dieses Systems enthält die Absorption und die Rückstrahlung von etwa 200 Watt pro Quadratmeter. Eine Verdopplung des CO2 hat eine Störung von 2 % dieses Energiebudgets zur Folge. Das bewirken auch kleinere Veränderungen in der Bewölkung oder andere Merkmale, und solche Veränderungen gibt es häufig. Die Erde empfängt etwa 340 Watt pro Quadratmeter von der Sonne, aber ca. 140 Watt pro Quadratmeter werden einfach in den Weltraum zurück reflektiert – sowohl von der Erdoberfläche, aber wichtiger noch, durch die Wolken. Damit bleiben etwa 200 Watt pro Quadratmeter, die von der Erde emittiert werden müssen um ein Gleichgewicht herzustellen. Die Sonne strahlt im sichtbaren Teil ihres Strahlungsspektrums, weil ihre Temperatur ca. 6000K beträgt. „K“ bezieht sich auf Kelvin-Grade, die einfach Celsiusgrade plus 273 sind. Null K ist die niedrigste mögliche Temperatur (-273 oC). Die Temperatur bestimmt das Spektrum der emittierten Strahlung.
Wenn die Erde gar keine Atmosphäre hätte (aber für die hier angewendete Beweisführung nach wie vor 140 Watt pro Quadratmeter reflektieren würde), dann müsste sie (den verbleibenden Überschuss) mit einer Temperatur von etwa 255K (minus 18 oC) abstrahlen, und bei dieser Temperatur wäre ihre Strahlung überwiegend im Infrarot.
Selbstverständlich hat die Erde eine Atmosphäre und Ozeane, und das schafft eine Unmenge an Komplikationen. Seien Sie also gewarnt, denn was nun folgt, erfordert eine nicht unerhebliche Konzentration. Die Verdunstung aus den Ozeanen schafft Wasserdampf in die Atmosphäre – und Wasserdampf absorbiert und emittiert sehr stark Strahlung im Infrarot. Das meinen wir, wenn wir Wasserdampf ein Treibhausgas nennen. Der Wasserdampf behindert ganz wesentlich Infrarotstrahlung, die die Erdoberfläche verlassen will, was dazu führt, dass sowohl die Oberfläche und (durch Energieübertragung) auch die Luft in Oberflächen-Nähe aufgeheizt wird und – wie bei einem erwärmten Topf mit Wasser – setzt die Konvektion ein. Weil die Luftdichte mit der Höhe abnimmt, dehnen sich die Bestandteile bei ihrem Aufstieg aus. Das führt wiederum dazu, dass sich diese Bestandteile abkühlen, und ihre Vermischung bewirkt eine abnehmende Temperatur bei größerer Höhe anstelle einer konstanten Temperatur. Um dieses Geschehen noch komplizierter zu machen, sinkt der Anteil des Wasserdampfs, den die Luft halten kann, bei dieser sinkenden Temperatur. Bei einer bestimmten Höhe existiert dort derart wenig Wasserdampf, dass nun die Strahlung ab dieser Höhe in den Weltraum „entkommen“ kann. An diesem Höhenniveau (um die 5 km) muss die Temperatur etwa 255K betragen, um die Energie der Einstrahlung auszugleichen. Aber weil die Konvektion – die Wärmeleitung – dazu führt, dass die Temperatur mit wachsender Höhe abnimmt, , muss jetzt die Erdoberfläche wärmer als 255K sein. Es stellte sich heraus, dass sie etwa 288K betragen muß (was die Durchschnittstemperatur der Erdoberfläche ist).
Das ist es, was man jetzt als Treibhauseffekt bezeichnet. Es ist eine interessante Kuriosität, dass es in dem Falle, dass die Konvektion eine konstante Temperatur erzeugt hätte, keinen Treibhauseffekt geben würde.
In der Realität ist die Situation noch komplizierter. Unter anderem blockiert die Existenz hoher Cirrus-Wolken, die sehr starke Absorber und Emitter von Infrarotstrahlung sind, sehr wirksam die von unten kommende Strahlung. Wenn es also derartige Wolken oberhalb von etwa 5 km gibt, dass bestimmen ihre obersten Teile die Höhe, ab der Infrarot in den Weltraum abgestrahlt werden kann. Das Hinzufügen von anderen Treibhausgasen (wie Kohlendioxid) hebt diese Emissionshöhe weiter an und wegen der konvektiven Vermischung wird dieser neue Level kälter sein. Das wiederum verringert den abgegebenen Infrarot-Strahlungsfluss und – um das Gleichgewicht wieder zu erreichen – würde sich die Atmosphäre erwärmen müssen. Eine Verdopplung der Kohlendioxid-Konzentration wird als eine Wirkung (forcing) von ca. 3,7 Watt pro Quadratmeter abgeschätzt, was etwas weniger als 2% der ankommenden Nettoenergie ist. Zahlreiche Faktoren wie Wolkenbedeckung und –höhe, Schneebedeckung und Ozeanströmungen verursachen gewöhnlich Änderungen von vergleichbarer Größenordnung.
Es ist wichtig, zur Kenntnis zu nehmen, dass ein solches System in Zeiträumen fluktuiert, die von Sekunden bis zu Jahrtausenden variieren – sogar in der Abwesenheit eines stabilen Antriebs anstelle der Sonne. Viele der populären Veröffentlichungen (auf beiden Seiten der Klimadebatte) nehmen an, dass alle Veränderungen von bestimmten externen Faktoren bewirkt werden. Selbstverständlich ist das Klimasystem von der Sonne angetrieben, aber selbst wenn der solare Antrieb konstant wäre, würde sich das Klima verändern. Das wussten Sie alle schon seit langem – selbst wenn Sie das nicht realisierten. Schließlich können Sie ohne Schwierigkeit feststellen, dass das Streichen einer Violinensaite mit einem Bogen dazu führt, dass die Saite zu vibrieren beginnt und Schallwellen erzeugt werden. In einer ähnlichen Weise reagiert das Atmosphäre-Ozean-System auf einen gleichmäßigen Antrieb mit seinen eigenen Variationsarten (die zugegebenermaßen oft komplexer sind als die Wirkungsweise einer Violinensaite). Überdies können angesichts der massiven Natur der Ozeane derartige Veränderungen eher Zeiträume von Jahrtausenden als von Millisekunden zur Folge haben. El Niño, das Jahre andauert, ist ein relativ kurzfristiges Beispiel, aber die meisten dieser innewohnenden Zeitveränderungen sind zu lang, um mit unseren – zeitlich relativ kurzen- gemessenen Aufzeichnungen erkannt zu werden. Die Natur bietet zahlreiche Beispiele von autonomen Veränderlichkeiten; einschließlich des annähernd 11-jährigen Sonnenflecken-Zyklus sowie die Umkehrungen des Magnetfeldes der Erde nach etwa hunderttausend Jahren. In dieser Beziehung unterscheidet sich das Klimasystem nicht von anderen natürlichen Systemen.
Selbstverständlich reagieren derartige Systeme auf externe Antriebe, aber solche Antriebe werden von ihnen nicht benötigt, um eine Variabilität zu zeigen. Während das oben Gesagte vollkommen unstreitig ist, denken Sie dennoch für eine Weile darüber nach. Bedenken Sie die gewaltige Verschiedenheit und Komplexität des Systems sowie die Vielfalt der Variabilitäts-Mechanismen wenn wir die derzeitige Schilderung bewerten, die gewöhnlich als entschiedene Wissenschaft (settled science) präsentiert wird.
Die populäre Schilderung und ihre politische Entstehung
Hier ist nun die derzeitige populäre Beschreibung dieses Systems. Das Klima, ein komplexes Multifaktoren-System, kann in gerade einer Veränderlichen, der globalen Temperaturveränderung, zusammengefasst werden – und ist hauptsächlich von der 1 – 2% Störung im Energiehaushalt auf Grund einer einzigen Veränderlichen – Kohlendioxid – unter vielen anderen Veränderlichen von vergleichbarer Bedeutung.
….Das ist ein merkwürdiges Paar von Behauptungen, das an Wunderglaube grenzt. Es ist allerdings die Beschreibung, die weithin akzeptiert wird; sogar von vielen Skeptikern. Diese Akzeptierung ist ein starker Hinweis auf das von Snow festgestellte Problem.
….Viele Politiker und akademische Gesellschaften gehen sogar noch weiter: Sie schließen sich der Ansicht an, dass Kohlendioxid die maßgebliche Variable ist und obwohl die von Menschen verursachten CO2-Beiträge im Vergleich mit den viel größeren, aber unbestimmten natürlichen Austauschprozesse mit den Ozeanen und der Biosphäre gering sind, sind sie davon überzeugt, dass sie genau wissen, welche Politiken eingeführt werden müssen, um die Kohlendioxidwerte zu steuern.
….Obwohl einige Wissenschaftler diese Ansicht in den vergangenen 200 Jahren vertreten haben, wurde sie bis zu den 1980er Jahren generell zurückgewiesen. Als 1988 der NASA-Wissenschaftler James Hansen dem US-Senat mitteilte, dass die Wärme des Sommers die erhöhten Kohlendioxid-Werte widerspiegelt, berichtete sogar das Science Magazin, dass die Klimatologen skeptisch waren. Die Etablierung dieser extremen Position als ein Dogma in dem bisherigen Zeitraum ist politischen Akteuren zuzuschreiben sowie Anderen, die die Möglichkeiten ausnutzen wollten, die im multi-Billionen-Dollar Sektor reichlich vorhanden sind. Ein Beispiel war Maurice Strong, ein weltumfassender Bürokrat und raffinierter Geschäftemacher (der seine letzten Jahre in China verbrachte, um seiner Strafverfolgung wegen seiner Rolle in den Skandalen im Öl-für-Lebensmittel-Programm der UN zu entgehen). Strong wird wiederholt zugeschrieben, dass er die Bewegung „Globale Erwärmung“ in den frühen 1980ern einleitete und später die Rio-Konferenz zu organisieren half, welche die Rahmenkonvention zum Klimawandel erarbeitete. Andere Personen wie Olof Palme und dessen Freund Bert Bolin, der der erste Vorsitzende des Intergovernmental Panel on Climate Change IPCC (Amn.:“Weltklimarat“) war, waren ebenfalls – schon in den 1970ern – beteiligt.
….Die politische Begeisterung verstärkte sich noch, als politische Ideologie eine Hauptrolle zu spielen begann. Einige Jahre zuvor sagte Christiana Figueres, die damalige Generalsekretärin, dass sich die Menschheit zum ersten Male in der Geschichte selbst die Aufgabe gesetzt hätte, absichtlich das Wirtschaftssystem zu verändern (L 1).
….Ms. Figueres glaubt dies nicht allein. Der engste Berater von Papst Franziskus übte scharfe Kritik an konservativen Klimawandel-Skeptikern in den USA und beschuldigte den Kapitalismus für ihre Ansichten. In einem Gespräch mit Journalisten kritisierte Kardinal Rodriguez Maradiaga gewisse „Bewegungen“ in den USA, die im Voraus ihre Opposition zu Franziskus‘ geplanter Enzyklika zum Klimawandel kundgetan hatten. „Die Ideologie, die Umweltthemen umgibt, ist auch mit einem Kapitalismus verbunden, der die Zerstörung der Umwelt nicht aufhalten will, weil sie ihre Profite nicht aufgeben wollen“, sagte er.
….In diesem kürzlich vergangenen August (Anm.:2018) erschien in den Proceedings of the National Academy of Sciences ein Artikel. Übersät mit „kann sein“ und „könnte sein“ folgerte er, dass eine „Gemeinsame Aktion der Menschen“ nötig sei, um „das System Erde von einer potenziellen Schwelle wegzusteuern“, um es bewohnbar bleiben zu lassen. Die Autoren sagten, dass dies eine „Verwaltung des gesamten Erdsystems – Biosphäre, Klima und die Gesellschaften“ einbeziehen müsse – und das könnte „Dekarbonisierung der Weltwirtschaft, Verstärkung der Kohlenstoffsenken in der Biosphäre, Änderung von Verhaltensweisen, neue Regierungsformen und geänderte soziale Werte beinhalten“.
….Denken Sie daran, dass in einer Welt, die an das „Vorsichtsprinzip“ glaubt, selbst die einfache Behauptung einer weit entfernten Möglichkeit extreme Aktionen rechtfertigt.
….Vermutlich ist die Macht, die diese Leute krampfhaft suchen, die Macht zum Zurückschrauben des Status und des Wohlergehens, die eine normale Person erreicht hat – und weiterhin auf der Grundlage der vom fossilen Treibstoff geschaffenen industriellen Revolution zu erreichen sucht – und sie zu ihrem
vermutlich angemessenen Status als Sklave zurückzubringen. Sehr Vielen der Ärmsten dieser Welt wird die Chance für eine Besserung ihrer Lebensumstände verboten sein.
….Dennoch haben unsere führenden Politiker Angst, sich dagegen zu wenden, wenn ihnen diese Forderungen – zusammen mit dem Schwindel, dass dem 97% der Wissenschaftler zustimmen – präsentiert werden. Und sie fahren damit fort, wie die Lemminge, den Selbstmord der Industriegesellschaft zu planen. Wiederum: Nichts beschreibt das Problem besser, als es Snow getan hat.
….Interessanterweise neigen „normale“ Leute (im Gegensatz zu unseren „gebildeten“ Eliten) dazu, den präsentierten Unsinn zu durchschauen. Woran liegt es, dass unsere Eliten so anfällig sind und was bringt viele unserer Wissenschaftler dazu, derartigen Blödsinn zu unterstützen? Die Antworten können für beide nicht sehr schmeichelhaft sein. Betrachten wir zuerst die „anfälligen“ Eliten.
1. Sie sind in einem System ausgebildet worden, wo Erfolg auf der Fähigkeit beruht, ihren Professoren zu gefallen. Anders gesagt sind sie darauf konditioniert worden, alles rational zu erklären.
2. Während sie anfällig für falsche Schilderungen sind, sind sie ökonomisch weitaus weniger anfällig als normale Leute. Sie halten sich für wohlhabend genug, um den wirtschaftlichen Leiden der vorgeschlagenen Politiken zu widerstehen – und sie sind schlau genug, um öfter von ihnen zu profitieren.
3. Die Beschreibung bzw. Schilderung ist für die Elite trivial genug, um sich einzubilden, dass sie Wissenschaft „verstehen“.
4 Für Viele (insbesondere die Rechten) veranlasst das Bemühen, als intelligent angesehen zu werden, die Furcht, dass eine Opposition gegen nirgend etwas, das als „wissenschaftlich“ bezeichnet wird, sie als ignorant erscheinen lässt. Und diese Furcht überwältigt jegliche ideologische Verpflichtung zur Freiheit, die sie haben könnten.
Keiner dieser Faktoren gilt für „normale“ Leute. Das könnte sehr wohl das stärkste Argument für eine Demokratie des Volkes und gegen eine Führung durch jene, die es „am besten wissen“:
….Wie steht es mit den Wissenschaftlern?
1. Wissenschaftler sind Spezialisten. Wenige von ihnen sind Experten der Klimaforschung. Das schließt viele vermeintliche „Klimawissenschaftler“ ein, die in dieses Gebiet Einzug hielten; als Reaktion auf die enormen Steigerungen der Fördermittel, die die Hysterie der Globalen Erwärmung begleiteten.
2. Wissenschaftler sind Menschen mit eigenen politischen Standpunkten, und viele waren darüber begeistert, dass sie ihren Status als Wissenschaftler benutzen konnten, um ihre politischen Positionen zu unterstützen (ganz ähnlich wie Berühmtheiten, deren Status einige Wissenschaftler anstreben). Als Beispiele betrachten Sie die Bewegungen gegen Nuklearwaffen, gegen die strategische Verteidigungs-Initiative, gegen den Vietnamkrieg und so weiter.
….Wissenschaftlern ist auch scharfsinnig und zynisch die Ignoranz der Nicht-Wissenschaftler bewusst und sie sehen darin eine Gefahr. Diese Furcht beruhigt die „verletzlichen“ Eliten, die besonders durch die Zusicherung erleichtert sind, dass die Theorie, die dem Alarm zugrunde liegt, trivial einfach sei und „alle“ Wissenschaftler dem zustimmen. Typisch war der frühere Senator und Außenminister John F. Kerry, als er bezüglich der Treibhaus-Erwärmung erklärte, „Ich weiß Einiges aus meiner High-School- und College-Zeit, einige Aspekte der Chemie oder Physik können schwierig sein. Aber das hier ist nicht schwierig. Es ist einfach. Kinder im geringsten Alter können es verstehen“.
Wie Sie gesehen haben, ist der Treibhauseffekt alles andere als einfach. Nur bemerkenswert brillante Kinder würden es verstehen. In Anbetracht von Kerry’s folgender Beschreibung des Klimas und der zugrunde liegenden Physik war es klar, dass er dieser Aufgabe nicht gewachsen war.
Der Beweis
An dieser Stelle könnten sich einige von Ihnen über alle der sogenannten Beweise des gefährlichen Klimawandels wundern. Was ist mit dem verschwindenden arktischen Eis, dem ansteigenden Meeresspiegel, mit den Wetterextremen, den verhungernden Eisbären, dem syrischen Bürgerkrieg und allem Übrigen? Die riesenhafte Vielfalt der Behauptungen macht es unmöglich, auf einen besonderen Fehler hinzuweisen, der auf alle von diesen zutrifft. Selbstverständlich würde die Erwähnung von Veränderungen – selbst wenn diese Beobachtungen korrekt sind (obwohl sie es überraschenderweise oft nicht sind) – diese nicht per se in einen Zusammenhang mit einer Treibhaus-Erwärmung bringen. Und auch nicht auf eine Gefahr hinweisen. Beachten Sie, dass sich die meisten dieser sogenannten Beweise auf Dinge beziehen, über die Sie keine persönlichen Erfahrungen besitzen. Einige dieser Behauptungen, zum Beispiel jene, die sich auf Wetterextreme beziehen, widersprechen dem, was sowohl die Theorie der Physik als auch Erfahrungsdaten zeigen. Die Absicht dieser Behauptungen ist offensichtlich, die Leute zu erschrecken und zu benebeln, um den Anschein zu erwecken, dass es Beweise gibt, wo es tatsächlich gar keine gibt. Falls es über irgendetwas einen Beweis gibt, dann ist es der über die Richtigkeit der Beobachtungen C.P. Snows. Einige Beispiele sollen zeigen, was ich meine.
Erstens: Wenn etwas ein Beweis sein soll, muss es eindeutig vorausgesagt worden sein. (Das ist eine notwendige, aber bei Weitem nicht ausreichende Bedingung.) Abbildung 1 zeigt die IPCC- Modellierungs-Vorhersage über das Sommer-Minimum des arktischen Meereises im Jahre 2100 – in Bezug auf den Zeitraum 1980-2000. Wie Sie sehen, gibt es ein Computermodell für jegliches Resultat. Das erscheint ein wenig wie das Rezept, ein Meisterschütze zu werden: Schieße zuerst und behaupte dann, was immer Du getroffen hast, sei das Ziel gewesen. Wenden wir uns dem Thema der Temperaturextreme zu: Gibt es irgendwelche Daten, die eine Besorgnis entstehen lassen? Was diese Extreme betrifft, zeigen die Daten keinen Trend – und das IPCC stimmt dem zu. Sogar Gavin Schmidt, Jim Hansens Nachfolger in NASA’s New Yorker Niederlassung, GISS, bemerkte, dass „allgemeine Behauptungen über Extreme können beinahe nirgends in der Fachliteratur gefunden werden, scheinen aber in populären Medien reichlich vorhanden zu sein“. Er sagte weiterhin, dass es nur wenige Sekunden des Nachdenkens brauche, um zu erkennen, dass die populäre Vorstellung, dass „globale Erwärmung“ bedeuten würde, alle Extreme müssten
immer stärker werden, „Nonsense“ sind.
Abb. 1: Klima-Modellierungsprojektionen der Meereis-Verlustrate
in der Arktis
Quelle: Eisenmann et al., J. Clim., 2011
Der Kern dieses Unsinns ist das Versagen bei der Unterscheidung zwischen Wetter und Klima. Also: Die globale Erwärmung bezieht sich auf den willkommenen Temperaturanstieg von ca. 1o C nach dem Ende der Kleinen Eiszeit vor etwa 200 Jahren. Andererseits sind Wetterextreme durch Temperaturveränderungen der Größe von 20o C gekennzeichnet. Dermaßen große Veränderungen sind durch eine vollkommen andere Entstehung gekennzeichnet als eine globale Erwärmung. Grob gesagt sind sie die Folge von Winden, die warme und kalte Luft aus entfernten Regionen, die sehr warm oder sehr kalt sind, transportieren. Diese Winde haben eine Wellenform. Die Stärke dieser Wellen hängt von dem Temperaturunterschied zwischen den Tropen und der Arktis ab (wobei größere Unterschiede zu stärkeren Wellen führen). Aber alle Modellrechnungen, die zur Beschreibung der globalen Erwärmung benutzt werden, präsentieren die Vorhersage, dass sich dieser Temperaturunterschied eher verringern als zunehmen wird. Somit würde ein Anstieg der Temperaturextreme die beste Unterstützung für die Idee einer globalen Abkühlung anstelle einer globalen Erwärmung sein. Wie dem auch sei, wissenschaftlich ungebildete Leute scheinen unfähig zu sein, eine globale Klima-Erwärmung von Temperaturextremen infolge des Wetters zu unterscheiden. Wie bereits erwähnt, scheint es tatsächlich keinen feststellbaren Trend bezüglich der Wetterextreme zu geben. Es ist nur die größere Beachtung , die die Medien dem Wetter widmen, und das Ausnutzen dieser „Nachrichten“-Berichterstattung durch Leute, die wissen, dass Projektionen von Katastrophen in ferner Zukunft kaum zwingend sind und dass sie deshalb eine Methode brauchen, um das Publikum davon zu überzeugen, dass die Gefahr unmittelbar droht, auch wenn es nicht so ist.
…Das war ebenfalls die Sache mit dem Meeresspiegel-Anstieg. Dieser betrug in Hunderten von Jahren um die 8 Zoll (20,3 cm) pro Jahrhundert – und wir waren ganz eindeutig in der Lage, damit zurecht zu kommen. Um Angst zu erzeugen, mussten die Modellierungen, die weitaus größere Anstiege voraussagen, zu Hilfe gerufen werden. Als eine Tatsache ist es seit Langem bekannt, dass bei den meisten Küstenstationen Veränderungen des Meeresspiegels, wie sie durch Pegelmessgeräte ermittelt wurden, hauptsächlich auf Veränderungen der Festlandshöhe zurückzuführen sind, die sowohl durch die Tektonik als auch durch die Landnutzung hervorgerufen werden.
…Außerdem ist die kleine Veränderung der mittleren globalen Temperatur (eigentlich die Änderung des Temperaturanstiegs) viel geringer als was die Computermodelle, die vom IPCC genutzt werden, vorausgesagt haben. Selbst wenn die gesamte Veränderung vom Menschen verursacht wäre, wäre das am meisten im Einklang stehend mit einer niedrigen Sensitivität (Anm.: das Maß der Temperaturbeeinflussung bei einer Konzentrationsveränderung) durch hinzugefügtes Kohlendioxid – und das IPCC behauptet nur, dass der größte Teil (nicht aber der gesamte Teil) der Erwärmung über die vergangenen 60 Jahre durch die Aktivitäten der Menschen verursacht wäre. Also scheint der Fall eines vom Menschen gemachten Klimawandels kein ernstes Problem zu sein. Allerdings hält das ignorante Politiker nicht davon ab, zu erklären, dass die IPCC-Behauptung über diesen eindeutigen Zusammenhang gleichbedeutend mit einem unzweideutigen Beweis für ein Desaster ist.
…Rosinenpicken ist immer ein Kernpunkt. So gab es vor Kurzem die Behauptung, dass sich der Eisverlust in Grönland verstärkt hat und dass die Erwärmung das noch verschlimmern wird. (L 2). Weggelassen von diesem Bericht wurde der Befund sowohl der NOAA (Anm.: National Oceanic and Atmospheric Administration der USA) als auch des Dänischen Meteorologischen Instituts, dass die Eismasse von Grönland tatsächlich zunimmt (L.3). In der Tat können diese Feststellungen beide wahr sein – und tatsächlich verursacht ein Anwachsen des Eises auf dem Festland auch einen stärkeren Transport der Eismassen von der Küste ins Meer (Anm.: – durch die Gletscher).
…Falsche Darstellungen, Übertreibung, Rosinenpickerei oder glatte Lügen überdecken ziemlich viele der sogenannten Beweise.
Schlussfolgerung
So sieht es aus. Eine unglaubwürdige Mutmaßung, versehen mit falschen Beweisen und unablässig wiederholt, wurde zu „politisch korrektem Wissen“ und wird benutzt, um den Sturz der industrialisierten Zivilisation zu organisieren. Was wir unseren Enkeln überlassen werden, ist nicht ein vom industriellen Fortschritt geschädigter Planet, sondern ein Zeugnis unergründlicher Dummheit sowie eine Landschaft, die durch verrostende Windparks und verfallende Solarmodul-Felder degradiert ist. Falsche Behauptungen über eine Zustimmung von 97 % werden uns nicht verschonen, aber die Bereitschaft von Wissenschaftlern, den Mund zu halten, wird wahrscheinlich das Vertrauen in die Wissenschaft und ihre Unterstützung verringern. Das wäre aber vielleicht im Grunde keine so schlechte Sache- sofern es die „offizielle“ Wissenschaft betrifft.
…Wenigstens gibt es zumindest eine positiven Seite in der gegenwärtigen Situation. Keine der vorgeschlagenen Politiken wird viel Wirkung auf die Treibhausgase haben. Deshalb werden wir weiterhin von der einen Auswirkung profitieren, die eindeutig zu einem erhöhten Kohlendioxid gehört, nämlich dessen effektive Rolle als Pflanzendünger und als wirksamer Schutz vor der Schädigung der Pflanzen durch Trockenheit. Mittlerweile verlangt das IPCC , dass wir eine weitere Erwärmung von 0,5 oC verhindern müssten, obwohl die 1 oC , die bisher eingetreten sind, von der größten Steigerung des Wohls der Menschheit in der Geschichte begleitet wurden. Wir pflegten in unserer Kindheit in der Bronx dazu zu sagen: „Stell Dir das mal vor“.
Notizen (Quellen) von R. Lindzen
L 1. Dies ist das erste Mal in der Menschheitsgeschichte, dass wir absichtlich die Aufgabe für uns festlegen, innerhalb eines definierten Zeitraums das Modell der ökonomischen Entwicklung zu ändern, das für mindestens 150 Jahre seit der industriellen Revolution herrschte.
L 2. KA Graeter et al (2018) Ice core records of West Greenland melt and climate forcing. Geophysical Research Letters45(7), 3164-3172.
L 3. https://www.climate.gov/news-features/understanding-climate/greenland-ice-sheets-2017-weigh-suggestes-small-increase-ice-mass.
Ergänzungen des Übersetzers
Die zeitliche Begrenzung des Vortrags führte zwangsläufig dazu, dass Prof. Lindzen einige Ergänzungen und Erläuterungen auslassen musste. Dazu gehören zahlreiche weitere von ihm in anderen Veröffentlichungen und Vorträgen vorgestellte Argumente, die man aus den unten angeführten Quellen entnehmen kann.
Hier werden Ergänzungen zu dem obigen Vortrag gebracht, die eine Anzahl von weiteren Beweisen vorstellen. Sie sollen für die Leser dieses Papiers eine zusätzliche Hilfe für das Verständnis der Manipulationen und bewusst falschen – angeblich wissenschaftlich verifizierten – Tatsachenbehauptungen durch das IPCC und dessen Unterstützer in den Medien sein.
Punkt 1: Prof. Lindzen hat im ersten Teil seines Vortrags eingehend die extreme Komplexität der im Klimageschehen ablaufenden Prozesse erläutert. Auf die in großer Anzahl durch wissenschaftliche Veröffentlichungen vorgestellten Versuche, mit Hilfe von mathematischen Beschreibungen (Algorithmen) dieser Prozesse, die dann in Computer „eingefüttert“ werden, Ergebnis-Aussagen für künftige Szenarien zu bekommen (die sog. Klimamodellierungen), gilt das in der Computerbranche bekannte „GIGO-Prinzip“. Es bedeutet „Garbage-in/ Garbage out“. Der eingegebene Müll wird wieder als Ergebnis ausgestoßen. Das gilt insbesondere für die Versuche, das Klima analytisch zu erfassen und mathematisch zu beschreiben – was aber unmöglich ist (s.u.). Es ist GIGO pur.
In der Fachwelt bestens bekannt, in den Medien aber niemals erwähnt, hat sogar das IPCC in zwei seiner Berichte diese Unmöglichkeit ausdrücklich festgestellt. Aber nur in den alle 5 Jahre veröffentlichten über 700 Seiten starken Langfassungen und niemals in den dagegen winzigen „Zusammenfassungen für Politiker“ und Medien.
Das IPCC veröffentlichte:
A) In seinem 3.Bericht (IPCC-TAR) von 2001 auf Seite 774: Quelle E 1.
„In climate research and modelling, we should recognize that we are dealing with a coupled non-linear chaotic system, and therefore that the long-term prediction of future climate states is not possible.”
Übersetzung:
„In der Klimaforschung und –Modellierung sollten wir zur Kenntnis nehmen, dass wir es mit einem gekoppelten nichtlinearen chaotischen System zu tun haben, und dass deshalb die Langzeit-Vorhersage zukünftiger Klimazustände nicht möglich ist.“
Und auch im 4. IPCC-Bericht vom Nov. 2007 (AR4), Kap. 8, Seite 600 – selbstverständlich nicht in der “Summary for Policymakers – veröffentlichte das IPCC (E 2):
„Models continue to have significant limitations, such as in their representation of clouds, which lead to uncertainties in the magnitude and timing, as well as regional details, of predicted climate change.“
Also:
“Modelle weisen weiterhin ernsthafte Begrenzungen auf, wie in ihrer Darstellung von Wolken, was zu Ungewissheiten im Ausmaß und in der Zeitbestimmung und ebenso bei den regionalen Details des vorausgesagten Klimawechsels führt.”
Wozu anzumerken ist, dass die Bewölkung den entscheidenden Einfluss auf das Klima hat: In ihrer Entstehung, sogar in der Wirkung extraterrestrischer Einflussmechanismen auf den atmosphärischen Wasserdampf zur Kondensation und Tröpfchenbildung; in ihren inneren Wechselwirkungen (- Wärmeflüsse; Strömungen; Aggregatzustand des Wassers und dessen Änderungen; interne Energiebilanz -); der Rückstrahlung in den Weltraum und angesichts der vorherrschenden, unberechenbaren Turbulenzen . Der zitierte Satz aus dem 3. IPCC-Bericht fasst diese Feststellung knapp und klar zusammen.
Es liegt somit am – offenbar für immer – unlösbaren Problem, dafür exakte Algorithmen zu finden. Auch der schnellste Rechner aller Zeiten kann da nicht helfen : GIGO. Und auf Grund der dennoch präsentierten unbrauchbaren Modellierungen und der unwissenschaftlichen Mittelwertbildung ihrer wild streuenden Kurven (s.u.) reagiert die Politik mit Panik und Aktionen anstatt mit Ablehnung.
Punkt 2: In seinem Vortrag hat Prof. Lindzen in dem Kapitel „Der Beweis“ ein besonders typisches Beispiel für die Resultate vorgestellt, die sich aus dem mangelnden Verständnis der Zusammenhänge und den dennoch dafür entworfenen mathematischen Modellen ergeben. Die nicht vom (schuldlosen) Computer erzeugten, sondern von den verschiedenen Annahmen der Modellierungsbetreiber beschlossenen Algorithmen, die die Geschwindigkeit des Meereisverlustes in der Arktis ermitteln sollten, produzierten unvermeidlich
geradezu grotesk streuende Ergebnisse.
Dass die jeweiligen Eisverlustmodellierer auch noch die Namen ihrer Institute nennen ließen, wird hier nicht weiter kommentiert.
Es gibt zu diesem Thema ein weiteres Beispiel, das durch den 5. Bericht des IPCC (AR 5; 2014/2015) eine besonders große Verbreitung und Beachtung erfuhr, weil damit das Ergebnis aller Projektionen und Trend-Voraussagen für die Veränderung der Globaltemperaturen bis zum Jahre 2050 zusammengefasst wurde.
Es handelt sich dabei um die Abbildung 11.25, die in der Technical Summary (TS) als Abbildung TS-14 zu finden ist (E 3 und E 4):
Bildunterschrift: Zeitnahe Projektionen der globalen Durchschnittstemperatur, relativ zum Zeitraum 1986-2005.
Erläuterungen des Übersetzers zu Abb. TS-14:
Dicke schwarze Linie: HadCRUT-Datensätze der mtl. Temperaturmessungen als Kombination der Meeresoberflächen-Temp.-Messungsergebnisse , die vom Hadley-Center des brit. Meteorologie Amtes ermittelt werden und der Landoberflächen-Temperaturen, die von der Klimaforschungs-Abteilung der Universität von East Anglia (CRU) zusammengestellt werden. Ende dieses Observationsberichtes in der Abbildung im Jahr 2012.
Die RCP-Linien bezeichnen Gruppen von Modellen, die die gleiche Randbedingung in ihrer Bezeichnung tragen: Z.B. geht RCP 4.5 von einem Strahlungsantrieb von 4.5 Watt/qm aus, was vom IPCC den Treibhausgas-Konzentrationen zugeschrieben wird und durch die Modellierung dann einen prognostizierten Temperaturverlauf darstellen soll – dazu wurden in der RCP 4.5-Gruppe 42 verschiedene Modelle eingesetzt. (So sieht es dann auch aus.) RCP bedeutet „representative concentration pathway“.
Das RPC 8.5 sei das „Weiter so“-Szenario, also ohne wesentliche „Klimaschutz“-Maßnahmen.
Diese Abbildung repräsentiert die Grundlage aller IPCC-Behauptungen und –Forderungen.
Die geradezu wüste, auseinanderlaufende Streuung der Temperaturverläufe, die die Simulationen zeigen, reichen im Jahre 2050 von +0,5oC bis + 2,5oC. Bereits im Jahr 2012 (Ende der hier eingezeichneten HadCRUT-Messungen) hat sich die gesamte Modellsammlung von dem wahren, gemessenen Verlauf getrennt. Wie der Unterschied heute, also 10 Jahre später, aussieht, wird das IPCC noch zu weiteren verzweifelten Erklärungsversuchen für seinen 6. Bericht AR 6, der ab 2017 erstellt und 2021 oder 2022 veröffentlicht wird, veranlassen. Das ist bereits heute schwierig, wie es im AR 5 nachzulesen ist (s.u.).
Die Probleme des IPCC mit der als „Pause“ ( engl. Hiatus) bezeichneten Stagnation des gemessenen realen Temperaturverlaufs.
Bezeichnend ist diese Benennung des eindrucksvollen Stopps einer weiteren Erwärmung seit 1998. Es durfte keinesfalls das Ende des Anstiegs der Globaltemperatur sein, sondern nur eine „Pause“ –eine allerdings wegen des weiteren deutlichen Anstiegs des angeblich schuldigen atmosphärischen CO2 rätselhafte Pause -nach der es mit der Erwärmungskatastrophe weitergehen sollte.
Dieser Stopp nach 1998 dauert nun mittlerweile 12 Jahre an, was allen IPCC-Modellprojektionen widerspricht.
Zum HadCRUT-Trend im besagten „Pausen“-Zeitraum 1998-2012 wird angegeben: 0,04 oC pro Dekade. Das ist nahezu eine Tendenz „Null pro Dekade“.
Dieser Stopp wird vom IPCC als „Pause“ (Hiatus) bezeichnet, um ihn als kurzzeitige Änderung des stets verkündeten katastrophalen Temperaturanstiegs zu markieren, der selbstverständlich dennoch eintreten werde. Dieses IPCC-Problem wird natürlich mit jedem weiteren „Hiatus“-Jahr größer. Das führte immerhin dazu, dass der in der obigen Abb. TS-14 im Technical Summary (TS) des AR 5 prognostizierte und immer unglaubwürdiger werdende künftige Verlauf der Globaltemperatur mit einem Bündel verschiedener Deutungen kommentiert wurde. Das Ergebnis dieser Verteidigung war klar: Diese (angebliche) Pause weckt zwar einige Zweifel, aber die Prophezeiungen der Modelle sind selbstverständlich zuverlässig. Bitter für die Tatsachen.
Die dazu angeführten möglichen Gründe für den Stopp werden – wie im gesamten Bericht – mit Wahrscheinlichkeits-Bewertungen versehen. Was nur zeigt, dass es in allen diesen Fällen keine wissenschaftlichen Beweise gibt – nur Einschätzungen und Vermutungen. Wissen wird durch Glauben ersetzt. Wobei auch dieser Glaube unglaubwürdig sein kann, wenn die von kritischen Wissenschaftlern vertretenen anderen, extraterrestrischen Einflüsse auf das Erdklima kurz abgewertet und dann ignoriert werden.
Die fast selbstkritisch klingende Kommentierung lautet:
„ Sämtliche historische Simulationen…..zeigen, dass 111 der 114 (!) Modellierungsergebnisse (RCP 4.5) von 2006 bis 2012 einen GSMT-Trend (Global Mean Surface Temp.) zeigen, der höher ist als der Trend im gesamten HadCRUT-Ensemble (Kasten TS.3, Abb.1a)….Diese Differenz zwischen simulierten und gemessenen Trends könnte zustande kommen durch eine Kombination von a) der internen Klima-Variabilität, b) fehlendem oder falschem RF (Strahlungsantrieb) und c) Fehler der Modell-Reaktion.“
Interessant ist jetzt die Erwähnung einer altbekannten, harten Kritik der skeptischen Wissenschaftler bezüglich der Wirkung des Wasserdampfs, für die das IPCC eine unbewiesene positive Rückkopplung (erhöhte Erwärmung) quasi verfügt hat, was laut den Kritikern den Anstieg der gesamten Modellierungsergebnisse bewirkt hat.-
Unter „Model Response Error“ (Fehler der Modellierungsergebnisse) werden im TS diese Möglichkeit genannt:
„Die in letzter Zeit beobachtete Erwärmungspause….kann annähernd in gleichem Maße einem Abkühlungsbeitrag durch interne Variabilität und einem verringerten Trend beim externen Antrieb (Expertenmeinung; mittlere Wahrscheinlichkeit) zugeschrieben werden. Die Verringerung des Antriebs-Trends ist hauptsächlich die Folge eines negativen Antriebs-Trends sowohl von Vulkanausbrüchen als auch von der Abwärts-Phase des Sonnenzyklus.
Allerdings gibt es nur eine niedrige Vertrauenseinschätzung („low confidence“) bei der quantitativen Abschätzung der Rolle des Antriebstrends bei der Verursachung der Pause – wegen der Ungewissheit in Bezug auf die Stärke des vulkanischen Antriebstrends und ebenfalls „low confidence“ in Bezug auf den Aerosol-Antriebstrend.“ (Ende des Zitats).
Diese eingeräumten Fehlermöglichkeiten sind bemerkenswert. Es werden sowohl die Sonnenzyklen (Sonnenflecken-Zahlen) als auch Aerosole genannt; allerdings mit der Bewertung der Ungewissheit bezüglich ihrer Stärke. Schon das ist anzuzweifeln, denn zu beiden Sachthemen gibt es zahlreiche wissenschaftliche Publikationen.
Besonders riskant für eine Begründung des Hiatus ist die Nennung von Vulkanausbrüchen, von denen einige durch extreme Staubemissionen die Sonneneinstrahlung für 1 – 2 Jahre verringern konnten. Das gab es bereits, aber gerade nicht im Zeitraum der „Pause“. Es gab davor zwei Ausbrüche, die eine kurze Absenkung der Globaltemperatur zur Folge hatten: El Chichon (1984-1986) und Pinatubo (1992-1994). Der Versuch, mit seit 1995 nicht mehr erfolgten klimabeeinflussenden Vulkanausbrüchen die „Pause“ zu begründen, könnte in das Gegenteil umschlagen: In der Literatur-Quelle E 8 (Roy Spencer, UAH Satellitenmessungen) wird vielmehr eine langfristig wirkende ozeanische Temperaturerhöhung durch die zwei o.g. Vulkanausbrüche erläutert. Die Messkurve müsste daher für die Zeit der „Pause“ weiter abgesenkt werden – für 2019 um ca. 0,1 oC.
Wenn also natürliche Vorgänge die Stagnation der Globaltemperatur verursachen, bleibt für die CO2-Hypothese nicht mehr viel übrig.
Die eingeräumte Möglichkeit c), also falsche Modelle, ist in einem IPCC-Bericht schon recht peinlich. Dabei ist die HadCRUT-„Pause“ inzwischen nicht nur bis 2012, sondern bereits mindestens bis 2018 messtechnisch bekannt – beträgt also 20 Jahre. Sechs weitere Jahre „Hiatus“ dürften die Verteidigung der Modellierungen und die verbliebenen Argumente für die dann überdeutlichen Diskrepanzen sehr erschweren.
Nach den bisherigen Erfahrungen mit dem politisch gesteuerten IPCC kann es jedoch keine Änderung seiner Politik geben, selbst wenn es angesichts der angewachsenen Kritik der an den Berichten beteiligten Wissenschaftler für die davon bereinigten ultrakurzen „Zusammenfassungen für politische Entscheidungsträger“ keine Rechtfertigung mehr gibt.
Notizen (Quellen) dieser Ergänzung
E.1: 3. IPCC-Bericht (IPCC-TAR) von 2001, Seite 774
E.2: 4. IPCC-Bericht vom Nov. 2007 (AR4), Kap. 8, Seite 600
E.3: 5. Bericht des IPCC (AR 5; 2014/2015); Abbildung 11.25, die in der Technical Summary (TS) als Abbildung TS-14 zu finden ist.
E 4: Paul Matthews: New Climate Models Even More Wrong; GWPF (Global Warming Policy Foundation). https://www.cliscep.com/2019/11/05/new-climate-models-even-more-wrong/ Chris Frey (Übers.) : „Neue Klimamodelle – sogar noch falscher“; EIKE; 19.11.2019.
E 5: Roger Pielke Sr.: “IPCC Models Close To Being Refuted By New Research” – Climate Science, 23. Jan. 2012
E 6: Clive Best: “Beweise für eine negative Wasserdampf-Rückkopplung”. 03.06.2012, Übers. Chris Frey, http://www.eike-klima-energie.eu/
E 7: Klaus–Eckard Puls, Sebastian Lüning: “Alles nur Schall und Rauch? Der Wasserdampf-Verstärker als Achillesferse in der Klimamodellierung“, 17.3.2013, http://www.kaltesonne.de/?p=9091
E 8: Roy Spencer, UAH, „40% of Ocean Warming Trend Since 1979 is Due to Volcanoes Early in the Record“, http://drroyspencer.com
E 9: Roy Spencer UAH –University of Alabama Houston, “Latest Global Temps. UAH Satellite-Based Temperature of the Global Lower Atmosphere (Version 6.0)”, 03.2020, https://www.drroyspencer.com/latest-global-temperatures/
E 10: Tim Blair: “Climate Science is settled, except when it’s not”, 11.02.2016; http://www.thegwpf.com/climate-science-is-settled-except-when-its-not/
E.11: Benny Peiser, GWPF, “Testimony to the Committee on Environment and Public Works of the U.S. Senate”, Washington, 2. Dez. 2014; http://www.thegwpf.com/content/uploads/2014/12/Peiser-Senate-Testimony-2.pdf
E.12: Dr. Timothy Ball „Eine einfache Wahrheit: Computer-Klimamodelle können nicht funktionieren“ (Übers. v. C. Frey), 28.10.2014, https://www..eike-klima-energie.eu/2014/10/28/eine-einfache-wahrheit-computer-klimamodelle-koennen-nicht-funktionieren/
Originaldokument: http://wattsupwiththat.com/2014/10/16/a-simple-truth-computer-climate-models-cannot work/
An einigen Stellen des Beitrags sowie der Kommentare musste ich stutzen. Ich denke, dass hier der Autor und einige Kommentatoren Unklarheiten aufweisen und ihre Fehler stur nicht einräumen wollen.
Das ist eines wahren Wissenschaftlers unwürdig. Das strikte Pochen auf die eigene Meinung, die Kritikunfähigkeit und besonders die stupide Eitelkeit sind hinderlich auf dem Pfad der wissenschaftlichen Richtigkeit.
Ankommen tut es also darauf, wie diese beiden Welten gekoppelt sind, wo gibt es Schnittstellen und wie arbeiten sie? In einer soliden Modellvorstellung müssen sowohl alle bekannten Grundsätze für Strahlung als auch für Thermodynamik und natürlich auch für Energieerhaltung gleichzeitig und an jedem Ort erfüllt sein.
Ein Schwergewicht in der gesamten Diskussion ist das Verhalten der Ozeane. Die Energiezufuhr erfolgt durch Strahlung (keine lokale Viertelung) und die Wirkung erfolgt über die gesamte Oberfläche permanent über 24 Stunden (Wärmespeicher, 12 Stunden Nacht sind vernachlässigbar gegenüber der Wärmekapazität der Ozeane).
Die „verbreitete“ minus 18°-Theorie ist ein Strahlungsmodell und setzt voraus, daß jeder Punkt der Erde unmittelbar jene Temperatur einnähme, die gem. SB der momentanen Bestrahlung entspricht, also ohne jegliche Speichereigenschaft. So verhält sich aber nur ein theoretischer „Schwarzer Strahler“, reale Materie tut das nicht. Die Temperatur wäre dann auch nicht konstant -18°C, sondern würde zwischen Werten von ca. +80°C und -270°C hin und her pendeln, was aber im Mittel keine -18° ergibt …
Sie haben das in ähnlicher Weise neulich schonmal formuliert und ich habe Sie auf den Artikel von Spencer hingewiesen, der das etwas realitätsnaher bestimmt (Berücksichtigung der Rotation der Erde, der Winkelabhängigkeit der Einstrahlung, Speicherfähigkeit der Erdoberfläche) … Haben Sie da mal einen Blick drauf geworfen? Sie können sich auch Herrn Kramms Papier zum „Mond als Testbett …“ ansehen. Die Temperaturen, die sich bei beiden Einstellen, sind unter den -18°C, die nur eine Abschätzung für die maximal mögliche Temperatur unter den verwendeten Annahmen darstellen …
„Die „verbreitete“ minus 18°-Theorie ist ein Strahlungsmodell und setzt voraus, daß jeder Punkt der Erde unmittelbar jene Temperatur einnähme, die gem. SB der momentanen Bestrahlung entspricht, ….“
Das ist falsch. Die Berechnung der minus 18°C setzt nur voraus, dass die von einem Flächenelement der Erde ABGESTRAHLTEN Leistung mit der Temperatur nach dem SB-Gesetz verknüpft ist. Für die Bestrahlung gilt das nicht (auch nicht bei einen theoretischen „Schwarzen Strahler“). Ist die Bestrahlung größer als die Abstrahlung wird sich das Flächenelement erwärmen, et vice versa. Von der Speicherfähigkeit (Wärmekapazität) hängt dann ab, wie schnell die Erwärmung/Abkühlung erfolgt. Mit diesen Zusammenhängen können Sie für jedes Flächenelement der Erde – abhängig von der geografischen Breite und dem Sonnenstand als Funktion der Zeit – eine zeitabhängige Differentialgleichung aufstellen und so den Temperaturgang an jedem Ort der Erde bestimmen. Wenn Sie das für alle Flächenelemente der Erdoberfläche durchführen und dann den über alle Elemente gewichteten Mittelwert bilden (nicht einfach den Mittelwert von T_min und T_max nehmen!), dann erhalten Sie eine mittlere Temperatur von minus 18°C bis minus 20°C (abhängig von den gewählten Werten für Albdo, Emissivität und Wärmekapazität).
Sie können sich das aber auch einfach mal in der von Herrn Müller erwähnten Arbeit von Dr. Spencer ansehen.
Insofern gebe ich ihnen recht das die -18 Grad C falsch sind, da die mittlere Temperatur fuer den Umstand das jeder Punkt der Erde unmittelbar jene Temperatur einnähme, die gem. SB der momentanen Bestrahlung entspricht, viel kleiner waere.
Diese Temperatur kann man leicht ermitteln wenn man sich die Verteilung ansieht die entsteht wenn die Erde sich nicht drehen würde.
Sie haben recht, dass die -18 Grad C die Wärmespeicherung voraussetzt, weil es für die Abstrahlung gilt und im Ersten Fall die Nachtseite eine Temperatur von 0 K hat.
Wo sie nicht recht haben ist das sie den Temperaturgang nur durch Berücksichtigung von Strahlung ermitteln können.
Vielleicht meinten sie das auch nicht so, aber so kommt es rüber.
Die Wärmekapazität findet numerisch keine Anwendung bei der Beschreibung des THE.
Die Arbeit von Herrn Spencer die Herr Mueller hier so anpreist, versucht das. Der Hauptdiskussionspunkt ist aber welche Tiefe man denn ansetzten kann. Fuer Strahlungsprozesse spielt das meistens keine Rolle. Fuer den Waermegehalt aber schon. Je mehr Volumen desto mehr Waerme.
Dieser Umstand wird deutlich wenn man bei Herrn Spencers Model mehr als nur ein paar Zentimeter eingibt.
Hinzu kommt das Sonnenlicht weit tiefer in Wasser eindringt als nur ein paar Zentimeter.
Eine entsprechende Analyse finden sie auch hier:
https://tallbloke.wordpress.com/2019/06/06/response-to-roy-spencer-regarding-his-support-of-flat-earth/#comment-149756
mfg
Werner
Was Werner hier wieder mal nicht erwähnt – dieser Punkt wurde ausführlich mit ihm diskutiert und es wurde erklärt, dass das Modell bei einer größeren Schichtdicke länger braucht, bis sich ein Zustand einstellt, in dem im Mittel genauso viel Energie abgegeben wird, wie aufgenommen wird. Sprich das Modell braucht länger, um sich zu erwärmen bzw. abzukühlen. Am einfachsten sieht man das, wenn man im Modell mit einer Temperatur von 0K startet. Bei der vorgegebenen Schichtdicke erreicht das Modell seine eingeschwungene Temperatur, erhöht man die Schichtdicke, reicht die Zeit nicht, um die Schicht zu erwärmen …
natürlich ist auch das Modell von Dr. Spencer noch ein stark vereinfachendes Modell. Insbesondere ist der Wärmetransport ins Innere der Erdoberfläche und der Ozeane nur pauschal durch Annahme einer bestimmten Wärmekapazität (Schichtdicke ca. 20cm) beschrieben. Das könnte man natürlich mit einem höheren Modellierungsaufwand noch deutlich besser machen. Aber ist das notwendig? Ich meine Nein. Für eine grobe Abschätzung der mittleren Temperatur ohne IR-aktive Atmosphäre ist das bei weitem ausreichend.
Je größer die Wärmekapazität angenommen wird, um so mehr gleichen sich die Tag-/Nachtemperaturen an. Bei einer angenommenen Wärmekapazität, die einer Schichtdicke von 200m entspricht, ist der Unterschied zwischen Tag und Nach praktisch verschwunden (entspricht also sicher nicht der Realität). Trotzdem ändert sich die mittlere Temperatur nur von minus 20,4°C auf etwa minus 20,0 °C.
Kommt’s darauf an?
sie sagen:
“ Bei der vorgegebenen Schichtdicke erreicht das Modell seine eingeschwungene Temperatur, erhöht man die Schichtdicke, reicht die Zeit nicht, um die Schicht zu erwärmen“
Komischerweise zeigt das Model waermere Temperaturen fuer eine hoehere Schichtdicke. Das Model muesste sich also Abkuehlen. Aber sie haben ja bei 0K angefangen.
Herr Maier-Schuler,
Ich weiss nicht was sie unter Wärmekapazität verstehen. Aber falls sie meinen das in 200 m Schichtdicke mehr Energie gespeichert ist, dann gebe ich ihnen Recht. Insofern sollte man das Model mit einer hohen Schichtdicke betreiben, das die Erde nicht gerade duenn ist!
Der Unterschied den sie dann Zwischen Tag und Nacht finden ist dann praktisch verschwunden? Leider ist Ihre Einschaetzung hier falsch, da die Realitaet bezeugt das der Unterschied zwischen Tag und Nacht in der Tat gering ist. Oder kuehlt sich die erde auf 0K ab in der Nacht???
Ich denke das Resultat ist, dass die Waermespeicherfaehigkeiten im Erdsystem eine wichtige Rolle spielen. Da nur 20 cm zu nehmen ist ein Witz.
Den hier gelesen?:
https://tallbloke.wordpress.com/2019/06/06/response-to-roy-spencer-regarding-his-support-of-flat-earth/#comment-149756
Rechnen Sie mit gemessenen Werten oder mit erfundenen? Weil wenn die Erde in Wahrheit um die +15° hat (Ozeane +16,1°, Land +8,5°), dann muß diese minus 18°-Rechnung wohl falsch sein, oder?
Im übrigen ermitteln z. B. Trenberth/Kiehl ihre 340 W/m² Strahlung ab Sonne nicht rückwärts über die Abstrahlung, sondern indem sie die angenommene Solarkonstante 1360 W/m² durch vier teilen (Kreis->Kugel). Davon ziehen sie 30% Albedo ab, ergibt 238 W/m² und diese 238 W/m² ergeben gem. SB eben -18°C. Also gemittelte Strahlung ergibt Mitteltemperatur! Sie setzen also eine permanente vollsphärische Rundumbestrahlung mit 340 W/m² in der Wirkung gleich einer 1360 W/m² Punkt-Bestrahlung mit 24h-Umlauf.
Die Wirklichkeit belegt, daß eine Modellvorstellung aufgrund dieser Mittelung falsch ist! Falsch vor allem deswegen, weil die Nachtphase im Vergleich mit der Tagphase aufgrund der Speicherung vor allem der Ozeane kaum abkühlt und weil die wetterbestimmenden angetriebenen thermodynamischen Effekte (Winde, Strömungen) nur unwesentlich vom Tag/Nacht-Wechsel betroffen sind.
Sie schreiben „Rechnen Sie mit gemessenen Werten oder mit erfundenen? Weil wenn die Erde in Wahrheit um die +15° hat (Ozeane +16,1°, Land +8,5°), dann muß diese minus 18°-Rechnung wohl falsch sein, oder?“
Die Rechnung, die für eine Erde OHNE IR-aktive Atmosphäre auf eine mittlere Temperatur von minus 18°C führt, muss wegen dieser Diskrepanz doch nicht falsch sein! Die Diskrepanz fordert vielmehr, dass es noch etwas geben muss, wodurch eine Erde MIT IR-aktiver Atmosphäre auf ca. 15°C erwärmt wird. Darum dreht sich doch die ganze Diskussion hier. Mir fällt dazu nur der ‚Treibhauseffekt‘ ; eine andere Erklärung dafür habe ich bislang noch nirgends gefunden. Lasse mich aber gerne belehren.
Wenn Ihr sog. „Treibhauseffekt“ sich nahtlos und vollständig erklärbar in das System einfügen ließe, gäbe es keine Diskussion. Eine Diskussion gibt es nur, weil dies nicht so ist und dieser „Effekt“ daher nur mittels eines Perpetuum Mobiles der 2. Art erklärbar ist.
Es wird der Geschichtsschreibung obliegen, zu klären, wieso eine „Wissenschaft“ hier auf einer „absurden Erfindung“ beharrt, statt wie allgemein üblich anzuerkennen, daß die selbst aufgestellte Modellrechnung falsch ist und daher eine aktualisierte Modellvorstellung erforderlich ist.
der Treibhauseffekt ist keine Erklärung sondern die Beschreibung einer Beobachtung im Vergleich mit einigen Annahmen.
Die Erklärung zu den Beobachtungen suchen wir noch, da wie Herr Strasser es schon sagt, die derzeitige falsch ist.
Da lachen ja die Hühner!
Diese blogs reichen nie an aktuelle wissenschaftliche Fragen heran, sondern krepieren bereits vorm Einstieg in die Wissenschaft an den notwendigen Voraussetzungen.
Es kann m.E. als gesichert angesehen werden, dass die Erdoberfläche ohne eine Atmosphäre mit IR-aktiven Bestandteilen (also Gasen wie Wasserdampf und CO2, die IR-Strahlung absorbieren und entsprechend ihrer Temperatur auch wieder abstrahlen können) eine Temperatur von rund minus 18 bis 20 °C hätte. Das kann heute auf jedem Heim-PC durchgerechnet werden. Ohne den wärmenden ‚Treibhauseffekt‘ von Wasserdampf (und CO2) wäre die Erde – so wie wir das kennen – nicht bewohnbar.
Die Rolle der Mathematik ist bei der Diskussion von komplexen Zusammenhängen (wie sie hier vorliegen) nicht zu überschätzen. Um die Wirkung von IR-aktiven Gasen auf die Atmosphäre und auf das Klima quantifizieren zu können, ist also eine Gleichung, die Strahlungstransportgleichung für die Atmosphäre zu lösen. Auch das bekommt heute ein handelsüblicher PC mit (zumindest für den Hausgebrauch) hinreichender Genauigkeit in wenigen Minuten hin.
Das Ergebnis ist, dass sich eine Atmosphäre mit ‚Treibhausgasen‘ wie eine wärmende Decke um die Erde legt. Den Hauptteil der Erwärmung trägt dabei der Wasserdampf; CO2 ist nur mit wenigen Graden beteiligt. Und eine weitere Verdopplung der CO2-Konzentration würde nur zu einer Erhöhung der Temperatur von um oder unter 1 °C führen. Alles überhaupt nicht katastrophal. So weit, so gut. Und physikalisch korrekt!
Hier beginnen dann aber die eigentlichen Probleme der Klimawissenschaft. Jetzt stellt sich nämlich die Frage, wie wird das Klimasystem auf Temperaturerhöhungen reagieren. Werden sie abgeschwächt oder verstärkt, wird es stürmischer, nasser oder trockener? Wird das Wetter extremer? Wie reagieren die Meeresströmungen? Da ist – gelinde gesagt – alles noch ziemlich unsicher. Aber darüber lohnt die Diskussion!
Halten wir uns also vorerst lieber an die Erfahrungen aus der Vergangenheit. Es gab Zeiten mit viel mehr (und auch mit etwas weniger) CO2 und es gab Zeiten da war es viel wärmer oder kälter als heute: Katastrophal war das, nach allem was wir wissen, nur während der verschiedenen Eiszeiten.
Eine Atmosphäre ohne Abstrahlung ist eine ideale Isolation. Das ist so als wenn Ihre Bettdecke an der Aussenseite keine Energie mehr loswerden kann.
Die Diskussion lohnt sich erst, wenn man von den richtigen Annahmen ausgeht.
Empfehle als Lektüre diesen Beitrag:
https://www.eike-klima-energie.eu/2016/10/28/die-trickserei-bei-festlegung-des-strahlungsantriebs-von-37-wm%c2%b2-an-der-tropopause/#comment-122171
mfg Werner
eine Atmosphäre, die Energie nicht abgeben kann, könnte auch keine Energie aufnehmen. Sie hätte dann auch heute noch genau dieselbe Energie, die ihr der Herrgott am ersten Schöpfungstag mitgegeben hat. Die reale Atmosphäre verhält sich allerdings doch etwas anders: Sie könnte auch ohne IR-aktive Bestandteile über Wärmeleitung und Konvektion (fühlbar, latent) Energie aufnehmen und wieder abgeben.
Am Wärmehaushalt und der Temperatur der Atmosphäre ist also immer auch die Erdoberfläche beteiligt. Dass sie ohne IR-Moleküle kälter wäre, ist deshalb keine Annahme, sondern Ergebniss der Berechnung des Gesamtsystems Atmosphäre/Erdoberfläche.
Eine Atmosphäre ohne Abstrahlung wäre im Übrigen auch keine ideale Isolation, sie wäre genau das Gegenteil: Würde sie nicht abstrahlen dann würde sie auch nicht absorbieren: sie wäre für Wärmestrahlung vollständig transparent.
Fake News
„eine Atmosphäre, die Energie nicht abgeben kann, könnte auch keine Energie aufnehmen.“
Das ist falsch. Sie kennen doch die Wärmeleitung und Konvektion?!
Ich bleibe dabei, die Atmosphäre kann außer durch Strahlung an der Aussenseite (TOA Top of Atmosphere) keine Energie über Wärmeleitung und auch nicht durch Konvektion zum Vakuum abgeben. Ohne die Möglichkeit von Wärmeverlust an der Oberseite durch Strahlung ist die Atmosphäre ein idealer Isolator für Wärmeverlust der Oberfläche über die Atmosphäre und isoliert sich selbst.
Hinzu kommt, dass die Deckschicht der Ozeane von etwa 10 m Dicke eine hoehere Waermekapazitaet hat als die gesamte Atmosphaere.
Dass die Atmosphaere nur ueber die Emission von infraroter Strahlung in den Weltraum Energie abgeben kann, ist nicht korrekt. Im Falle naechtlicher Temperaturinversionen, die insbesondere ueber Landmassen entstehen, sind die Waermefluesse abwaerts gerichtet, was bedeutet, dass den Landmassen Energie zugefuehrt wird. Ein kleiner Anteil der von den Landmassen emittieren Strahlung geht im atmosphaerischen Fenster direkt in den Weltraum.
Sie sagen: „Dass die Atmosphaere nur ueber die Emission von infraroter Strahlung in den Weltraum Energie abgeben kann, ist nicht korrekt.“
Sie haben natuerlich recht. Im Falle das die Atmosphäre weder durch Konvektion, Latente Wärme oder Strahlung Energie an der TOA verlieren kann und damit ein perfekter Isolator für das Erdsysteme ist, kann die Atmosphäre noch durch Verlust von Materie Energie verlieren.
In Ihrem Beispiel, wo die Atmosphäre die Energie wieder an die Oberfläche abgibt, wird diese dann vond er Oberfläche abgestrahlt. Insofern widerlegen sie meine Aussage aber nicht, sondern sie zeigen nur den anderweitige Weg auf, auf dem die Erde Energie in Richtung Weltraum loswird, wenn die Atmosphäre nicht strahlen kann.
Diesen Weg habe ich aber nicht ausgeschlossen.
genau so ist das!
das haben Sie mit „Diskussionen, die es laut Lindzen eigentlich nicht geben sollte“ sehr schön formuliert! Wer so einen Satz wie der von Ihnen zitierte von sich gibt, will verhindern, dass es über seine Aussagen eine Diskussion gibt. Dazu kommt dann noch der Passus „… enthält nichts, was im Geringsten kontrovers ist …“ – eine klare Aussage gegen Wissenschaftlichkeit und pro „Glauben“!
MfG
The unit for describing energy flows is watts per square meter. The energy budget of
this system involves the absorption and reemission of about 200 watts per square meter. Doubling CO2 involves a 2% perturbation to this budget. So do minor changes in clouds and other features, and such changes are common. The Earth receives about 340 watts per square meter from the sun, but about 140 watts per square meter is simply reflected back to space, by both the Earth’s surface and, more importantly, by clouds. This leaves about 200 watts per square meter that the Earth would have to emit in order to establish balance. The sun radiates in the visible portion of the radiation spectrum because its temperature is about 6000K. ‘K’ refers to Kelvins, which are simply degrees Centigrade plus 273. Zero K is the lowest possible temperature (−273◦C). Temperature determines the spectrum of the emitted radiation. If the Earth had no atmosphere at all (but for purposes of argument still was reflecting 140 watts per square meter), it would have to radiate at a temperature of about 255K, and, at this temperature, the radiation is mostly in the infrared.
Lindzen (2019), „Climate sensitivity“:
2. The Atmospheric Greenhouse Effect
Let us assume for the moment that the earth has no atmosphere, and that the surface is non-reflecting. What would the temperature of the surface be? Incoming radiation would be about 341 Watts per square meter. In order for the earth to balance this, it would have to have a temperature given by the expression sigma T^4 where sigma, the Planck constant, is 5.67×10^(−8) W⋅m^(−2)⋅K^(−4). Interestingly, this leads to a temperature of about 278.5 K or 5.5 C, at which temperature, the Planck function for the spectral distribution of radiation tells us that the radiation emitted by the earth is primarily in the infrared portion on of the spectrum. This is only about 10 C less than today’s 288 K. If we allow for a surface reflectivity of 0.1, then the incoming radiation is reduced to about 307 Watts per square meter, and we get a temperature of about 271 K or -2 C. This is still only 17 C less than today’s mean temperature of 288 K.
The common claim that the earth would be 33 C less than today’s temperature comes from including the reflectivity of clouds, which brings the reflectivity to about 0.3. This reduces the incoming radiation to 240 Watts per square meter and leads to a temperature of 255 K. We will ignore the implausibility of an atmosphere-free earth having clouds. Of course, even in this simple situation, the surface temperature will vary with latitude, but for convenience of presentation we will assume that the temperature represents some sort of average.
Da man hier keine griechischen Buchstaben und Exponenten schreiben kann, habe ich mich der dann ueblichen Schreibweise bedient.
Man sollte beide Texte miteinander vergleichen. Dass Lindzen dann auch noch die Stefanschen Konstante mit der Planckschen Konstanten verwechselt, laesst nur einen Schluss zu. Er weiss nicht mehr, was er schreibt.
Die andere Welt ist die belebte Welt, die Biosphäre der Landmassen, der Ozeane und der Atmosphäre. Dieses System weist eine nichtlineare, chaotische Struktur auf, die nur mehr mit Wahrscheinlichkeiten der Quantenphysik zu beschreiben ist und unerwartete Brüche und Verzweigungen aufweist, die nicht mehr zu prognostizieren sind. (Beispiel: Der mikroskopisch kleine Coronavirus hat die Welt plötzlich aus den Angeln gehoben und sämtliche Prognosen zu nichte gemacht).
Daher sollte alle Kraft auf die Untersuchung der deterministisch bestimmten linearen Vorgänge (kosmische, solare, terrestrische) im Klimasystem gelegt werden, da hier tatsächlich die Aussicht besteht, einigermaßen zutreffende Prognosen über die zukünftige Klimaentwicklung treffen zu können.
Der Versuch, die nichtlinearen, chaotischen Strukturen der Biosphäre zu prognostizieren wird hingegen wegen der nicht überschaubaren Rück- und Gegenkoppelungen und der vielen, teilweise unbekannten Parameter scheitern.
Das kann ich so nicht unterschreiben, ich sehe in diesen Ausführungen Widersprüche zu den Hauptsätzen der Thermodynamik.
Im Einzelnen wie folgt:
Die Verdunstung aus den Ozeanen schafft Wasserdampf in die Atmosphäre – und Wasserdampf absorbiert und emittiert sehr stark Strahlung im Infrarot. Das meinen wir, wenn wir Wasserdampf ein Treibhausgas nennen. Der Wasserdampf behindert ganz wesentlich Infrarotstrahlung, die die Erdoberfläche verlassen will
Diese Aussage macht nur Sinn in Bezug auf die Aggregatszustände flüssig/fest da H2O nur in dieser Form reflektierend wirkt. Den vertikalen Luftaustausch können Wolken jedoch nicht verhindern. Eine erwärmende „Gegenstrahlung“ lässt auch mittels H2O nicht plausibel begründen.
Bei einer bestimmten Höhe existiert dort derart wenig Wasserdampf, dass nun die Strahlung ab dieser Höhe in den Weltraum „entkommen“ kann. An diesem Höhenniveau (um die 5 km) muss die Temperatur etwa 255K betragen, um die Energie der Einstrahlung auszugleichen. Aber weil die Konvektion – die Wärmeleitung – dazu führt, dass die Temperatur mit wachsender Höhe abnimmt, , muss jetzt die Erdoberfläche wärmer als 255K sein. Es stellte sich heraus, dass sie etwa 288K betragen muß (was die Durchschnittstemperatur der Erdoberfläche ist). Das ist es, was man jetzt als Treibhauseffekt bezeichnet.
Puh, jetzt wird es kompliziert…. Also, die Abstrahlungshöhe richtet sich nach dem Druckgradienten dadurch dass bei geringem Gasdruck die Stoßdeaktivierung nicht mehr stattfindet und ir-anregbare Gase ihre Energie abstrahlen können. Mit „wenig Wasserdampf“ hat das m.E. nichts zu tun.
Die Konvektion führt nicht dazu dass die Temperatur mit steigender Höhe abnimmt sondern der fallende Luftdruck (Barometrische Höhenformel). Dadurch begründet sich auch die Erdbodentemperatur, da umgekehrt bei steigendem Druck die Temperatur zunimmt. Das ist kein Treibhauseffekt sondern ein „Atmosphäreneffekt“. Der Erdboden ist nicht 288 K warm weil es oben 255 K kalt ist und die „Treibhausgase“ die Wärme dazwischen „stauen“.
Das Hinzufügen von anderen Treibhausgasen (wie Kohlendioxid) hebt diese Emissionshöhe weiter an und wegen der konvektiven Vermischung wird dieser neue Level kälter sein. Das wiederum verringert den abgegebenen InfrarotStrahlungsfluss und – um das Gleichgewicht wieder zu erreichen – würde sich die Atmosphäre erwärmen müssen.
Jetzt wird es vollkommen unplausibel, denn die Emissionshöhe hängt einzig von der Dichte ab, so dass ein paar ppm CO2 diese Emissionshöhe nicht anheben können! Zudem ist die Aussage dass es dadurch am Erdboden wärmer werden muss ein Verstoß gegen den 2 HS.
Dieser Zustand (oben kälter, unten wärmer) ist statistisch unwahrscheinlicher als der Zustand vor der Temperaturspreizung, also eine Verringerung der Entropie; was aber nach 2 HS nur durch Zufuhr externer Arbeit möglich ist. Und diese Zufuhr ist nirgends zu sehen bzw. wird nicht begründet.
Fazit: das hier beschriebene Klimasystem steht im Widerspruch zu den Hauptsätzen der Thermodynamik. Ein irgendwie geartetes „Forcing“ der Erdtemperatur durch ir-anregbare Gase ist physikalisch nicht plausibel zu begründen.
So ist das!
Lindzen behauptet:
Das Einheitsmaß für die Beschreibung von Energieflüssen ist Watt pro Quadratmeter. Der Energievorrat dieses Systems enthält die Absorption und die Rückstrahlung von etwa 200 Watt pro Quadratmeter. Eine Verdopplung des CO2 hat eine Störung von 2 % dieses Energiebudgets zur Folge. Das bewirken auch kleinere Veränderungen in der Bewölkung oder andere Merkmale, und solche Veränderungen gibt es häufig. Die Erde empfängt etwa 340 Watt pro Quadratmeter von der Sonne, aber ca. 140 Watt pro Quadratmeter werden einfach in den Weltraum zurück reflektiert – sowohl von der Erdoberfläche, aber wichtiger noch, durch die Wolken. Damit bleiben etwa 200 Watt pro Quadratmeter, die von der Erde emittiert werden müssen um ein Gleichgewicht herzustellen. Die Sonne strahlt im sichtbaren Teil ihres Strahlungsspektrums, weil ihre Temperatur ca. 6000K beträgt. „K“ bezieht sich auf Kelvin-Grade, die einfach Celsiusgrade plus 273 sind. Null K ist die niedrigste mögliche Temperatur (-273 oC). Die Temperatur bestimmt das Spektrum der emittierten Strahlung. Wenn die Erde gar keine Atmosphäre hätte (aber für die hier angewendete Beweisführung nach wie vor 140 Watt pro Quadratmeter reflektieren würde), dann müsste sie (den verbleibenden Überschuss) mit einer Temperatur von etwa 255K (minus 18 oC) abstrahlen, und bei dieser Temperatur wäre ihre Strahlung überwiegend im Infrarot.
Daran ist alles falsch. Die planetare Albedo im solaren Bereich des Systems Erde-Atmosphaere betraegt A = 0,30 +/- 0,02 (nicht 140/340 = 0,41, wie von Lindzen behauptet). Das bedeutet, dass das System Erde-Atmosphaere
R_S = (1 – A) S/4 = 0,7 x 340 W/m^2 = 238 W/m^2
im globalen Mittel an solarer Strahlung absorbiert, wobei S = 1360 W/m^2 die sog. Solarkonstante ist. Von diesen 238 W/m^2 entfallen auf die Atmosphaere etwa 82 W/m^2, so dass die Wasser- und Landmassen nahe der Erdoberflaeche nur etwa 156 W/m^2 im globalen Mittel absorbieren. Im Energieschema von Trenberth et al. (2009), was in dem Beitrag von Herrn Furrer (https://www.eike-klima-energie.eu/2020/05/20/klima-und-scheinwissenschaft-teil-3/) enthalten ist, sind es etwa 78 W/m^2) im globalen Mittel, so dass die von den Wasser- und Landmassen nahe der Erdoeberflaeche absorbierte solare Strahlung im globalen Mittel etwa 161 W/m^2 betraegt. (Trenberth et al. haben auch mit einer Solarkonstanten von S = 1365 W/m^2 gearbeitet.)
Im Falle einer Erde ohne Atmosphaere wuerden bei gleicher planetaren Albedo von A = 0,41, wie von Lindzen behauptet, wuerde die effektive Strahlungstemperatur T_e bei einem planetaren (relativen) Emissionsvermoegen von E = 1 etwa T_e = 244 K betragen; der Wert von etwa T_e = 255K ergibt sich nur mit A = 0,30.
Wenn aber die planetare Albedo des Systems Erde-Atmosphaere im wesentlichen von den Wolken bewirkt wird, dann kann A niemals gleich 0,30 oder gar 0,41 sein, denn eine Erde ohne Atmosphaere kennt keine Wolken.
Nimmt man eine planetare Albedo von A = 0,15 an, wie sie in der Fussnote 1 der Stellungnahme der Deutschen Meteorologischen Gesellschaft zu den Grundlagen des Treibhauseffektes erwaehnt wird, dann ergaebe sich fuer E = 1 effektive Strahlungstemperatur von etwa T_e = 267 K.
Die effektive Strahlungstemperatur einer Erde ohne Atmosphaere ist jedoch keine reale Temperatur. Sie ist nur ein Synonym fuer die emittierte infrarote Strahlung. Sie waere nur dann als reale Oberflaechentemperatur einer Erde ohne Atmosphaere akzeptabel, wenn die Oberflaechentemperatur gleichverteilt waere, was eine Gleichverteilung der solaren Einstrahlung voraussetzen wuerde. Spaetestens seit der Arbeit von Wiener (1879), „Ueber die Stärke der Bestrahlung der Erde durch die Sonne in den verschiedenen Breiten und Jahreszeiten“, ist jedoch bekannt, dass eine Gleichverteilung der solaren Einstrahlung nicht existiert. Von daher ist die effektive Strahlungstemperatur einer Erde ohne Atmosphaere eine sinnloser Wert.
Das Konzept der effektiven Strahlungstemperatur wurde urspruenglich nur auf Sterne wie unsere Sonne angewendet (siehe z.B. Chandrasekhar, 1950/1960, „Radiative Transfer“). Es auf Planeten und deren natuerliche Satelliten anzuwenden, ist barer Unsinn.
Wendet man das Konzept der effektiven Strahlungstemperatur auf unsere Sonne an und beruekcsichtigt, dass der Radius der Sonne etwa 696000 km und der mittlere Abstand der Erde vom Sonnenzentrum 149,6 Millionen km betraet, dann erhaelt man fuer die von der Sonne emittierte Strahlung einen Wert von 6,3 x 10^7 W/m^2. Nimmt man nun einen schwarzen Strahler an, denn ergaebe sich eine Temperatur fuer die „Oberflaeche“ der Sonne von 5770 K, wobei diese „Oberflaeche“ etwa 500 km dick ist.
Die von Lindzen erwaehnten 2 %, die eine Verdopplung der CO2-Konzentration ergeben soll, bestehen nur in seiner Phantasie, denn das wuerde voraussetzen, dass die vom IPCC propagierte Berechnung des anthropogenen radiativen Forcing von (siehe Ramaswamy et al., 2001)
RF = 5,35 W/m^2 x ln {[CO2](t)/[CO2](t=1750)} = 5,35 W/m^2 x ln 2 = 3,71 W/m^2
korrekt waere, was nicht der Fall ist. Nimmt man dann noch an, dass die angebliche Erhoehung der global gemittelten Oberflaechentemperatur DT proportional zu RF ist (siehe Ramaswamy, et al., 2001) d.h.
DT = C_S x RF
dann ergaebe sich mit dem Klimansensitivitaetsparameter von C_S = 0,8 K m^2/W, wie er im WBGU-Sondergutachten von 2009 erwaehnt wird, eine Temperaturerhoehung von
DT = 0,8 K m^2/W x 3,71 W/m^2 = 3,0 K
Fuer das Jahr 2015 ergibt sich nach der obigen Formel ein anthropogenes radiatives Forcing von etwa RF = 1,94 W/m^2. Nimmt man wieder C_S = 0,8 K m^2/W an, so betraegt die Erhoehung dre Oberflaechentemperatur
DT = 1,6 K
Danach waere also schon bei der Unterzeichnung der Pariser Klimavereinbarung das darin verankerte 1,5-Grad-Ziel ueberschritten gewesen. Zieht man den Wert C_S = 0,75 K m^2/W von Hansen et al. (2011) heran, so waere das 1,5-Grad-Ziel im Jahr 2017 ueberschritten worden.
Fazit: Dieser Formelmechanismus des IPCC, der auch von Lindzen verwendet wird, dient nur dem Klimaklamauk, eine besondere Form der Esoterik.
In Lindzens GWPF-Lecture von 2018 sind noch mehr absurde Behauptungen zu finden.
Zum Schluss noch eine Anmerkung: Lindzens Aussage, „Die Erde empfängt etwa 340 Watt pro Quadratmeter von der Sonne“, beinhaltet, dass das globale Mittel der solaren Einstrahlung gleich S/4 ist. Herr Weber hat hier in mehr als 20 Beitraegen behauptet, dass dieses Ergebnis falsch ist. Es waere angebracht, dass EIKE zu Webers absurden Beitraegen endlich Stellung bezieht.
Mal falsch, mal richtig. Wie es halt so passt.
Wir lassen Ihre unterhaltsamen Beiträge zu, sofern sie zur Sache beitragen und sachlich bleiben. Jedoch nur wenn Sie sich in Zukunft deutlich kürzer fassen.
Herr Weber zeigt einen Fehler auf der in der Klimadiskussion bei der Anwendung von den Einheiten der Leistung, Energie und Strahlstärken gemacht wird.
Wenn man Energie meint, und die Verteilung von Energie über ein Fläche benutzt man Joule pro Quadratmeter. Wenn man Energie über eine Zeit verteilt, dann benutzt man die Angabe der Leistung.
Wenn sie eine Strahlstärke (Leistung pro Flaeche) mitteln wollen, können sie nicht Leistung und Fläche mitteln, da die Strahlstärke flächenunabhängig ist.
Die solar eingetragene Energiemenge kann ganz leicht mit S, der Hemisphäre als Fläche und jedweder Zeit berechnet werden.
S/4 ist ein Spezialfall, der bei der Erde im Sommer und Winter nicht mal nach 24 Stunden stimmt, da die Erdachse schief gestellt ist und die bestrahlte Fläche kleiner als die gesamte Oberfläche ist.
S/4 mit Albedo ist ein theoretisches Konstrukt der Abstrahltemperatur und gilt nur pro Erdumdrehung, ohne Achsneigung. Mit den klimatischen Temperaturen hat das aber nichts zu tun.
Ein nicht rotierender Körper ermöglicht als einfaches Modell die Mittelung der wirksamen Strahlstärke, und das ohne das man über Achsneigung oder Zeit der Rotation nachdenken muss. Das hat Herr Weber erkannt und sie wissen das auch, weil sie ohne dieses Model ihre eigenen Rechnungen nicht durchführen können, bei denen sie die direkt wirksame Strahlstärke verwenden müssen.
Übrigens ist die Albedowirkung auch auf eine Hemisphäre begrenzt, weil die Nachtseite kein Sonnenlicht reflektieren kann.
Der Kürze wegen hier erstmal Schluss.
mfg Werner
Zum Vergleich: Webers verwendeter Ansatz des lokalen Strahlungsgleichgewichtes liefert 156 K. Das ist ein Unterschied von 52 K.
Ihre Behauptung,
„Das hat Herr Weber erkannt und sie wissen das auch, weil sie ohne dieses Modell ihre eigenen Rechnungen nicht durchführen können, bei denen sie die direkt wirksame Strahlstärke verwenden müssen.“
ist vollkommen falsch. Weber hat ueberhaupt nichts erkannt. Was er ueber die Arbeiten von Gerlich & Tscheuschner (2009) sowie Kramm et al. (2017) verbreitet hat, sind Falschaussagen, die sich an Hand dieser Arbeiten auch als Falschaussagen nachweisen lassen. Offensichtlich kennt Weber weder die Arbeit von Gerlich & Tscheuschner (2009) noch die Arbeit von Kramm et al. (2017).
Fuer Sie zum Mitschreiben:
Kramm et al. (2017) haben sowohl die Schraegstellung der Rotationsachsen von Erde und Mond als auch den variierenden Abstaende von Erde und Mond vom Sonnenzentrum beruecksichtigt. Dabei wurde die „planetary and lunar ephemeris“ DE 430 des Jet Propulsion Laboratory des California Institute of Technology verwendet, die z.Zt. die genauesten Berechnungen fuer unser Sonnensystem erlaubt. Die Berechnungen der lokalen Oberflaechentemperaturen wurden mit einem Zeitschritt von 600 s durchgefuehrt. Kramm et al. (2017) haben zudem die Ergebnisse von Gerlich & Tscheuschner (2009) fuer eine rotierenden Mond und eine rotierende Erde ohne Atmosphaere „en passant“ bestaetigt.
Aber wenn Weber behaupten darf, dass Kramm et al. (2017) mit einem globalen Modell gearbeitet haetten und auch noch das Ergebnis der Verteilung der solaren Einstrahlung als Eingangsdaten bezeichnet, dann kann man sich nur noch wundern, was jemand ungestraft an Unsinn auf der Webseite von EIKE verbreiten darf.
Ihre Behauptung, dass S/4 fuer die globale gemittelte solare Einstrahlung ein Spezialfall sei, ist ebenfalls falsch. Seit der Arbeit von Wiener (1879), „Ueber die Stärke der Bestrahlung der Erde durch die Sonne in den verschiedenen Breiten und Jahreszeiten“, ist die Verteilung der Einstrahlung bekannt. Da Wiener nicht den genauen Wert der Solarkonstante kannte, hat er seine Ergebnisse in normierter Form dargelegt. Seine Ergebnisse stimmen mit den heutigen Werten der Einstrahlung an Abhaengigkeit vom Breitenkreis und der Jahreszeit sehr gut ueberein, wenn man die heutigen Werte ebenfalls in normierter Form darstellt. Und die globale Mittelung dieser normierten Verteilung liefert im Falle von Wiener (1879) einen Wert von 0,24959. Auch Ihnen sollte bekannt sein, dass das praktisch dem wert von 1/4 entspricht.
das alles sind bekannte Wiederholungen mit denen sie sich gegen Herrn Weber stellen. Auf meine Klarstellung der physikalischen Groessen und Einheiten gehen sie nicht ein. Der Punkt ist, das die Strahlstaerke flaechenunabhaengig ist und sie damit die Flaeche angeben muessen, die Energie erhaelt. Im Falle der Erde ist das jederzeit die Hemisphaere.
Ihre Arbeit ueber den Mond ist bemerkenswert. Sie wissen aber auch der Unterschied zwischen dem Mond und der Erde ist gravierend, weil der Mond fast keine Atmosphaere hat.
Da wir hier ueber atmosphaerische Prozesse reden, ist der Mond als Beispiel nur fuer die theoretische Abstrahltemperatur interessant. Das koennen sie mit S/4 und der Albedo halten wie ein Dachdecker und die Erde schief. Eine sinnvolle Aussage was und wie die Atmosphaere funktioniert, werden sie dadurch nicht treffen koennen.
Ist es ihnen gelungen Anhand der Einstrahlleistung die Temperaturen der Venus zu erklaeren? Ich denke mal nicht. Da tun sie sich genauso schwer wie Herr Weber.
Die Oberflaechentemperatur ergibt sich naemlich aus der Lapsrate und der Dicke der Atmosphaere und der Graukoerperabstrahltemperatur.
Die Venusbedingungen erlauben mehr oder weniger ein direkte Anwendung dieser Rechnung, da sowohl Einstrahlung und Abstrahlung bei der Venus nur die Atmosphaere betrifft.
Die Erde ist bei erster Ansicht komplizierter, da sich aufgrund der Durchlaessigkeit der Atmosphaere und ueber die Erdkruemmung ein medialer Temperaturgradient einstellt, die Ozeane eine Rolle spielen, die Erdachsschiefstellung Jahreszeiten erzeugt, die Abstrahlung von der Oberflaeche und aus der Atmosphaere kommt usw usf.
Durch eine einzige Vereinfachung kann man aber auch fuer die Erde den Treibhauseffekt erklaeren. Nehmen sie Eiskristalle, Aerosole, Staub, Ozon, Wasserdampf und CO2 und erklaeren sie die Atmoshaere zum Grauen, wenn nicht sogar fast schwarzen Strahler.
Nunmehr strahlt die Atmosphaere an der Oberkante die von der Sonne erhaltene Energie wieder ab. Das ist dann von der gesamten aeusseren Flaeche S/4 abzueglich atmpshaerischer Albedo,
Wie entwickeln sich in dem Falle die Temperaturen darunter? Richtig, die Lapserate stellt einen Gradienten zur Oberflaeche dar, die damit eine Temperatur erhaelt, die zusaetzliche nur noch von der Dicke der Atmosphaere abhaengt.
Und jetzt die Erkenntnis, Stickstoff, Sauerstoff und auch das bisschen CO2 tragen alle zu der Dicke der Atmosphaere bei. Die Strahlung stellt sich entsprechend den Temperaturen ein aber nicht anders herum. Und ob das durch voellstaendige Absorption erfolgt oder nur in Einzelbaendern ist voellig egal.
Machen sie ein Probe und fragen sie sich einfach mal was passiert wenn sich die Masse der Atmosphaere verdoppelt. Ich sage die Temperaturen nehmen zu, die Oberflaechentemperaturen steigen entsprechend dem Anstieg der Dicke der Atmoshaere.
In Naeherung entspicht, das einer einfachen Isolation und die Lapserate ist der Waermedurchgang. Der isolierte Koerper ist der der Erde, von dem wir wissen das er im Inneren noch viel waermer ist und sich auch durch einen Temperaturgradienten auszeichnet.
Die solare Energie spielt am Ende keine Rolle, da sie am Oberrand der Atmosphaere wieder verlustig geht.
mfg Werner
Ich habe schon Popkorn Nachschub besorgt; soll es etwa bei crickets bleiben?
Mäßigen Sie Ihre Wortwahl, dann wird auch freigeschaltet.
„Eine Verdopplung der Kohlendioxid-Konzentration wird als eine Wirkung (forcing) von ca. 3,7 Watt pro Quadratmeter abgeschätzt, was etwas weniger als 2% der ankommenden Nettoenergie ist. Zahlreiche Faktoren wie Wolkenbedeckung und –höhe, Schneebedeckung und Ozeanströmungen verursachen gewöhnlich Änderungen von vergleichbarer Größenordnung.“
Die m.E. notwendige Relativierung steckt im 2. Satz. Soweit ich mich erinnere, hat Lindzen aufgrund seiner Untersuchungen Klimasensitivitäten von nur etwa 0,6 Grad C publiziert. Lindzen folgt demnach zwar der gängigen These der Strahlungsheizung durch CO2 in der Atmosphäre, die aber etwa halbiert (und nicht wie beim IPCC verstärkt) wird durch diverse weitere atmosphärische Einflüsse auf die Temperatur. Soweit ich mich erinnere, hat Lindzen dies aus der Reaktion der Atmosphäre auf einen Vulkanausbruch abgeleitet.
Im uebrigen hat Lindzen (wie viele andere) bis heute nicht begriffen, dass das Konzept der Klimasensitivitaet auf dem globalen Energiebilanzmodell von Schneider & Mass (1975) beruht, was aus physikalischen udn mathematischen Gruenden zu verwerfen ist. Ein falsches Modell liefert im allgemeinen auch nur falsche Ergebnisse.
Im uebrigen sollten Sie sich einmal die Abbildung 1 bei Gervais (2016) anschauen, die auch unter https://www.eike-klima-energie.eu/2019/08/21/was-sie-schon-immer-ueber-co2-wissen-wollen-teil-4-die-klimasensitivitaet/ zu finden ist. Danach ist der Wert von Lindzen & Choi (2011) das Minimum in einer Vielzahl von Ergebnissen zur „equilibrium climate sensitivity“ (ECS). Das Ergebnis von Lewis & Curry (2014) betraegt z.B. das Dreifache des Wertes von Lindzen & Choi (2011).
Hm, ja – für mathematisch/naturwissenschaftlich geprägte Menschen.
Für Ideologen/Politiker kann ein falsches Modell aber durchaus „richtige“ Ergebnisse liefern…
Jetzt nutzen die gleichen „Ideologen/Politiker“ die Pseudo-Corona-Krise für weitere immense Bereicherung, bei gleichzeitiger Schädigung des Volkes. Den Überblick zu behalten, wie groß der Schaden in Euro/US-Dollar ist, wird immer schwerer. Immer mehr Ebenen kommen hinzu. Sicher ist, er steigt und steigt und steigt und steigt.
Die Klimasensitivität sehe ich übereinstimmend als Überlagerung der spektroskopischen Erwärmung mit Rückkopplungseffekten, wobei man bei letzteren offenbar im Dunkeln tappt. Die Trägheit der Ozeane führt dann zu einer verzögerten Einstellung des Gleichgewichts (ECS).
Wo ist hier Platz für ein falsches Energiebilanzmodell? Hat uns nicht kürzlich ein Kommentator mitgeteilt, dass CO2-Messungen (z.B. Mauna Loa) mittels CO2-Absorptionsmessungen durchgeführt werden? Die spektroskopischen Zusammenhänge also alles in allem als erwiesen gelten können? (ich erahne die Proteste!)
Klimaveränderungen zu berechnen ist in etwa so:
Man kann recht einfach berechnen, wann und wo eine Kugel aufschlägt, die man vom Eiffelturm wirft.
Dazu braucht man Höhe, Gewicht, Kraft des Wurfes und evtl. ein paar zusätzliche Dinge wie Luftdruck, Windgeschwindigkeit usw..
Wenn man versucht, das Klima für Jahre voraus zu berechnen, ist das, als wenn man ausrechnen will, wo und wann ein Papierflugzeug den Boden berührt, dass man vom Eiffelturm wirft.
Das schafft kein Computer der Welt!
Weil man nie genug Parameter berechen kann!
„Der fundamentale Punkt war immer folgender.
Klimawandel wird durch hunderte von Faktoren oder Variablen bestimmt; und allein der bloße Gedanke, dass wir den Klimawandel vorhersagbar managen können, indem wir den einzigen politisch gewählten Faktor CO2 verstehen und manipulieren, ist so abwegig wie nur irgendwas; das ist wissenschaftlicher Unsinn.“
Philip Stott (Emeritierter Professor für Biogeographie an der School of Oriental and African Studies der University of London und ehemaliger Herausgeber des Journal of Biogeography.)
Ein sehr eleganter Vergleich von Ihnen. Danke! Habe ihn sogleich – hoffentlich mit Ihrer Erlaubnis – zum Verteilen an meine FfF-Hüpfer abgespeichert.
Ich erlaube mir trotzdem folgenden Vorschlag, und will dabei nicht korrigierend erscheinen: Ich würde „Computer“ durch „Mensch“, „man“ durch „der“ und „eingeben“ zumindest durch „erfassen, messen, verifizieren und eingeben“ ersetzen.
(Mein Laptop lacht mittlerweile über Big Blue, welcher 1996 erstmals den Schachweltmeister schlug, programmiert von Menschen! Sowohl Big Blue, als auch meinem LT ist es nämlich vollkommen egal, was am Ende herauskommt!)
natürlich dürfen Sie meinen Kommentar verwenden, gern auch angepasst.
Freut mich immer, wenn etwas von mir positiv ankommt…?
Ob das irgend eine Auswirkung auf die kontroversen Diskussionen hier hat? (Diskussionen, die es laut Lindzen eigentlich nicht geben sollte 🙂 )