Die Computer-Programmierung von Prof. Neil Ferguson wurde von führenden Persönlichkeiten als „vollkommen unzuverlässig“ verspottet. Sie warnten, dass man „darauf niemals sein Leben wetten sollte“.
Das Modell, welches man als ursächlich für die Kehrtwende der Regierung beschrieben hat, einen landesweiten Lockdown zu verhängen, ist ein „mit Fehlern behaftetes Durcheinander, das mehr wie eine Schüssel mit Fadennudeln als wie ein sorgfältig konstruiertes Stück Programmierung daherkommt“. Das sagt David Richards, Mitbegründer des britischen Datentechnologie-Unternehmens WANdisco. „In unserer kommerziellen Realität würden wir jeden feuern, der so etwas wie das hier entwickelt, und jeder Geschäftszweig, der sich darauf zur Software-Entwicklung stützt, würde wahrscheinlich bald den Bach runtergehen“.
Diese Kommentare werden wahrscheinlich eine neue Diskussionswelle auslösen, ob UK recht daran getan hat, der Öffentlichkeit einen Lockdown überzustülpen, zumal andere wissenschaftliche Modelle gezeigt hatten, dass die Bevölkerung bereits substantiell eine Herden-Immunität entwickelt und das Virus schon früher als gedacht in UK um sich gegriffen hat. Außerdem waren sich die Wissenschaftler uneinig hinsichtlich der Todesrate von Covid-19, was zu völlig verschiedenen Modellergebnissen geführt hat.
Bisher jedoch hat man nur das Modell von Imperial herangezogen, welches die Todesrate höher als alle anderen Modelle simulierte und prophezeite, dass ohne Lockdown in UK 510.000 Menschen sterben würden.
Dieses Modell hat zu einer dramatischen Änderung der Politik seitens der Regierung Anlass gegeben, in deren Zuge Unternehmen, Schulen und Restaurants sofort zu schließen hatten. Die Bank of England hat prophezeit, dass es ein Jahr dauern könnte, bis die Wirtschaft wieder ein normales Maß erreicht hätte nach der schlimmsten Rezession seit über drei Jahrhunderten.
Das Imperial-Modell funktioniert mittels Simulation von Übertragungswegen, Bevölkerungszahl, sozialen Netzwerken und Gesundheitsvorsorge, um zu berechnen, wie sich das Coronavirus ausbreiten könnte. Allerdings gab es von Anfang an Fragen dergestalt, ob das Modell genau genug ist, nachdem die Forscher den Code dahinter veröffentlicht hatten. Dieser nahm in originaler Form „tausende Zeilen“ in Anspruch, entwickelt während mehr als 13 Jahren.
In seiner ursprünglichen Form behaupteten die Entwickler, dass der Code unlesbar sei, wobei einige Passagen den Eindruck erweckten, dass sie „mittels Übersetzungsmaschine aus der alten Programmiersprache Fortran übersetzt worden seien“, sagt der Amerkianer John Carmack, der den Code vor der Veröffentlichung zu bereinigen half. Aber die Probleme scheinen viel gravierender zu sein als nur auf schlampiger Programmierung zu beruhen.
Viele haben behauptet, dass es fast unmöglich ist, aus den gleichen Daten die gleichen Ergebnisse mittels der gleichen Programmierung zu erhalten. Darüber berichteten Wissenschaftler der University of Edinburgh. Sie sagten, dass sie zu unterschiedlichen Ergebnissen gekommen seien, wenn sie verschiedene Maschinen benutzten, und manchmal sogar auch bei Nutzung der gleichen Maschinen*.
[*Es ist nicht sicher, ob man den englischen Begriff „Machine“ hier mit Maschine übersetzen kann. Es könnten auch Computer gemeint sein. Ist ein Computer eine Maschine? Anm. d. Übers.]
„Es scheint einen Fehler zu geben, entweder bei der Erstellung oder bei der Wiederbenutzung der Netzwerk-Datei. Falls wir zwei vollständig identische Läufe starten mit dem einzigen Unterschied, dass der zweite Lauf mit dem network file des ersten Laufes starten sollte, bekommen wir ziemlich unterschiedliche Ergebnisse“.
Nach einer Diskussion mit den Entwicklern wurde später eine Korrektur eingebracht. Damit war aber nur einer der Fehler aus einer ganzen Reihe derselben bereinigt, die innerhalb des Systems entdeckt worden waren. Die Entwickler erklärten dies damit, dass das Modell „stochastisch“ sei und man „diverse Läufe starten muss mit unterschiedlichen Eingaben, um das mittlere Verhalten zu ergründen“.
Dies hat jedoch Spezialisten veranlasst, augenblicklich Fragen zu stellen, sagen diese doch, dass „Modelle in der Lage sein müssen, den grundlegenden wissenschaftlichen Test zu bestehen, dem zufolge die gleichen Ergebnisse herauskommen beim gleichen ursprünglichen Satz von Parametern … anderenfalls gibt es einfach keine Möglichkeit zu erkennen, ob sie zuverlässig sind“.
Dies kommt inmitten einer breiter angelegten Debatte, ob sich die Regierung nicht besser auf mehrere Modelle hätte stützen sollen, bevor sie politische Entscheidungen traf.
Sir Nigel Shadbolt, Rektor am Jesus College, schrieb für telegraph.co.uk , dass „es unglaublich stark ist, eine diverse Anzahl von Modellen zu haben, vor allem solche, die Politiker in die Lage versetzen, Prophezeiungen unter verschiedenen Hypothesen zu erkunden“.
Wie beim Imperial Code läuft ein rivalisierendes, von Prof. Sunetra Gupta an der Oxford University entwickeltes Modell auf der Grundlage einer Dreiteilung, wobei die Bevölkerung unterteilt wird in jene, die empfänglich, infiziert und gemeldet sind. Allerdings gab es sehr unterschiedliche Hypothesen. Gupta ging von einer Todesrate von 0,1% von allen mit dem Coronavirus infizierten Menschen aus, Ferguson gab diese Zahl mit 0,9% an.
Dies führte zu einer dramatischen Kehrtwende der Regierungspolitik. Wollte man zunächst eine „Herden-Immunität“ zu erreichen versuchen, wandte man sich dann einem vollständigen Lockdown zu. Experten blieben verblüfft zurück, warum die Regierung sämtliche anderen Modelle verworfen hatte.
„Wir hätten uns auf etwas gefasst machen können, falls die Wettervorhersage nur auf einem einzigen Modelllauf von einem einzigen Modell beruhen würde und keinen Regenschirm dabei hatten, als der Wolkenbruch kam“, sagte Michael Bonsall, Professor für mathematische Biologie an der Oxford University.
Bedenken waren vor allem hinsichtlich des Modells von Ferguson aufgekommen, und Konstantin Boudnik von WANdisco sagte, dass seine Zurückverfolgung der Art und Weise der Modellierung nicht geeignet sei, Vertrauen zu erwecken.
Anfang der 2000er Jahre prophezeite Fergusons Modell fälschlich bis zu 136.000 Todesfälle durch den Rinderwahnsinn, 200 Millionen durch Vogelgrippe und 65.000 durch Schweinepest.
„Die Fakten aus jener Zeit sind einfach eine weitere Bestätigung dafür, dass deren Verfahren der Modellierung bis ins Mark falsch war“, sagt Dr. Boudnik. „Wir wissen nicht, ob das gleiche Modell/das gleiche Programm verwendet worden war, aber wir sehen klar, dass ihr Verfahren damals nicht effizient genug war und sich seitdem mit Sicherheit nicht verbessert hat“.
Ein Sprecher des Covid-19-Teams am Imperial College sagte: „Die UK-Regierung hat sich niemals auf nur ein einziges Modell bei der Entscheidungsfindung verlassen. Wie wiederholt festgestellt, beruhten die Entscheidungen hinsichtlich des Lockdowns auf einem Konsens der wissenschaftlichen Standpunkte, darunter mehrere Modellierungs-Studien von verschiedenen akademischen Gruppen“.
Link: GWPF Rundbrief
Übersetzt von Chris Frey EIKE
HINWEIS: Dieser Beitrag ist aus einem Rundbrief der GWPF übersetzt. Daher kann kein Link zum Original angegeben werden. Für alle, welche die Übersetzung überprüfen wollen, hier das Original als DOC:
4_ModellingFalse
Neulich bei Lanz, die Virologin Prof. Dr. Melanie Brinkmann auf die Frage, warum denn Deutschland so glimpflich davon kam was die niedrigen Corona Infizierten Zahlen und erst Recht die Todesopferzahlen im Vergleich zum übrigen Europa angeht (Gedächtnisprotokoll):“Deutschland hat das gut gemanged, und wir hatten einfach Glück!“ Also mit anderen Worten, hätten wir Pech gehabt, hätten wir Zustände gehabt „wie im alten Rom“ oder Spanien. Nun ja, wenn sie Virologin ist, muss es ja stimmen…MP Stephan Weil, der auch zugegegen war, hat ebenfalls son paar Klopper rausgehauen. Von dieser Sendung „Lanz“ kann ich jedenfalls nur abraten!
2. Wer sich bei derart komplexen Fragen wie nach einem totalen Stillstand der Wirtschaft auf Computersimulationen verlässt, begeht einen elementaren Fehler. Da kann man alles hinein programmieren und die Parameter (je mehr je besser!) beliebig setzen (wie bei den „Klimamodellen“) und heraus kommt dann irgendwas. Ich nenne derlei „fantasy computing“.
n der Umgangssprache der Programmierer ist „engine“ der Ausdruck für das eigentliche Rechenprogramm. Es ist inzwischen üblich sich eine „engine“ zu kaufen und mit einem Pre- und Post-Programm ein „eigenes“ Programm“ auf die Beine zu stellen. Angefangen hat das (zumindest in dem Bereich, in dem ich mich auskenne, Bruchmechanik, Bauingenieur) mit „offenen Uni-Programmen“, die dann einfach aufgemotzt wurden….
Bei Wiki:
Eine Spiel-Engine (englisch game engine [ˈgeɪmˌɛndʒɪn]) ist ein spezielles Framework für Computerspiele, das den Spielverlauf steuert und für die visuelle Darstellung des Spielablaufes verantwortlich ist. In der Regel werden derartige Plattformen auch als Entwicklungsumgebung genutzt und bringen dafür auch die nötigen Werkzeuge mit. ……
noch einen schönen Sonntag.
Falko Ducia
Das Problem der Reproduzierbarkeit ist bei Verwendung von stochastischen Methoden (sogenannte „Monte Carlo-Rechnungen“) schon etwas knifflig, aber normalerweise sollte selbst dies möglich sein, wenn die Initialisierung des Zufallszahlengenerators (der sogenannte „seed“) identisch gewählt wird. Trotzdem kann es manchmal immer noch zu Problemen bei sogenannter „Vektorisierung“ oder „Parallelisierung“ kommen, aber selbst dies ist oft mit entsprechenden Compileroptionen („niedrige Optimierung“) in den Griff zu bekommen.
Aus meiner Erfahrung gibt es aber zwei andere wichtige Gründe, die die Ursachen für Probleme darstellen können:
1) manche Programme haben noch uraltes Erbgut, das auf Maschinen entwickelt wurde, deren Betriebsysteme gar nicht mehr existieren (Vax, Risc, Alpha, Sparc, ..). Damals haben zum Teil auch Compiler andere Toleranzen gehabt und so noch Minibugs toleriert (und repariert) die heute nicht mehr zulässig sind.
2) im akademischen Bereich werden vielfach Teilprojekte als Bachelor- und Masterarbeiten an junge (unerfahrene) Studenten gegeben, die nur den von ihnen bearbeiteten Teil kennen und naturgemäß nicht den Gesamtüberblick über den Code besitzen. Da können sich Fehler und Inkohärenzen einschleichen, die vielfach von den Betreuern nicht bemerkt werden. Insbesondere, wenn das „erwartete Ergebnis“ auch wirklich heraus kommt, ist man dann schnell geneigt, nicht nochmals genauer abzuchecken, ob denn wirklich alles korrekt ist.
Mein Fazit: bevor man einem Computercode wirklich Vertrauen schenken kann, brauch man wirklich Benchmarking, Subversioncontrol und code qualification. Das ist aber ziemlich aufwendig und unbequem für die „Forscherseele“, weil man dann halt nicht mehr schnell am Programm herumschrauben kann, um etwas Neues auszuprobieren.
1.
„… können sich Fehler und Inkohärenzen einschleichen, die vielfach von den Betreuern nicht bemerkt werden. Insbesondere, wenn das „erwartete Ergebnis“ auch wirklich heraus kommt…“
Diese Sätze beschreiben das Grundsatzdilemma von Smulationen: Man programmiert IMMER in die Richtung, um ein erwartetes Ergebnis, dass man z.B. aus Prozesse der Natur kennt, zu erreichen. In dieser einer Parameterzusammensetzung stimmt die Berechnung mit dem naturerbebnis nicht zufällig überein, sondern man strebt es bewußt an. Mit anderer Parameterzusammensetzung gehen aber Rechenergebnisse und natürliche Prozesse auseinander. Weil die Simulation nun mal nicht das exakte Spiegelbild der Natur ist, sondern eine Nachahmung von bekannten Teilergebnissen von natürlichen Protessen mit anderen Mitteln. Unabhängig ob der Student oder Sie persönlich den Code schreiben.
2.
„…Zufallszahlengenerators…“ Sie wissen natürlich, dass die Ausbreitung von Covid, wie auch zig weitere Prozesse von Zufall abhängen. Zufall kan man nicht berechnen! daher kann man nie das Wetter exakt vorherberechnen, wie zig andere Dinge auch. „…die Initialisierung des Zufallszahlengenerators (der sogenannte „seed“) identisch gewählt wird…“ bedeutet nur, dass der Zufall praktisch ausgeschaltet wird, um Berechnungswege nachzuvollziehen. Zufälle in der Natur kann man nicht initialisieren…. und damit nicht simulieren!
1) Die Grundproblematik der Simulation ist mir auch bekannt und eigentlich kann man ein Modell (wie eine Theorie) nie „validieren“ sondern (gemäß Popper) nur falsifizieren. Allerdings wird einem Modell, das eine gewisse „predictive power“ besitzt, also Vorhersagen treffen konnte, die so auch (nach der Vorhersage!) nachgemessen und bestätigt werden konnten, meist schon ein gewisses Vertrauen entgegengebracht.
Mir ging es wirklich darum, wie es es möglich ist, dass solche Simulationsprogramme explizite Fehler enthalten (Überschreiben von anderen Variablen, Fehlinterpretationen von Daten etc), so dass man letztendlich noch nicht einmal das berechnet, was man eigentlich vorgibt zu machen. Und das ist eine Gefahr, weil dann in extremen Extrapolationen das Ergebnis möglicherweise überhaupt nichts mehr mit den eigentlichen Modellannahmen zu tun hat. Man produziert einfach nur „garbage“.
2) Modelle wollen nicht den Zufall simulieren. Selbst bei Rechnungen zum sogenannten „deterministic chaos“ geht es nicht um die Beschreibung einzelner Ereignisse sondern von mittleren Häufigkeiten, Varianz der Abweichung etc.
Zufallszahlen im Computer (die eigentlich keine wirklichen Zufallszahlen sind, sondern nur stark dekorrelierte Zahlenfolgen – nach einer bestimmten deterministischen Prozedur berechnet und mit einem speziellen Anfangswert, dem „seed“) sind eine Methode um in komplexen Systemen mit sehr vielen Freiheitsgraden möglichst viel Information zu sammeln. Programme, die diese Methode benutzen sind insofern schlechter auf Fehler zu testen, weil zwei Rechnungen mit dem identischen Programm bei identischen Parametern aber unterschiedlichen Zufallszahlen leider nicht identische Ergebnisse liefern. Und dadurch wird es halt schweren festzustellen, ob man doch bei einer kleinen Programmänderung einen Fehler eingebaut hat. Mir ging es hier wiederum nur um Frage, ob man sich wirklich sicher sein kann, dass mit der Modifikation eines Computerprogramms sich nicht doch ein Fehler in den Code eingeschlichen hat, so dass selbst „Standardrechnungen“ auf einmal nicht mehr korrekte Ergebnisse produzieren
mir ist sehr wohl bewußt, dass es (bei sehr komplexer Software) Bugs gibt, die praktisch nicht auffindbar sind, da sie die Berechnung nur bei ganz bestimmten, extrem seltenen Konstellationen auftreten. Es gibt sicher mehr Fehelrquellen durch die von Ihnen beschriebenen Erstellung des Codes.
Hier geht es aber erstmal darum, dass jemand eine Simulation geschrieben hat, für die es erstmal keine wissenschaftlich fundierte Grundlage gibt, und zweitens, niemand niemals Simulationsergebnise mit gemessene Echtergebnisse verglichen hat, um die Bugs und falsche Ansätze zu berichtigen. So etwas bietet nur einer an, der sich für Gott mit der Glaskugel hält oder ein bewusster Betrüger. Das Ergebnis kann man auch würfeln, wäre ehrlicher.
Und meine Hauptkritik bestätigen Sie ja mit „… insofern schlechter auf Fehler zu testen, weil zwei Rechnungen mit dem identischen Programm bei identischen Parametern aber unterschiedlichen Zufallszahlen leider nicht identische Ergebnisse liefern….“
Natürlich, wenn der Zufall dabei ist, kommt nie das Gleiche raus. Ich möchte auf den typischen Zufallsgenerator der Natur hinweisen:
„Kräht der Hahn auf den Mist, ändert sich das Wetter oder bleibt wie es ist“
Ich möchte damit sagen, dass man auch die absolut fehlerfreie Simulation nur wie den Wetterbericht betrachten kann (wahrscheinlich sind die Wettersimulationen die meistgetestete Computerprogramme), es haut meistens einigermaßen hin ohne Garantie, kann aber auch ganz anders kommen.
Daher ist der Kernpunkt der Diskussion für mich nicht, wieviel Fehler die zitierte Simulation hat, sondern das grundsätzlich keine Simulation die absolute Wahrheit betrachtet werden kann, und somit als wichtigste Entscheidungsgrundlage untauglich ist. Man neigt hierzulande die Genauigkeit, mit denen Computer x-stellige Zahlen multiplizieren, auf die Ergebnisse von Simulationen zu übertragen (sofern sie bugfrei sind).
Das ist aber grundverkehrt.
(angeblich zum Schutz der obduzierenden Ärzte…)
Erst nach Protesten wurde der Eintrag auf der Internetseite entfernt. An der falschen Haltung änderte sich aber wenig.
Autopsien würden ja die Regierung überführen, etwas, was in diesem verkommenen Land nicht passieren dar. Das wäre ja ein Affront gegen die Esel in Berlin. Die dürfen auf gar keinen Fall deren Gesicht, Visage verlieren.
Die Tatsache, dass der Rechtsmediziner aus Hamburg von der Regierung & Co. ignoriert wird, spricht doch Bände.
„Machine“ ist eine durchgängige Bezeichnung von Rechenanlagen. Ein von mir entwickeltes System mit der Wirkung eines Computers im Computer für sehr anspruchsvolle technologische Prozesse wird gängig als „Virtual Machine“ bezeichnet.
Der Artikel erschließt auf beeindruckende Weise, was von Computer-Modellierern zu erwarten ist. Eine Programmierung, die nicht einem Erfolgstest unterzogen werden kann, darf mit aller Wahrscheinlichkeit als fehlerhaft eingestuft werden. Bei Programmen, die nicht das Ziel von Projektionen haben (oft fälschlich mit Prognosen verwechselt), stellen sich die Fehler unmittelbar bei der Kopplung des Rechnersystems mit einer realen Maschine dar. Das Gesamtsystem verhält sich nicht so, wie es sich der Programmierer vorgestellt hat. Eben, weil die menschliche Programmierung naturgemäß fehlerhaft ist. Und es sind in der Regel viele „Bugs“, die auf Flüchtigkeitsfehlern oder falschen Denkansätzen beruhen und mit großem Zeitaufwand beseitigt werden müssen.
Nur sehr von sich überzeugte Menschen, wie sie auch in Potsdam beim PIK zu finden sind, verkaufen ihre Projektionen als fehlerfrei. Das liegt einfach daran, dass Programmierungsfehler erst in 50 oder 100 Jahren nachweisbar sind, bei Epidemien-Auswirkungen überhaupt nicht. Nach Lesen des Artikels, gibt es hier tatsächlich Parallelen zwischen Pandemie und Klima.
danke für Ihre Bemerkungen. Ich möchte diese mit einem wichtigen „philosophischen“ Punkt ergänzen.
Jede Sumulation oder Programm spiegelt nicht die Wirklichkeit wider, sondern die Sicht des Programmierers über die Wirklichkeit. Neben alle Bugs und falsche Ansätze kommt also eine persönliche , also äußerst subjektive Denkweise dazu. Eine Simulation ist also grundsätzlich nur dann im gewissen Rahmen aussageklräftig, wenn sie eine „Schnittstelle zu Wirklichkeit“ hat, also wenn berechnete Daten permanent mit wirklichen Daten verglichen und der Code demsntsprechend permanent abgapasst wird.
Es ist mir nicht ganz begreiflich, warum man so fest an die Unfehlbarkeit von Simulationen, sofern sie Bug-frei sind, glaubt. Und dann noch als Grundlage für solche weitreichende Entscheidungen nimmt. Unbegreiflich!!!
Eine schöne Analogie dazu: Wenn man eine schöne Frau auf einem analogen Foto von den vor Photoshop-Zeiten betrachtet, weiß man, sie war wirklich so schön war. Wenn man eine schöne Frau auf einem gemalten Bild sieht, kann man davon ausgehen, vielleicht war das Modell wirklich so schön oder die Schönheit entstammt ausschlichlich der Phantasie des Künstlers, oder beides zusammen.
Fast alle begreifen Ergebnisse von Programmen wie analoge Bilder und wollen nicht sehen, dass es in Wirklichkeit das durch die Werktzeuge von Photoshop entstellte Wirklichkeit nach dem subjektiven Geschmack des Programmierers ist.