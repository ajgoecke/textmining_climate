Vor vier Wochen überprüfte TWTW [The Week That Was] die neue Studie von Richard Lindzen, in dem er zusammenfasst, was wir mit hinreichender Sicherheit wissen, was wir vermuten und von was wir wissen, was in Bezug auf Klimawandel, Treibhauseffekt, Temperaturtrends, Klimamodellierung, Ozeanchemie und Meeresspiegelanstieg falsch ist. Grundlegende Erkenntnisse sind u. A. Folgende (mit Ergänzungen in Fettschrift):
1) Das Klimasystem befindet sich niemals im Gleichgewicht.
2) Im Mittelpunkt des Systems befinden sich zwei turbulente Flüssigkeiten, die miteinander in Wechselwirkung stehen und von der Sonne ungleichmäßig erwärmt werden, was zu einem Wärmetransport vom Äquator zu den Polen (meridional) führt, wodurch wiederum Ozeanzyklen entstehen, die bis zu 1000 Jahre dauern können.
3) Die beiden wichtigsten Akteure im Treibhauseffekt sind Wasserdampf und Wolken, die nicht vollständig verstanden und nicht stabil sind.
4) Ein lebenswichtiger Bestandteil der Atmosphäre ist Wasser in seinen flüssigen, festen und gasförmigen Phasen, und die Veränderungen der Phasen haben immense dynamische Konsequenzen.
5) Die Verdoppelung des Kohlendioxids (CO2) verursacht eine 2%ige Beeinflussung des normalen Energieflusses in das System hinein und aus dem System heraus, was der Beeinflussung durch Veränderungen der Wolken und anderer natürlicher Merkmale ähnelt. Angesichts der turbulenten Bedingungen des Klimasystems, die nicht durch Gleichungen beschrieben werden können, hat das Konzept der Forcings wenig Bedeutung.
6) Die Temperaturen in den Tropen waren extrem stabil. Es sind die Temperaturunterschiede zwischen den Tropen und den Polarregionen, die extrem wichtig sind. Berechnungen wie die der globalen Durchschnittstemperatur lassen diesen bedeutenden Unterschied weitgehend außer Acht.
Vor drei Wochen nutzte TWTW die Arbeit von William van Wijngaarden und William Happer (W & H), um zusammenzufassen, was wir mit hinreichender Sicherheit wissen, was wir vermuten und was wir über den Treibhauseffekt nicht wissen. Beide Herren sind Experten auf dem Gebiet der Atom-, Molekular- und optischen Physik (AMO), was bei weitem keine einfache Physik ist, aber notwendig, um zu verstehen, wie Treibhausgase die Abstrahlung von Energie von der Oberfläche in den Weltraum beeinflussen (verzögern) – d.h. um die Prozesse zu verstehen, durch die die Erde jede Nacht Wärme verliert.
1) Es gibt kein allgemeines Verständnis des Treibhauseffektes, das ausreicht, um elegante Gleichungen zu entwickeln.
2) Die optische Tiefe oder optische Dicke der Atmosphäre (Transparenz) ändert sich mit zunehmender Höhe. Die Tiefe wird in Form eines natürlichen Logarithmus gemessen und bezieht sich in diesem Fall auf die Entfernung, die ein Photon einer bestimmten Frequenz zurücklegen kann, bevor es von einem geeigneten Molekül absorbiert wird (eines, das Photonen dieser Frequenz absorbiert und wieder aussendet).
3) Im Gegensatz zu anderen natürlichen Treibhausgasen ist Wasserdampf, das dominierende Treibhausgas, nicht gleichmäßig in der Atmosphäre verteilt. [SEPP-Kommentar: Die Variabilität des Wasserdampfs während des Tages und die Bildung von Wolken aus H2O usw. machen theoretische Berechnungen der „Klima“-Dynamik mit beliebigen Werten unmöglich. Da Wasserdampf stündlich, täglich und jahreszeitlich und je nach Ort variiert, erkannte der Charney-Bericht, dass eine vernünftige Berechnung unmöglich ist. Also ging er den falschen Weg, H2O zu ignorieren, und nahm einen CO2-Wert an, um kam dann mit einem „Feedback“-Konzept aufzuwarten, um zu versuchen, H2O zu berücksichtigen. Das Konzept scheitert und ist nach wie vor eine schlechte Modellierungspraxis, die Misserfolge produziert.
4) Es besteht eine logarithmische Beziehung zwischen Treibhausgasen und Temperatur.
5) „Sättigung“ bedeutet, dass das Hinzufügen weiterer Moleküle nur eine geringe Veränderung der Erdstrahlung in den Weltraum bewirkt. Der sehr enge Bereich, in dem Methan (CH4) Photonen absorbieren und emittieren kann, liegt unterhalb der Tropopause, wo die Atmosphäre dick ist, bereits durch Wasserdampf (H2O), das dominierende Treibhausgas, gesättigt. Daher hat die Zufuhr von Methan wenig Einfluss auf die Temperaturen.
6) Die hochauflösende Transmissions-Molekular-Absorptions-Datenbank (HITRAN) ist eine Zusammenstellung spektroskopischer Parameter zur Berechnung und Simulation der Transmission und Emission von Licht in der Atmosphäre. Das Datenbankprojekt wurde Ende der 1960er Jahre von den Air Force Cambridge Research Laboratories (AFCRL) als Antwort auf den Bedarf an detaillierten Kenntnissen über die Infraroteigenschaften der Atmosphäre gestartet. „Das Ziel von HITRAN ist ein in sich konsistenter Satz von Parametern. Gleichzeitig besteht jedoch die Anforderung, möglichst genaue Parameter zu archivieren. Es muss betont werden, dass die in HITRAN vorhandenen Parameter eine Mischung aus berechneten und experimentellen Parametern sind. Häufig sind die experimentell ermittelten Werte genauer als die berechneten und umgekehrt“. Aus dieser Datenbank berechnen W & H, dass eine Verdoppelung des CO2 die Temperaturen um nicht mehr als 1,5 ⁰ C erhöht – eine obere Grenze auf der Grundlage dessen, was wir heute über die Atmosphäre wissen. Diese Obergrenze ähnelt der, die von den erfahrenen Apollo-Wissenschaftlern und Ingenieuren des TRCS-Teams berechnet wurde. Die Apollo-Veteranen nahmen an den Meilensteinen des Raumfahrtprogramms der NASA teil.
Vor zwei Wochen überprüfte TWTW die Probleme mit Modellen, wie sie vom etablierten japanischen Klimamodellierer Mototaka Nakamura diskutiert und in einer neuen Studie von Ross McKitrick und John Christy aufgezeigt wurden. Zuvor fasste Tony Thomas einige der von Nakamura erkannten Hauptprobleme zusammen:
● Unwissenheit über groß- und kleinräumige Ozeandynamik.
● Völliges Fehlen aussagekräftiger Darstellungen von Aerosolveränderungen, die Wolken erzeugen.
● Mangelndes Verständnis der Treiber von Eis-Albedo-Rückkopplungen (Reflektivität): „Ohne eine einigermaßen genaue Darstellung ist es unmöglich, aussagekräftige Vorhersagen über Klimaschwankungen und -änderungen in den mittleren und hohen Breiten und damit auf dem gesamten Planeten zu machen.
● Unfähigkeit, mit Wasserdampfelementen umzugehen.
● Willkürliche „Anpassungen“ (Fudges) von Schlüsselparametern, die nicht verstanden werden.
Ferner lehnt Nakamura das Konzept des IPCC ab, dass der Einfluss des CO2 hinzufügenden Menschen durch Modelle vorhergesagt werden kann. Er stellt fest:
„Ich möchte auf eine einfache Tatsache hinweisen, dass es unmöglich ist, auch nur den Sinn oder die Richtung der Veränderung eines Systems korrekt vorherzusagen, wenn das Vorhersageinstrument wichtige nichtlineare Prozesse, insbesondere Rückkopplungen, die im tatsächlichen System vorhanden sind, vermisst und/oder grob verzerrt“. Kurz gesagt, wir wissen einfach nicht, ob die Rückkopplungen Störungen des Systems verstärken oder dämpfen werden“.
Nakamura stellt weiter fest, dass zwei Hauptprobleme in den Modellen die Ozeanströmungen (Ozeanzirkulation) und das Wasser in der Atmosphäre sind. Beide Probleme werden von Lindzen benannt.
McKitrick und Christy testeten die aus 38 neuen CMIP6-Modellen berechneten Werte für den Zeitraum von 1979 bis 2014 mit Datensätzen aus drei verschiedenen Arten von Beobachtungen. 1) Vier verschiedene Sätze von Radiosonden- (oder Sonden-)Daten von Wetterballonen. 2) Vier verschiedene Datensätze von Mikrowellensensoren an Bord polumlaufender Satelliten, die die Intensität der Mikrowellenemissionen von Luftsauerstoff messen, welche direkt proportional zur Temperatur sind. 3) Vier verschiedene Datensätze, bekannt als Reanalysen, zwei aus Europa, einer aus Japan und einer aus den USA, NASA.
Die 12 Datensätze decken 35 Jahre ab und sind seit mindestens 5 Jahren verfügbar. Die drei verschiedenen Arten von Datensätzen aus Beobachtungen sind sowohl für den globalen als auch für den tropischen Raum eng gruppiert. Bei den meisten Modellen liegt der Mittelwert für Satellitenbeobachtungen unter dem unteren Teil des 95%-Konfidenzintervalls für dieses Modell, was darauf hinweist, dass das Modell keine atmosphärischen Temperaturtrends abschätzen kann. Wie Nakamura geschrieben hat, haben die globalen Klimamodelle keinerlei Vorhersagewert. Das IPCC und seine Anhänger haben sich eindeutig von der wissenschaftlichen Methode verabschiedet und in die Welt der wilden Spekulation eingestiegen.
TWTW vom 27. Juni knüpfte an Roy Spencers Vergleich der Ergebnisse von 13 CMIP6-Modellen mit Temperaturdaten an der Erdoberfläche an, die weit weniger umfassend als atmosphärische Daten sind und viel mehr menschlichen und natürlichen Einflüssen unterliegen als atmosphärische Daten. Die CMIP6-Modelle erzeugen deutlich mehr Erwärmung als die älteren CMIP5-Modelle und überschätzen die HadCRUT4-Oberflächenerwärmung tendenziell um etwa 50%. Daher haben die getesteten Klimamodelle wenig oder keine nachgewiesene Fähigkeit zur Vorhersage von Temperaturtrends jetzt oder in der Zukunft.
In der vergangenen Woche erörterte TWTW zwei Versuche, den Einfluss von Kohlendioxid zu verzerren: erstens die Veränderung der Ozeanchemie, die so genannte Versauerung der Ozeane, und zweitens den so genannten beschleunigten Anstieg des Meeresspiegels. Einige der fruchtbarsten Gebiete der Ozeane sind jene mit stärkerem Aufwallen, weil dabei Tiefenwasser an die Oberfläche kommt, welches Nährstoffe und vermehrt Kohlendioxid aus der Tiefe mitbringt. Dies wiederum fördern die Photosynthese, das pflanzliche und tierische Leben und den Beginn der marinen Nahrungskette. Wie Jim Steele beschrieb, erlebte der pazifische Nordwesten der USA in den Jahren 2006 bis 2008 einen besonders starken Auftrieb, der zu einem Absterben der Austern führte.
Das Absterben erfolgte nicht durch atmosphärisches CO2, sondern durch die Austernindustrie, die pazifische Austern aus Japan importierte, welche sich im Nordwestpazifik nicht entwickelt haben. Diese Austern geben ihre Eier einfach ins Wasser ab, weil sie sich in Gewässern ohne rasche Änderungen des pH-Wertes infolge Aufwallen entwickelt haben. So starben die Eier und Larven in den Jahren 2006-2008 bei starken Veränderungen des pH-Wertes ab. Umgekehrt „brüten“ die im pazifischen Nordwesten heimischen Olympia-Austern ihre Larven in ihren Schalen, wo der pH-Wert besser kontrolliert wird. Diese natürlichen Bedingungen, die das Absterben in den Jahren 2006-2008 verursachten, wurden durch die Handlungen der Austernzüchter im Nordwesten kompliziert.
Bei der Erörterung der sogenannten Beschleunigung des Meeresspiegelanstiegs erörterte TWTW die beobachtete Änderungsrate des Meeresspiegels am geologisch stabilen Newlyn, Cornwall, England, wie in einem Beitrag von E. Bradshaw im Journal of Marine Geodesy dargelegt. Es gibt zwei verschiedene Änderungsraten, eine hundertjährige Änderungsrate, gemessen mit Gezeitenmessern von 1,8 mm pro Jahr. Die zweite Änderungsrate beträgt 3,8 mm pro Jahr über einen Zeitraum von 11 Jahren (von 1993-2014), gemessen durch Satelliten. Man kann nicht sagen, dass die zweite Änderungsrate die richtige ist und dass sich der Anstieg des Meeresspiegels beschleunigt, denn während der hundertjährigen Aufzeichnung der Gezeitenpegel gab es mehrere andere Perioden mit ähnlichen Änderungsraten wie die des Satelliten. Daher ist die Behauptung, dass der Meeresspiegelanstieg zunimmt, eine übereilte Schlussfolgerung, die auf einem Wechsel der Instrumentierung beruht.
Link: https://wattsupwiththat.com/2020/08/10/weekly-climate-and-energy-news-roundup-419/
Übersetzt von Chris Frey EIKE
CO2 kühlt – eine Reduktion der Wirkung wäre „Erwärmung“…
Der Webersche Hemisphären-Ansatz ist falsch.
https://www.academia.edu/43740786/Kommentar_zu_WEBERS_Beitrag_Weitere_%C3%9Cberlegungen_zur_hemisph%C3%A4rischen_Herleitung_einer_globalen_Durchschnittstemperatur_
Heutzutage werden in den Medien wissenschaftliche Fakten zurechtgebogen, um irgendwelche obskuren Ziele zu verfolgen. Ich danke Herrn Kramm für die mühevolle Arbeit im Dienste der Wissenschaft.
Warum wollen Sie die hier dargestellten grundlegenden Erkenntnisse von Herrn Richard Lindzen mit dem hemisphärischen Ansatz von Herrn Weber auf eine Stufe stellen und damit die Aussagen von Herrn Lindzen abwerten??
Herr Lindzen vertritt leider auch den falschen Ansatz. Im physikalischen Sinne ist er ein Flacherdler!
Herr Weber hat das hier gut erklärt:
https://www.eike-klima-energie.eu/2020/07/27/umgebungsgleichung-stefan-boltzmann-gesetz-ungerechtfertigte-kritik-an-ulrich-o-weber-strahlungsgleichgewicht-mittelwertbildung/#comment-257512
Aber beide Ansätze sind nicht geeignet den THE zu erklären. Der Webersche Ansatz ist besser geeignet die Temperaturen der Erdoberfläche zu erklären.
Das ist nur eine Behauptung.
Eigentlich erwarte ich hier im Forum eine faktenorientierte Argumentation.
Ich verweise auf einen Artikel, der hier schon eimal erschienen ist.
Kritisches Hinterfragen des IPCC Basis Modell KT97, seines
atmosphärischen Treibhauseffektes, seiner Ableitung von CO2 mit
einem Strahlungsantrieb von 32 W/m² und seiner politischen
Dimension von A. Agerius, 2020 (Langversion)
Auf dieser Basis könnte man über das Thema diskutieren.
Fläche der Erde 6×10^6m = 36×10^12 m^2
×4 ×3 (gerundet für pi) = 500×10^12=5×10^14 m^2
das Ganze Mal die berühmten 32 W/m^2 gibt ungefähr 16×10^15 J/s. 1KT ist 4×10^12 J. Also meinte man Atombomben mit 1000KT. Richtig lustig wurde es am Stammtisch aber erst, als ich die Erwärmung der Atmosphäre für ein Jahr ausgerechnet hatte
Ergebnis oben mit den Sekunden eines Jahres multiplizieren (3×10^8),ergibt 5×10^23 K. Geteilt durch die Masse der Atmosphäre (5×10^18), geteilt durch die spezifische Wärmekapazität von Luft (750 J/k/K) ergibt eine jährliche Erwärmung von rund 135 K pro Jahr. Somit hatten alle verstanden, wieso für die Rechnungen so leistungsfähige Computer benötigt. Man muss unglaublich viel rechnen, um diesen Schwachsinn, der da angenommen wird, wieder einzufangen.
Man kann von meiner Rechnung halten, was man will. Wenn der Kentnnisstand der 8. Klasse ausreichend ist, hochkomplexe mathematische Spielereien als Spielereien zu enttarnen, dann sollten erst mal grundsätzliche Fragen gestellt werden. Beispielsweise handelt es sich bei dem ganzen Theater um eine Treibhaustheorie oder um eine Irrenhaustheorie.
“ Kritisches Hinterfragen …. “
KT97 ist mit seinem Modell (1/4 Verteilung, Gegenstrahlung 324 W/m², CO2 -Antrieb mit 32W/m²)
das IPCC Basis Modell. Es ist ein falsches, die Wirklichkeit nicht abbildendes Modell. Seine
Grundzüge sind im Kern die Vorlage für viele Computerklimamodelle. Bezieht man alle vom
Satelliten gemessenen Strahlungs- und Albedo Messwerte z.B. des ERBS Satelliten in einem
neuen Modell mit ein, braucht es zur Herleitung sämtlicher Größen keinen Treibhauseffekt von
33 K. Der vielfältige Einfluss der Sonne, insbesondere über den Sonnenwind und die dadurch
modulierte kosmische Höhen-Strahlung, die die Wolkenbildung steuern, sind neben inneren
Effekten wie AMO und PDO die naheliegendste Ursache der beobachteten Erwärmung. Die
anthropogene Klimakatastrophe wird im Computermodell erzeugt. Sie dient politisch, extremste
Forderungen zu legitimieren.