Diese Annahme wird auch bei den Klimamodellen verwendet. Neuere Studienergebnisse zu diesem Thema mit einem wesentlich geringeren Anstieg wurden vom IPCC ignoriert. Wie auch jüngst wieder durch PIK Professor Anders Levermann beim öffentlichen Fachgespräch im Umweltausschuss des Deutschen Bundestages am 28.11.18.
Doch auch Prof. Marotzke (MPI) gab kürzlich zu, dass die bisherige Annahme offenbar zu hoch ist. Dies wird durch den Vergleich des berechneten Temperaturanstieges mit der Realität der letzten 20 Jahre bestätigt. Bis heute konnte der angebliche CO2-Effekt auf die Globaltemperatur nicht durch historische Fakten belegt werden, sondern ist laut IPCC-Bericht eine Sache von „confidence“, d.h. von Glauben oder Vertrauen.
Die Milchmädchen-Rechnung, die Klima-Politiker, Journalisten und etliche Klimatologen nicht machen können oder wollen, lautet wie folgt:
Bis zur Verdoppelung des CO2-Gehaltes von heute 400 auf 800 ppm vergehen bei den aktuellen Emissionen mit einem Anstieg von ca. 2 ppm pro Jahr noch 200 Jahre. Das bedeutet real in 100 Jahren ca. + 1,5°C – genau das, was sich Politiker bei der Pariser Konferenz so vorgestellt haben, aber ohne kostspielige und risikoreiche CO2-Verminderungsmaßnahmen.
Das international anerkannte Modtran-Programm der Atmosphärenphysik zeigt zwischen 400 und 800 ppm einen Temperaturanstieg von nur 1,7°C (anstelle von den 3°C des IPCC) mit Berücksichtigung des Sättigungseffektes, der ansonsten oft ignoriert wird. Das wäre dann in 100 Jahren nur ein Anstieg um ca. +0,85°C.
Tatsächlich ist aber kein weiterer Anstieg der mittleren Globaltemperatur zu erwarten, sondern das Gegenteil: sie wird in den nächsten 400 Jahren aller Wahrscheinlichkeit nach wieder um ca. 1,5°C absinken – so, wie dies schon seit 8000 Jahren regelmäßig alle 1000 Jahre der Fall war, zuletzt in der mittelalterlichen Kaltzeit (auch „ Little Ice Age“ genannt). Dieser natürliche Klimazyklus basiert auf solaren und astrophysikalischen Faktoren und kann von Menschen nicht beeinflusst werden – genauso wenig wie andere Natur-Ereignisse wie Erdbeben und Vulkanausbrüche.
Stillschweigend vergessen bleibt dabei allerdings, die für Flora und Fauna unverzichtbare und konzentrationsabhängige Nahrungsversorgung mit ihrem signifikanten Einfluss auf das Wetter und dessen Statistik, das Klima! – Auch nicht berechenbar, oder?
Für die Saurier reicht es bereits nicht mehr und unterhalb 150 ppmv soll es auch für viele Andere Spezies knapp werden.
Der Treibhauseffekt ist sehr komplex. Die Einstrahlung und Reflexion ändert sich mit der Erdoberfläche, Wetter, Jahreszeit…Auch die Abstrahlung ist sehr komlex. In manchen Wellenlängenbereichen kann die Rückstrahlung ungehindert erfolgen. In anderen Wellenlängenbereichen wird die Rückstrahlung nach einigen m durch IR aktive Gase absorbiert. Die Wärme wird weitgehend über die Wasserverdunstung und Aufwinde abgeführt. Die eigentliche Abstrahlung erfolgt aus etwa 5500m Höhe wo die Atmosphäre weniger dicht ist. Eine Erhöhung des C02 Gehalts würde die Abstrahlungshöhe erhöhen und damit die Abstrahlungstemperatur (damit die Wellenlänge) reduzieren. Dies ist der Haupteffekt einer Erhöhung des CO2 Gehalts in der Atmosphäre.
Man könnte die Klimasensitivität analog der Reaktorphysik über eine Montecarlo Simulation mit Wirkquerschnitten der Moleküle für Strahlen, verschiedenen repräsentativen Oberflächen, Breitengraden, Wetter, Jahreszeiten, Schwankungen der Klimagasanteile, zusammen mit einer Berechnung des Wärmetransports vornehmen. Dies würde die einschlägigen wissenschaftlichen Institute Jahrzehnte beschäftigen.
Meine komplette Analyse findet man unter: https://holgernarrog.hpage.de/ Die Schwachstellen der Hypothese vom Klimawandel
Holger Narrog
1. enthält der ECS-Wert eine Wasserdampf-Rückkopplungs-Annahme, oder basiert er auf der Rückstrahlung des CO2 allein?
2. was ist TCR?
ECS ist der Wert der Temperaturerhöhung, der nach Erreichen eines neuen Gleichgewichtszustands nach Verdopplung der CO2-Konzentration erreicht wird. Dafür dürfte es mehrere hundert Jahre dauern.
Der TCR-Wert („Transient Climate Response“) ist die Temperaturerhöhung, die in dem Moment eingetreten ist, in dem die CO2-Verdopplung erreicht wird. Da dieser Zeitpunkt deutlich vor Erreichen des neuen Gleichgewichtszustands stattfindet, ist TCR immer kleiner als ECS.
Der Artikel von Dr. Koelle ist leider etwas missglückt. Der geplante zulässige Temperaturanstieg um 2 Grad (bzw. 1,5 Grad) bezieht sich nicht auf den Jetzt-Zustand bei ~400 ppm, sondern auf die „vorindustrielle“ Zeit (mit einem CO2-Anteil von 280 ppm), wobei verschleiert wird, dass dieser Referenzzeitpunkt mit dem Ende der Kleinen Eiszeit zusammenfällt. Damit wird suggeriert, dass für den Temperaturanstieg seitdem auschließlich der Mensch verantwortlich sei. Diese Annahme kann aber nur ein echter Klimaleugner treffen, der einen natürlichen Klimawandel ausschließt.
Die meisten (alle?) auf Messungen beruhende ECS-Abschätzungen (z.B. Lewis & Curry) gehen implizit von einem zu 100% anthropogenen Anteil aus, da man das anthropogene Signal nicht separieren kann, und führen daher eine Abschätzung nach oben aus.
Wenn man berücksichtigt, dass ein Teil des Temperaturanstiegs natürlichen Ursprungs ist, kommt man leicht auf ECS-Werte um oder unterhalb 1 Grad.
ECS heißt natürlich „Equilibrium Climate Sensitivity“.
im Wesentlichen halte ich Ihren Artikel für zutreffend, aber ich hoffe, Sie gestatten mir 2 Anmerkungen.
Sie schreiben von der mittelalterlichen Kaltzeit. Nun, das Mittelalter datiert etwa vom 8. bis zum 14. Jahrhundert. In dieser Zeit war es ungewöhnlich warm auf der Erde. Etwa so warm wie heute, kein Wunder wenn man die Solarzyklen beim Klima berücksichtigt. Die sogenannte “kleine Eiszeit“ begann ziemlich genau zum Ende des Mittelalters und dauerte bis ins 19. Jahrhundert. Eine mittelalterliche Kaltzeit, wie von Ihnen beschrieben hat es nie gegeben.
Im Artikel zeigen Sie weiterhin ein Diagramm zur durchschnittlichen Temperatur der IPCC Klimamodelle verglichen mit den real gemessenen Temperaturen. In gut 2 Wochen schreiben wir das Jahr 2019. Dann sollten die Diagramme der realen Temperaturen nicht mehr im Jahr 2014 enden. Die Erde hat sich weitergedreht und es hat in der Zwischenzeit jede Menge Bewegung bei der globalen Durchschnittstemperatur gegeben, sowohl nach oben wie nach unten. Das sollte man dann auch entsprechend darstellen um sich nicht dem Vorwurf der “Rosinenpickerei“ auszusetzen.
Das NOAA gibt einen Strahlungsantrieb der Treibhausgase seit 1750 von 3,062 W/m^2 an!
https://tinyurl.com/y94v9a2x
1°C Temperaturerhöhung entsprechen 3,7 W/m^2. Bei einer Klimasensitivität von 3°C erhält man:
dT = 3,062 / 3,7 x 3 = 2,5 °C !!! durch die Treibhausgase.
Ohne kühlende Luftverschmutzung natürlich. Die Luftverschmutzung möchte man in Zukunft ja auch nicht mehr haben.
Das ist die Temperaturerhöhung gemäß IPCC, die wir jetzt schon vorprogrammiert haben, bei einer Klimasensitivität von 3°C!
Alle Klimaziele sind damit schon jetzt obsolet!
https://tinyurl.com/y8q9ohlo
Und der Nachlauf der Temperatur im Klimasystem ist gering, beträgt vielleicht 0,1-0,2°C und die Treibhausgase sind nicht alleine verantwortlich.
Die Klimasensitivität liegt also bei 1°C und nicht bei 3°C.
Das alles streng unter den Annahmen des IPCC!
Wenn die ihre Annahmen nicht schnellstens ändern, fällt das selbst Laien auf. Man muss nur das CO2-Äquivalent zeigen und sagen dass hat sich schon nahezu verdoppelt, die Temperatur ist aber nur um ca. 1°C angestiegen und der Nachlauf ist gering!
Somit sind auch das 1,5°C, oder 2°C-Ziel obsolet, denn die müssten schon lägst erreicht sein bei 90% Erhöhung der Treibhausgaskonzentration, die wir jetzt schon haben!
Der Physiker stutzt sofort, wenn er auf der Ordinate „Grad C“ und nicht „Watt/m^2“ findet:
Rückstrahlung, die vom Wasser absorbiert und gewandelt wird, ändert zwar die Temperatur, Strahlung, die von der Flora absorbiert wird, oder die Eis schmilzt dagegen überhaupt nicht!
Doch es kommt noch viel schlimmer: CO2 tummelt sich bekanntlich überwiegend unterhalb der Wolken und anderer nicht sichtbarer Aerosole, die insgesamt ankommende Energie einsammeln und soviel Wärmestrahlung emittieren, dass die Temperatur in ihrer Umgebung den Erdball umspannend bis unter -50 Grad C absinkt. Da fehlt ihnen doch immer die Energie, die ihnen das CO2 jeweils stiehlt, oder?
Der Physiker erkennt: Hier sind Witzbolde am Werk.
https://de.scribd.com/document/372778420/Klimasensitivita-t-des-CO2-eine-Seifenblase
https://www.scribd.com/document/379087623/Das-CO2-ist-klimaneutral
Also Aufwärm- und Abkühlzeitkonstanten können beeinflußt werden, niemals aber kann eine Absoluttemperaturerhöhung entstehen. Echte Zusatzwärme benötigt gem. 1. HS der Wärmelehre zusätzliche Energie, und die kann CO2 nicht aus sich selbst heraus bei unveränderter Energiezufuhr durch die Sonne liefern. Und ein Einfluß auf die Albedo durch CO2 ist auch unbekannt.
Ganz abgesehen davon, daß eine Temperatur alleine kein Maß für die Wärme im Gesamtsystem ist.
Wer sich die Herleitung des „Treibhauseffektes“ durch Gegenstrahlung anschaut, stößt reihenweise auf falsche oder zumindest für das vorliegende System, Atmosphäre der Erde, nicht zutreffende Annahmen. Es ist daher überhaupt nicht verwunderlich, dass man noch keine experimentellen Verifikationen zu dieser abstrusen These gefunden hat.
Und das (Strahlungs-) Modtran-Programm kann bestenfalls eine Veränderung der Rückstrahlung zeigen. Und von wem ist das international anerkannt? Und vor allem wieso? Hier wäre zunächst mal zu zeigen, dass Gemessene, errechnete und dem Gegenstrahlungsmodell zugrunde liegende Leistungen/Fläche zusammen passen. Das wäre die erste grundlegende Verifikation des Modells. Wenn die erbracht wäre, würden man das an die ganz große Glocke hängen. Aber wie gesagt, gibt es noch gar keine Verifikation des Modells.
Die Veränderung des Strahlunsantriebs ist die Basis für Atmosphärenmodelle, die auch recht unterschiedliche Ergebnisse liefern.
Die Frage muss lauten: „Rechnet hier überhaupt jemand?“
Wir sollten uns langsam klar machen, dass es sich hier genauso wenig um Wissenschaft handelt, wie bei der „Gender-Wissenschaft“.
Man muss doch nichts Falsifizieren, dass nicht mal im Ansatz zeigen kann, dass es sinnvoll ist.
1. Es wurde niemals gemessen, sondern immer nur angenommen, es handelt sich also um eine hiypotetische Größe und keine physikalische Eigenschaft.
2. Man kann annehmen was man will, mich kann aber niemand überzeugen, dass wenn das CO2 wirklich einen Einfluß haben sollte, dann die Verdoppelung von 50% auf 100% einen ganz anderen Temperaturanstieg nach sich ziehen würde als die Verdopplung von 400 auf 800 ppm. Bemerkungen dazu wie „man geht von einem logarithmischen Verlauf aus“ bringt mich immer zur Erkenntnis, wenn man es nicht weiß und nicht messen kann, dann sollte „man nicht davon ausgehen“, sondern ehrlicherweise würfeln um zu einem Ergebnis zu kommen, oder es gleich sein lassen.
3. „Das international anerkannte Modtran-Programm der Atmosphärenphysik zeigt zwischen 400 und 800 ppm einen Temperaturanstieg von nur 1,7°C…“ bedeutet nur, dass dieser Zusammenhang im Code hinterlegt wurde. Genauso kann man, wie bekannt auch 3°C hinterlegen, aber auch 0°C oder gar eine Abkühlung. Wenn man die Mittel das PIK hat, kann man mit ausreichender Zeit und gute Software-Entwickler beliebige Sensivität „nachweisen“, sofern der Wert nicht aus komplett dem Ruder läuft im Sinne von 15°C. Ich habe genug mit Software-Entwicklung zu tun um zu wissen, dass das Ergebnis einer komplexen Software nur eine Bedingung erfüllen muß: Die Erwartung des Auftraggebers zu entsprechen. Dass in der Regel 1+1 im Computerprogramm =2 ergibt liegt nur daran, dass dieses Ergebnis in der Regel erwartet wird, daher wird es so programmiert.
Der dafür propagierten Ideologie wohnt bekanntlich das dogmatische Zurückweisen entgegenstehender Fakten inne, erst recht, wenn das der Sicherung eigener, lukrativer Positionen dient.
Die in der Wissenschaft übliche Falsifizierung durch Erbringen entsprechender Gegen-Beweise unterbleibt dabei geflissentlich. Sie wird durch eine Vielzahl gleichlautender „Expertisen“ ersetzt, in denen die Autoren sich gegenseitig die Richtigkeit ihrer Thesen bestätigen. The show must go on …
die Wissenschaft hat die „übliche Falsifizierung durch Erbringen entsprechender Gegen-Beweise“ schon lange absolviert. Hier ein paar Beispiele:
– Der vom IPCC 1995 gemachten Erklärung des Treibhauseffekts (Reduzierung des atmosphärischen Fensters – die einzige Erklärung bisher, die wissenschaftlich fundiert ist) wurde schon ein Jahr später mit Methoden der Ex-Physik der Zahn gezogen, als nachgewiesen wurde, dass die so mögliche Erwärmung maximal 0,3°C bei CO2-Verdopplung beträgt.
– Die Idee vom Verstärkungseffekt durch Wasserdampf (Verdreifachung der Wärmewirkung des CO2) wurde durch zigtausende Ballonmessungen widerlegt, die die dann nötige Existenz eines Wäreme-hotspots in der tropischen Troposphäre widerlegt haben.
– Alle vom IPCC im Bericht 2007 benutzten Klimamodelle haben sich bereits 10 Jahre später als falsch herausgestellt – siehe hierzu die mittlere Abbildung im Artikel. (Hierbei sollte man aber ergänzen, dass Klimamodelle auch die zukünftige CO2-Emmisionen beinhalten. Alle Klimamodelle, die die tatsächliche CO2-Emmissionen enthielten, liegen bei den geweissagten Temperaturen oberhalb der roten Linie. D.h. die realen Temperaturen müssten deutlich oberhalb der roten Linie sein, wenn die Klimamodelle des IPCC korrekt wären – knapp daneben!)
All diese Falsifizierungen der Idee vom gefährlichen menschgemachten Klimawandel (jede einzelne Falsifikation reicht, um dieser Idee den Garaus zu machen) können die Gläubigen dieser Idee aber nichts anhaben, denn es ist eine wissenschaftsfeindliche, religiöse Ideologie und innerhalb so einer Ideologie wird nie die Frage gestellt „Stimmt das denn?“ – selbst dann nicht, wenn die Antworten auf dem Tisch liegen!
MfG