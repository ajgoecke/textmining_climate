Teil 2 hier und Teil 3 hier
Mein Beitrag Ã¼ber zwei kleine Anfragen der AfD (hier) zur ungelÃ¶sten Problematik der absoluten globalen Mitteltemperatur in jÃ¼ngerer historischer Zeit â€“ die Pariser KlimaÃ¼bereinkunft bezieht sich sogar auf â€Werte vorindustrieller Zeitâ€œ ohne diese auch nur ansatzweise zu beziffern- hat eine groÃŸe, manchmal hitzige, Diskussion ausgelÃ¶st. Mit aktuell 127 Kommentaren und Ã¼ber 3000 Aufrufen (Stand 27.4.20 12:00 Uhr) gehÃ¶rt er sicher zur Klasse der Meistgelesenen. Soweit so gut.
Die z.T. hitzigen Kommentare zeigen m.E.n. aber auch, dass viele Leser mit der zugrunde liegenden, manchmal komplizierten, meteorologischen Messtechnik nicht so vertraut sind, wie sie der Artikel vielleicht voraussetzt. Das hat zur Folge, dass sie nicht auf Anhieb verstehen, warum der Artikel VersÃ¤umnisse â€“ gar Falschinformationen- der Bundesregierung â€“ beanstandet.
Sie vertrauen dabei auf ihre eigenen, oft in wissenschaftlicher Arbeit gemachten Erfahrungen aus eigener Anwendung der Messtechnik und ihrer Behandlung von Messdaten. Diese stimmen aber â€“ so meine eigene Erfahrung â€“ wenig bis nicht mit den hier angewendeten Methoden und Verfahren, insbesondere bei der Erstellung und Bewertung von Zeitreihen, Ã¼berein. Und sind deshalb in ihrer Schlichtheit selten auf diese Messdaten anzuwenden.
Dies voraus geschickt will ich die Leser jetzt ein wenig in die meteorologische Messtechnik von Temperaturdaten und deren Auswertung einfÃ¼hren. Dabei sollen die alten wissenschaftlichen GrundsÃ¤tze gelten, klassifiziere erst und vergleiche dann, aber vergleiche immer nur Vergleichbares.
ZunÃ¤chst schauen wir uns an, wie, wo, wann und wie oft die historischen Temperaturdaten gemessen und vorverarbeitet wurden.
Dann werden wir uns die Bildung von Anomalien aus diesen bereits vorverarbeiteten Daten und die sich daraus ergebenden Besonderheiten anschauen und zum Schluss die Bildung von Zeitreihen, und wie diese auf Fehlerkompensation bzw. Nichtkompensation Einfluss nehmen.
Was nicht behandelt wird.
GÃ¤nzlich auÃŸen vor lassen wir hier den magischen Trick der Klimaforscher, aus bekannten absoluten Unsicherheiten der Messwerte durch Bildung von Anomalien aus vielen Temperaturdaten statistische Wahrscheinlichkeiten werden zu lassen, die sich mit der bekannten Wahrscheinlichkeitsverteilung der Normalverteilung Ã¼ber ein oder zwei Sigma berechnen lieÃŸen. DarÃ¼ber hat Kip Hansen eine sehr eindrÃ¼ckliche Kritik geschrieben. Sie finden Sie hier unter dem Titelâ€Der â€Trickâ€œ anomaler Temperatur-Anomalienâ€œ
Weil es das Thema viel zu weit ausdehnen wÃ¼rde, lassen wir auch die merkwÃ¼rdige Behauptung des Stammvaters aller KlimaÃ¤ngste, James Hansen auÃŸen vor, der in einem paper zusammen mit seinem Kollegen Lebedeff 1987[2] behauptet, es wÃ¤re mÃ¶glich auch mit einer sehr geringen Zahl von ca. 80 Gitterzellen gleicher FlÃ¤che von 2500 x 2500 km die Temperaturentwicklung der gesamten Erde zu bestimmen. Und dies obwohl in den meisten Zellen nur sehr wenige, in anderen Ã¼berhaupt keine Messstationen vorhanden waren. Dieser Artikel war die Grundlage und ist es noch heute, fÃ¼r die Behauptung, dass die Anomalienbildung und deren Korrelation Ã¼ber groÃŸe FlÃ¤chen hinweg aussagefÃ¤hig fÃ¼r die Temperaturentwicklung sehr groÃŸe FlÃ¤chen seien. Diese Idee, die jeder Korrelations- und Fehlerrechnung widerspricht, wird hier unter dem Titel Von Korrelationen, Trends bei Anomalien! besprochen
Und ebenfalls lassen wir die Erkenntnis von Dresdner Statistikern auÃŸen vor, die nachwiesen, dass selbst echt zufÃ¤llige Fehler, sofern sie sich in Daten finden, die fÃ¼r autokorreliert sind und in entsprechenden Zeitreihen verwendet werden, systematische Komponenten in der GrÃ¶ÃŸe von einigen Zehntel Grad entwickelten. Details dazu finden Sie im Artikel mit dem Titel â€Konfidenz-Intervalle* fÃ¼r zeitliche Mittelwerte bei langfristigen Korrelationen: eine Fallstudie zu Anomalien der Temperatur auf der Erdeâ€œ
Am Schluss der Vorbemerkung erlaube ich mir noch den Hinweis auf die Tatsache, dass mit der Bildung von raum- und zeitÃ¼berspannenden Durchschnittswerten, die darin enthaltenen Temperaturwerte ihre physikalische Bedeutung als Temperatur verlieren. Es entsteht stattdessen irgendeine Art von Index. Er ist fÃ¼r statistische Zwecke begrenzt brauchbar, wenn man die Randbedingungen beachtet, aber keine physikalische energetisch relevante GrÃ¶ÃŸe mehr. Wer dazu genaueres wissen mÃ¶chte, der sei auf den schÃ¶ne Beitrag von C. Essex und R. McKittrick von 2006 verwiesen mit dem schÃ¶nen Titel â€Does A Global Mean Temperature existâ€œ[3]
Wie, wo und wann wurden die historischen meteorologischen Messdaten gewonnen?
- Thermometer
Am Anfang stand und steht das Thermometer. Dabei ist es zunÃ¤chst egal, ob es ein mechanisches oder ein elektronisches ist, obwohl jede Art einen erheblichen Einfluss auf die (systematischen) Fehler hat, natÃ¼rlich auch auf die erzielbare Genauigkeit.
- Messumgebung
Doch mindestens ebenso wichtig ist die Messumgebung. Diese wird bestimmt durch das GehÃ¤use (MesshÃ¼tte), welches das Thermometer vor verfÃ¤lschenden UmgebungseinflÃ¼ssen, wie Strahlung, Wind oder Niederschlag aber auch BeschÃ¤digung schÃ¼tzen soll, aber auch in starkem MaÃŸe von der unmittelbaren Umgebung. Um vergleichbare Ergebnisse wenigstens annÃ¤hernd zu erreichen, muss dies bestimmten Ã¼berall vergleichbaren Standards genÃ¼gen und sorgfÃ¤ltig gebaut und gepflegt werden.
- Messregime
Und auch das Messregime ist von entscheidender Bedeutung, nÃ¤mlich wann und wie oft, von wem und wie das Thermometer abgelesen wird.
- Die geografische und zeitliche Abdeckung
Und da das Wetter- wie das Klimageschehen nicht an nationalen Grenzen haltmacht, muss die geografische Abdeckung mit Messstationen gesichert sein, und das sowohl nach Zahl als auch nach Ort und Zeit. Alle drei Anforderungen werden erst seit dem Aufkommen der Satelliten hinreichend erfÃ¼llt. Dabei tauscht man die VorzÃ¼ge der Satellitenmessung gegen einen enormen Nachteil ein, denn man musste einen unangenehmen Kompromiss akzeptieren. Es ist die sehr schlechte vertikalen AuflÃ¶sung. Eine Satellitenmessung ist daher immer der Mittelwert von TemperaturÃ¤nderung Ã¼ber eine Distanz von ca. 5000 m. (z.B. der lower troposphere)
- Was geschieht mit den meteorologischen Messdaten? Die Weiterverarbeitung!
Bis zur â€Erfindungâ€œ der Idee von der Notwendigkeit einer weltweiten Mitteltemperatur, gab es nur wenige Meteorologen, die die vorhandenen Messdaten nutzen wollten, um evtl. aus ihnen eine weltweite Mitteltemperatur zu extrahieren. Man begnÃ¼gte sich damit â€“ nachdem solche groÃŸartigen Wissenschaftler der Meteorologie wie KÃ¶ppen die Definition von Klimazonen etabliert hatten- fÃ¼r diese Klimazonen oder Regionen Mitteltemperaturen und deren JahresgÃ¤nge zu berechnen. Normalerweise dienten die meteorologischen Messdaten aber vor allem der Ã¶rtlichen Wettervorhersage und bez. ihrer Langzeit-Analyse auch dazu ggf. lokale KlimaÃ¤nderungen im Nachhinein zu erkennen.
Anmerkung: Achten Sie mal auf die Ã¶rtlichen Wetterberichte, dort wird man ihnen so gut wie nie die erwartetet Durchschnittstemperatur der nÃ¤chsten Tage nennen, sondern ausschlieÃŸlich deren Maxima und Minimawerte.
Dazu erstellte man Zeitreihen (hier Temperatur-Ganglinien) entweder aus absoluten Werten oder von Anomalien je nach Bedarf, deren erzielbare Genauigkeit fÃ¼r die gewÃ¼nschten Zwecke vÃ¶llig ausreichte.
Zu 1. Thermometer
Abbildung 1 zeigt eine klassische, deutsche WetterhÃ¼tte (erweiterter Stevenson Screen) und (rechts) ein darin befindliches modernes Thermometerensemble gezeigt, wie es immer noch in vielfÃ¤ltiger Vewendung ist. Die senkrechten Thermometer sind (links) das Assmansche Aspirationthermometer zur Temperaturmessung, rechts daneben ein befeuchtetes und belÃ¼ftetes Psychrometer zur Messung der Luftfeuchte. Quer darunter befinden sich zwei liegende Thermometer zur Messung von Max. und Min. Temperaturen. Die runde Plastiktrommel enthÃ¤lt den Federmotor mit Ventilator zur Erzeugung des Luftstromes mit Ã˜ 2,5 m/s vor und wÃ¤hrend der Messung. Im MÃ¤rz 2009 vom Autor im Metereologischen Institut der Freien UniversitÃ¤t Berlin aufgenommen
Obwohl schon Galileo erste Versuche unternahm mit einem Thermometer meteorologische Messungen zu machen, und sich danach ein ziemlicher Wildwuchs an solchen Thermometer ausbreitete, weil viele LandesfÃ¼rsten die aufkommende Meteorologie aus naheliegenden GrÃ¼nden fÃ¼r Ã¤uÃŸerst wichtig hielten, kann man erst ab 1892 von einem Standardinstrument fÃ¼r meteorologische Zwecke sprechen. Es handelt sich dabei um das zwangsbelÃ¼ftete Assmansche (Quecksilber gefÃ¼llte) Aspirationsthermometer (linkes, senkrechtes Thermometer). Die zuvor verwendeten tlw. mit Alkohol gefÃ¼llten Thermometer mit mehr als Â± 1 Kelvin Gesamt-Unsicherheit wurden nach und nach weltweit ersetzt. In den angelsÃ¤chsisch orientierten tlw. auch regierten LÃ¤ndern setzte sich hingegen oft zusÃ¤tzlich das Max-Min-Thermometer durch.
Doch allein die Wahl des Thermometers, sein jeweiliger Zustand, seine Montage, sein Betrieb und seine Ablesung sorgen fÃ¼r eine ganze Reihe von zufÃ¤lligen wie systematischen Fehlern. Diese wurden vielfach sorgfÃ¤ltig untersucht, aber nur in geringer Anzahl auch quantitativ bestimmt und damit Ã¼berhaupt erst theoretisch korrigierbar.
Zu 2. Messumgebung
FÃ¼r die Messumgebung war von Anfang an klar, dass dieses zuallerst eine Art SchutzhÃ¼tte sein sollte, von denen besonders im 19 Jahrhundert eine ganze Reihe von Konstruktionen erprobt wurden, wobei sich letztendlich die erweiterte englische HÃ¼tte als Standard durchsetzte. Doch bis dahin waren die (wenigen) Messergebnisse praktisch unvergleichbar, weil die HÃ¼tte auf ihre eigene Art die Messergebnisse beeinflusste.
Abbildung 2 Darstellung der zum Ende des 19. Jahrhunderts und bis heute verwendeten HÃ¼ttentypen bzw. Screens von links: franzÃ¶sische HÃ¼tte, daneben (kaum erkennbar) Mitarbeiter zum GrÃ¶ÃŸenvergleich, daneben (Original) Stevenson HÃ¼tte, dann Wildsche HÃ¼tte, dann mehrere erweiterte Stevenson HÃ¼tten und moderne runde kleine Screens. (Quellen. Linkes Bild [Sprung, 1890] rechtes Bild: KNMI Messfeld von van der Meulen fÃ¼r die Zeit von 1989-1995)
Praktisch unkorrigiert, obwohl seit langem bekannt, ist der systematische Fehler, der allein durch die WÃ¤rmetrÃ¤gheit der HÃ¼tte entsteht, und der ca + 1 bis 2 Kelvin zu jedem Ergebnis der eigentlich gesuchten AuÃŸentemperatur hinzuaddiert. Praktischerweise definieren jedoch die Meteorologen die Temperatur in der HÃ¼tte[4], als die gesuchte AuÃŸentemperatur. Woraus geschlossen werden muss, dass jede durch diese Messungen gewonnene absolute Temperatur im Mittel im 1 bis 1,5 Kelvin gegenÃ¼ber der wahren AuÃŸentemperatur zu hoch sein muss.
Abbildung 3(links): Englische HÃ¼tte und moderne Messstation am Messort Detroit Lakes USA, kurz vorher aufgenommen und ins Netz gestellt am 25.07.07 von A. Watts; (rechts) Temperaturverlauf aus der GISS Datenbank zur Messstelle Detroit Lakes USA
Beispiel fÃ¼r den Zustand und Umgebung einer meteorologische US Messstation sowie deren Zeitreihe ihrer Jahresmittelwerte, die vom Projektteam untersucht wurde. Man beachte den Sprung ab etwa dem Jahr 2000 und die EntlÃ¼ftungen der Klimaanlagen in unmittelbarer NÃ¤he.
Nicht weniger Einfluss hat die unmittelbare Umgebung in der die MesshÃ¼tte.
Zur KlÃ¤rung dieses Sachverhaltes hat sich der Meteorologe und Betreiber des inzwischen weltweit erfolgreichsten Klimarealisten- Blogs Wattsupwiththat Antony Watts groÃŸe Meriten erworben. Er rief 2007 das SurfaceStationorg. Netzwerk ins Leben, das 2009 erfolgreich mit einer VerÃ¶ffentlichung [5]abgeschlossen wurde. Freiwillige im ganzen Land untersuchten Ã¼ber 850 der 1200 meteorologischen Messstationen, die von der US-Wetter- und OzeanbehÃ¶rde NOAA betrieben wurden.
Abbildung 4 Untersuchung von bisher (Ende Februar 2009) 854 von 1221 offiziell verwendeten Messstationen der unterschiedlichsten Art. Man beachte, dass nur 3 % aller Stationen dem Standard voll entsprechen, weitere 8 % kÃ¶nnen evtl. noch als dem Standard genÃ¼gend zugeordnet werden. Die restlichen 89 % zeigen jedoch potentielle Abweichungen nach den Vorgaben der CRN von mindestens 1 Â°C (20 %) Ã¼ber 2 Â°C ( 58% ) bis 5 Â°C (11%).
Auswertung der ÃœberprÃ¼fung von 807 US Messstationen in Bezug auf die offizielle Einstufung ihrer QualitÃ¤t und Umgebung (CRN).
Das Ergebnis war erschreckend. Der Zustand der Messstationen und damit die Messergebnisse waren miserabel. 89 % von ihnen lagen oberhalb der Fehlerklasse 2, was bedeutet, dass sie > 2 Â° C bis > 5 Â°C fehlerhafte Ergebnisse zeigten. Nur 3 % lagen gleich oder unterhalb von 1 Â°C. Und noch schlimmer war, dass diese Stationen fÃ¼r die gesamte Welt sozusagen als Goldstandard in Punkto QualitÃ¤t galten. D.h. die vom IPCC und ihren Zuarbeitern immer wieder herangezogenen Daten aus den anderen Kontinenten waren mit hoher Wahrscheinlichkeit noch schlechter.
Es wÃ¼rde hier zu weit gehen, die vielfÃ¤ltigen Fehlerquellen allein der landbasierten Messstationen aufzulisten, daher beschrÃ¤nke ich mich auf zwei kurze ErgÃ¤nzungen.
- Selbst bei identischer Konstruktion von HÃ¼tte und Messinstrument, ergeben sich groÃŸe Unterschiede allein durch das Material und seinen Anstrich, bzw. dessen VerÃ¤nderung Ã¼ber der Zeit, z.B. durch Alterung
- Sind die landbasierten Messungen schon oft mit mehr (systematischen) Fehlern behaftet als ein Hund FlÃ¶he hat, so ist das bei den seegestÃ¼tzten Messungen noch weit schlimmer. Und weil das Land nur 29 % der ErdoberflÃ¤che bedeckt, aber 71 % der Erde Ozeane sind, ist dieser Umstand eigentlich noch schlimmer. Denn dort sind die Messungen wirklich nur punktuell, d.h. Ã¤uÃŸerst selten sowohl in rÃ¤umlicher als auch in zeitlicher Hinsicht. Ein wenig wird dieser Mangel gelindert durch die Tatsache, dass statt der eigentlich gesuchten Lufttemperatur, stattdessen, als Proxy, die Temperatur der Seewasser OberflÃ¤che (SST Sea Water Surface Temperature) verwendet wird. Wegen der groÃŸen WÃ¤rmetrÃ¤gheit und guten -LeitfÃ¤higkeit des Wassers deckt ein Messpunkt im Vergleich zum Land viel grÃ¶ÃŸere Gebiete ab.
Wer Interesse daran hat, diese Fehler etwas genauer behandelt zu sehen, der sei auf das Video â€Wie glaubwÃ¼rdig sind die Zeitreihen historischer Klimadatenâ€œ (hier) verwiesen, dass einige der wichtigsten Aspekte zu diesen Fehlern aufzeigt.
Ende Teil 1
[1] https://www.wsj.com/articles/SB10001424052970204422404576594872796327348#printMode
[2] HANSEN, JAMES LEBEDEFF, SERGEJ (1987) Global Trends of Measured Surface Air Temperature. JOURNAL OF GEOPHYSICAL RESEARCH 92:13345 13372.
[3] Essex, C, Mc Kittrick, R. Andresen, B. (2006) Does A Global Temperature Exist? Non-Equilibrium Thermodynamics
[4] Weischet [Weischet, 1995] S115 â€“ S 109 ff
[5] Is the U.S. Surface Temperature Record Reliable? By Antony Watts SurfaceStations.org ISBN 13: 978-1-934791-29-5
FÃ¼r alle, die diese Information am StÃ¼ck lesen wollen hier das Ganze als pdf Die schwierige nimmer endende Fehlerdiskussion
Nein. Vielleicht sollten Sie mal einen Blick auf die Algorithmen werfen, statt Ihrer eigenen KreativitÃ¤t freien Lauf zu lassen. Siehe z.B. Homogenization of Temperature Series via Pairwise Comparisons
Man kann sicherlich diskutieren, ob das die betrachteten Fehlerquellen eleminiert oder deren Fehler reduziert, oder â€¦ Aber einfach frei Dinge erfinden ist kein gutes Zeichen.
Herr Marvin MÃ¼ller, auch wenn Ihre Geduld mit mir zu Ende geht: Ãœbersehen Sie doch bitte nicht das Fragezeichen am Satzende. ğŸ˜‰ Sie dÃ¼rfen dann mit Recht annehmen, dass ich nichts erfunden, sondern maximal eine Hypothese aufgestellt habe.
Ein Auszug aus Ihrem Link *paarweise Homogenisation* Ã¼bersetzt:
â€â€¦ Der Algorithmus bildet paarweise Differenzreihen zwischen seriellen monatlichen Temperaturwerten aus einem Netzwerk von Beobachtungsstationenâ€¦..Der Algorithmus verwendet auch Stationsverlaufsinformationen, sofern verfÃ¼gbar, um die Identifizierung kÃ¼nstlicher Verschiebungen in Temperaturdaten zu verbessern. ZusÃ¤tzlich wird eine Bewertung durchgefÃ¼hrt, um TrendinhomogenitÃ¤ten von abrupten Verschiebungen zu unterscheiden. Wenn die GrÃ¶ÃŸe einer scheinbaren Verschiebung, die einer bestimmten Station zugeordnet ist, zuverlÃ¤ssig geschÃ¤tzt werden kann, wird eine Anpassung fÃ¼r die Zielserie vorgenommen. â€¦ â€œ
Das scheint doch ziemlich in die von mir angedachte Richtung zu gehenâ€¦.
Aber egal, zurÃ¼ck zum Ursprung: Auf meine hinter allem stehende Frage, ich formuliere einmal zusammenfassend: â€Wie kann es sein, warum darf es sein, dass in einem GISS- Diagramm von 1989 die *US- Main- Temperaturen* der 1930-er Jahre anders angegeben sind als in einem GISS- Diagramm von 2012?â€œ
antworten Sie, ich fasse zusammen: â€Das hat alles seine Richtigkeit. Die Daten werden stÃ¤ndig PrÃ¼fungen unterzogen, werden homogenisiert, periodisch durch Algorithmen mit anderen Stationen verglichen und korrigiert. Die QualitÃ¤tskontrollen sind transparent, die Algorithmen Ã¶fentlich. das ergibt regelmÃ¤ÃŸig ein neues Bildâ€¦â€œ
Ist das so in etwa richtig, Herr Marvin MÃ¼ller?
Ich jedenfalls finde die Frage gut, warum wir hinnehmen sollen, das die Data zweck-homogenisiert werden.
Siehe auch hier https://jennifermarohasy.com/2019/02/changes-to-darwins-climate-history-are-not-logical/
und
hier https://ipa.org.au/publications-ipa/bureau-cooling-the-past-to-declare-record-heat
Was bei der Kritik an den fuer die ATE-Forschung fuer relevant gehaltenen Messungen voellig uebersehen wird, ist dass CO2 nur etwa 1/4 des seit vorindustrieller Zeit im globalen Mittel (?) beobachteten transienten Temperaturanstiegs von fast 1 Grad bewirkt hat und praktisch kaum aus den Messwerten zu bestimmen ist. Haupttreiber der Temperaturaenderungen sind (abgesehen von Waermeinseln) Solaraktivitaet, Wasserdampf und Wolkenbedeckung.
Mit der auf Basis hochpraeziser Labor- sowie Satelliten- und Strahlungsmessungen heute recht gut bekannten Verdoppelungs-Sensitivitaet des CO2 von 0,6 Â°C am Boden* (im Gleichgewicht, mit Wolken, Feedback etc.) ergibt sich rechnerisch deltaT=0,6*ln(ppm/280)/ln(2). Auch der emissionsbedingte CO2-Anstieg unter Beruecksichtigung der ppm-abhaengigen Netto-Senkenfluesse von Ozeanen und Biomasse (die pro 20 ppm um ~20 GtC/a steigen) â€“ weshalb eine Dekarbonisierung garnicht noetig ist â€“ kann mit einem C-Modell berechnet werden, siehe unter http://www.fachinfo.eu/dietze2020.pdf (engl. Version unter 2020e).
*) Der Wert 0,6 Â°C von ECS (equilibrium climate sensitivity) laesst sich â€“ unter der Praemisse dass der heute bei 410 ppm transient beobachtete Temperaturanstieg von 0,95 Â°C etwa um den Faktor 0,7 geringer ist und CO2 (wie Regressionsanalysen ergaben) einen Anteil von 25% daran hat â€“ leicht berechnen mit
ECS = 0,95/0,7/ln(410/280)*ln(2)*0,25 = 0,62 Â°C
Netto-Senkenfluesse von Ozeanen und Biomasse (die pro 20 ppm um ~1 GtC/a steigen)
Dann gibt es die vereinzelten Messungen der Wassertemperatur in den Ozeanen, die theoretisch 1m unter der OberflÃ¤che gemessen werden sollten. Dann gibt es die Satellitenmessungen auf Basis der Sauerstoffbande im 0,5-0,6 cm-Band, welche z. B. bei Roy Spencer regelmÃ¤ÃŸig gefÃ¼hrt wird. Diese Messung ergibt offenbar eine vertikale Mitteltemperatur Ã¼blicherweise der TroposphÃ¤re, aber auch andere Luftdruckzonen sind mÃ¶glich.
Und schlieÃŸlich gibt es die Methode der Okkultation, welche die dichteabhÃ¤ngige Laufzeit von Funksignalen quer durch die ErdatmosphÃ¤re als Referenz fÃ¼r die Temperatur nimmt.
All diese Verfahren haben unterschiedliche Eigenschaften bzw. Vor- und Nachteile. Und sie korrelieren teilweise, teilweise aber auch nicht. Auf welcher Basis stellt nun die Mainstream-Hysteriker-Klimatologie fest (auf zwei Stellen hinter dem Komma), welche Temperatur wir haben. Die Unterschiede in den einzelnen Methoden zeigen doch, daÃŸ wir â€hochgenauâ€œ eigentlich gar nichts wissen. Und wenn eine Harmonisierung nicht gelingt, mÃ¼ÃŸte man ehrlicherweise eine Bandbreite angeben, innerhalb der der wahre Wert mit einer gewissen Wahrscheinlichkeit liegt. Ggfs. noch unterteilt in unterschiedliche WahrscheinlichkeitsbÃ¤nder, also z. B. 20%, 50%, 80%, 100%. Und, man wÃ¤re dadurch auch gezwungen, einmal klar zu definieren, was man eigentlich unter einer â€Weltmitteltemperaturâ€œ Ã¼berhaupt versteht!
Zur Sauerstoffmessung gibt es z. B. die Beschreibung: â€Satelliten messen die thermische Mikrowellenemission von Luftsauerstoff im Sauerstoffabsorptionskomplex von 50-60 GHz. Die resultierenden kalibrierten Helligkeitstemperaturen (Tb) sind nahezu gleichwertig mit der thermometrischen Temperatur, insbesondere einem vertikal gewichteten Durchschnitt der Lufttemperatur, wobei die vertikale Gewichtung durch â€Gewichtungsfunktionenâ€œ dargestellt wird.â€œ
Frage: kennt jemand Details zu diesen Messungen, speziell wie das genaue MeÃŸprinzip arbeitet? Allgemein wird doch behauptet, zweiatomige MolekÃ¼le wie Oâ‚‚ strahlen nicht. Nach der Beschreibung scheinen sie aber mit temperaturabhÃ¤ngiger Frequenz doch zu strahlen?
Spencer schreibt dazu in â€UAH Version 6 Global Satellite Temperature Products: 3Methodology and Resultsâ€œ
Die Aussage, das O_2 nicht strahlt ist in der Regel, dass es nicht im langwelligen Infrarotbereich absorbiert und strahlt. hier wird die Abstrahlung im Gigahertz-Bereich genutzt â€¦
Satelliten zur Temperaturmessung haben Bahnen unter 90Â° zum Ã„quator, laufen also auf einer Polarbahn, wÃ¤hrend die Erde sich unter ihnen durchdreht. Wenn so ein Satellit wie Ã¼blich 12 UmlÃ¤ufe pro Tag macht, erfolgt der nÃ¤chste Scan um 30Â° versetzt zum letzten. Weil sich die Erde kontinuierlich dreht, verlaufen die Projektionen der Bahn auf die OberflÃ¤che aber nicht auf Meridianen sondern schrÃ¤g dazu. Dadurch ergibt sich ein Muster, bei dem ein Zweieck zunÃ¤chst von SÃ¼d nach Nord abgetastet wird und ein dazu um 15Â° versetztes Zweieck 12 Stunden spÃ¤ter von Nord nach SÃ¼d.
Wenn man die LÃ¤nge des Ã„quators grob mit 40 Mio. km rechnet, ergeben 30Â° am Ã„quator einen Abstand von 3,33 Mio. km und 15Â° ergeben 1,66 Mio. km. Es erhebt sich also die Frage, erfaÃŸt das MeÃŸprinzip die tatsÃ¤chliche FlÃ¤che der entstehenden schrÃ¤gen Kugelzweiecke oder was wird eigentlich genau gemessen?
Wenn die Definition der Mitteltemperatur jene ist, die 2 m Ã¼ber der OberflÃ¤che gilt, wÃ¤ren solche Satellitenmessungen nur dann Ã¼berhaupt heranziehbar, wenn es eine eindeutige Beziehung gÃ¤be, wie man TroposphÃ¤rendurchschnitte in bodennahe Werte umrechnen kann, die es meines Wissens aber nicht gibt.
Nebenbei bemerkt ist der â€langwellige Infrarotbereichâ€œ mit WellenlÃ¤ngen im Micrometerbereich wesentlich kurzwelliger als der Gigaherzbereich mit hier um 0,5 cm WellenlÃ¤nge.
der Erdumfang ist natÃ¼rlich nicht 40 Mio. km sondern 40.000. Damit sind 30Â° 3333 km und 15Â° 1670 km. Sorry â€¦
Keine. Denn alle beruhen sie auf denselben Rohdaten und unterscheiden sich nur durch die Art der â€Datenpflegeâ€œ. Die Rohdaten, bzw. ihre nicht mehr verbesserabre QualitÃ¤t sind das Problem. Und das Nichtwissen der â€Klimaforscherâ€œ, bzw. ihre Ignoranz das vorhandene Wissen nicht anzuwenden, wie man mit fehlerbehafteten Zeitreihen umzugehen hat.
M.W.n. sind nur noch die â€korrigiertenâ€œ Daten verfÃ¼gbar. Suchen Sie mal nach Ewert auf unserer Suchleiste, der hat das verfolgt. Oder hier: https://www.eike-klima-energie.eu/2019/06/28/adjustierte-unadjustierte-daten-nasa-nutzt-den-zauberstab-des-frisierens-und-erzeugt-erwaermung-dort-wo-es-nie-eine-gab/
oder hier https://realclimatescience.com/wp-content/uploads/2018/06/NASA-US-1999-2016-2.gif
Hier ist mehr dazu: https://realclimatescience.com/2019/02/61-of-noaa-ushcn-adjusted-temperature-data-is-now-fake/
Ich bin selbst etwa 2012 auf Diskrepanzen in den Datenreihen gestoÃŸen, als ich Ã¼ber Ã¶kologisch vertrÃ¤gliche(re) Anbaumethoden in der LW recherchierte (Strip- Farming z. B.), die die DÃ¼rre im Mittleren Westen der USA in den 1930- er Jahren hervorbrachte und bin in diesem Zusammenhang in einer Arbeit von Hansen 1989 auf ein Diagramm mit der *US- Main- Temp* gestoÃŸen. Ich habe dann eine aktuelle Version dieser *US- Main- Temp* auf den GISS- Seiten gesucht und besonders bei den Temp genau dieser 1930-er jahre erhebliche Unterschiede gefunden (Temp. der Mitte der 30-er Jahre erniedrigt in der Neuversion).
In der Folge stieÃŸ ich dann auf die Arbeiten von Friedrich Karl Ewert und erkannte das mÃ¶gliche AusmaÃŸ des Betruges, wohl ausgehend vom GISS, milder kann man es ja nicht bezeichnen.
Was mich wundert ist die Stille in der Wissenschaftsgemeinde. Normalerweise mÃ¼sste ein Aufschrei durch die Reihen gehen, wenn eine wichtige Datenbasis quasi unbrauchbar gemacht wurdeâ€¦.
NOAA stellt sowohl unadjusted, als auch adjusted Daten zu VerfÃ¼gung:
Warum setzen Sie GISSTEMP und GHCN gleich? GISSTEMP verwendet GHCN4 adjusted als Input fÃ¼r die Analysen, arbeitet also nicht mit â€raw dataâ€œ. Aber das ist auch dokumentiert auf deren Seiten (z.B. in deren FAQ). Sie verweisen allerdings auf ISTI als die ultimative Quelle fÃ¼r die Rohdaten.
Wenn Sie sagen, dass Gisstemp die GHCN- *adjustet* Daten zur Grundlage nimmt, sollten dann also die nachtrÃ¤glichen Korrekturen z. B. der Erniedrigung historischer Werte von den GHCN- Daten ausgehen und kÃ¶nnen keine Rohdaten seinâ€¦????
Man kann bei GISStemp nachlesen, wann es welche VerÃ¤nderung in den GISStemp-Versionen gab. Und wenn man Version 2 und Version 3 oder 4 miteinander vergleicht, dann muss ein Unterschied nicht daran liegen, dass die Roh-daten verfÃ¤lscht wurden, sondern es kann sein, dass es andere Input Daten sind (Vorverarbeitete Daten statt Rohdaten; mehr Stationen; andere Stationen ringsherum die Einfluss auf sowas wie die paarweise Homogenisierung haben, â€¦) oder das der Algorithmus geÃ¤ndert wurde, oder â€¦
Herr Limburg hat im Artikel den Artikel von Richard Muller zitiert. Ich wÃ¼rde Ihnen empfehlen, den mal komplett zu lesen und vielleicht auch mal die anderen Temperaturreihen, die es so gibt, in Ihre Ãœberlegungen einzubeziehen.
Also etwa so: Wenn eine Station in X seit 100 Jahren misst, dann vor 50 Jahren 2 Stationen in der NÃ¤he hinzukommen und bestÃ¤ndig ca. 1,5Â° weniger messen, geht man dann einfach davon aus, dass die Station in X auch die 50 Jahre zuvor 1,5Â° *zuviel* gemessen hat?
Nun, da kann man mit etwas KreativitÃ¤t alles mÃ¶gliche veranstaltenâ€¦ Danke trotzdem.
Das wird auch in Zukunft schwierig bleiben, denn das fÃ¼r die OriginaldatensÃ¤tze verantwortliche britische Hadley Center schrieb in einer mail (http://rogerpielkejr.blogspot.com/2009/08/we-lost-original-data.html ) an Prof. R. Pielke (12.8.09). â€We are not in a position to supply data for a particular country not covered by the example agreements referred to earlier, as we have never had sufficient resources to keep track of the exact source of each individual monthly value. Since the 1980s, we have merged the data we have received into existing series or begun new ones, so it is impossible to say if all stations within a particular country or if all of an individual record should be freely available. Data storage availability in the 1980s meant that we were not able to keep the multiple sources for some sites, only the station series after adjustment for homogeneity issues. We, therefore, do not hold the original raw data but only the value-added (i.e. quality controlled and homogenized) data.â€œ
Mit anderen Worten, die OriginaldatensÃ¤tze sind verschwunden und somit der Forschung nicht mehr zugÃ¤nglich
â€¦ Hmmm, damit wird einiges klar. Aber irgendwie glaube ich nicht an Zufall. Andererseits kÃ¶nnte dadurch auch niemand der Behauptung entgegentreten, die jetzt verwendeten TemperaturdatensÃ¤tze seien durchweg gefÃ¤lscht und Klimamodelle, die historische Temperaturdaten als Eingangsparameter nutzen, seien allein dadurch unbrauchbarâ€¦
Das die *Wissenschaftsgemeinde* soetwas nicht thematisiert, ist zumindest erstaunlichâ€¦
Zumindest fÃ¼r den Bereich des DWD: die Monats(mittel)werte wurden (mindestens) bis in die 80er Jahre in gedruckter Form verteilt. Klarheit darÃ¼ber und Infos dazu kÃ¶nnten doch einfach von den hier mitlesenden Meteorologen kommen.
MfG
Ketterer
Vielleicht liegt das einfach daran, dass sich hier jemand irrt?
Wenn das CRU die Roh-Daten nicht archiviert und die dann dort verloren gehen, sind die dann auch an der Stelle weg, an der die Daten erhoben wurden? Hat z.B. der deutsche Wetterdienst jetzt keine Daten mehr von vor 2000? Woher stammen die Daten, die BEST, GHCN, ISTI zur VerfÃ¼gung stellen?
Es wÃ¤re schÃ¶n, wenn Sie nicht nur hypothetische Fragen stellen wÃ¼rden, sondern Quellen lieferten. Wir alle wÃ¤ren dankbar.
Der ist in den Messdaten reichlich vorhanden, hat sich doch die WeltbevÃ¶lkerung von rd. 1,5 Mrd. Menschen auf rund 3 Mrd. 1960 verdoppelt und liegt heute bei 7,8 Mrd. Menschen, wovon rd. 50 % in StÃ¤dten leben. Diese wiederum begannen dank ihres Wachstums auch die ursprÃ¼nglich auÃŸerhalb liegenden Messstationen zu umwachsen. Er liefert also einen krÃ¤ftigen Teil der systematischen Fehler.
Das war aber nicht das Thema dieses Berichtes, sondern die Frage, warum die historischen Daten fÃ¼r die Berechnung einer globalen Mitteltemperatur schlicht nicht taugen.
Das ist das eigentliche Problem. Alle bisher vorgeschlagenen Korrekturmittel, z.B. Messung des Nachtlichts per Satellit, um die Ausbreitung der StÃ¤dte zu bestimmen, leiden unter dem einen oder anderen Mangel, und lassen sich deshalb nicht wirklich als KorrekturmaÃŸ verwenden. Bei den Satelliten ist das anders, da die eine extrem viel bessere FlÃ¤chenauflÃ¶sung haben, mittelt sich der WI dort aus.
Ist es tatsÃ¤chlich so, dass Satellitenmessungen, die O2-Mikrowellenstrahlen messen, eine so hohe FlÃ¤chenauflÃ¶sung haben, dass sie WI-Muster klar erkennen und â€ausmittelnâ€œ kÃ¶nnen? So gesehen mÃ¼sste man mit Satellitendaten â€WI-bereinigteâ€œ Ergebnisse bekommen kÃ¶nnen? Der Vergleich von Satellitendaten mit dem, was z.B. Herr Kowatsch ausfÃ¼hrt, der sich auf Bodenmessungen bezieht und 2/3 des Temperaturanstiegs mutmaÃŸlich den WI zuschreibt, wÃ¤re dann sehr interessant! Mein Eindruck ist, dass zumindest die mir bisher bekannten Satellitendaten â€“ vielleicht dank Adjustierung â€“ keine niedrigeren Wert zeigen.
â€Schaut man sich die RMSS-Satellitendaten an, dann gibt es im SÃ¼dpolarbereich praktisch keine Temperaturanstiegeâ€œ
Bei
http://images.remss.com/msu/msu_data_monthly.html
habe fÃ¼r die Antarktis keine Satelliten-Daten gefunden.
https://www.derklimarealist.de/
Dort hat er seine meist unverÃ¶ffentlichten Leserbriefe an die Augsburger Allgemeine verÃ¶ffentlicht. Ich habe sie immer gerne gelesen. Er ist wahrscheinlich auch der Autor des 1970 erschienenen Buches â€Die Temperaturmessungen in MÃ¼nchen, 1781-1968â€œ. Seine letzter Beitrag behandelte die Horror-Hitze im Februar in der Antarktis: â€Man hat wirklich selten so viel Unsinn gelesen, wie in diesem Artikel! Die â€“ noch nicht einmal bestÃ¤tigten â€“ Ã¼ber 20 Grad wurden an einerâ€¦â€œ. Die Skeptiker sterben leider alle aus.
http://www.remss.com/measurements/upper-air-temperature/
Und dort rechts im Bild die Region einstellen. Links im Bild kann man die verschiedenen HÃ¶henbereiche wÃ¤hlen.
â€Ich bin da aber eher skeptischâ€œ.
Ich glaube ich habe die Ursache des anomalen Temperaturanstiegs der Wetter-Station Amundsen Scott in den Sommermonaten seit ca. 2000 gefunden. Hilfreich sind die monatlichen Wetterballon-Daten dieser Station (NOAA IGRA). FÃ¼r den Monat Dezember im Zeitraum von 1989-2019 ergibt sich fÃ¼r den Boden ein positiver Trend von 0,6 +/- 0,4 Â°C/Dek, in 5,6 km HÃ¶he (500 mbar) ein negativer Trend von -0,1 +/- 0,3 Â°C/Dek. In den Sommer-Monaten 9..2 betrÃ¤gt der Trend 1989-2019 an der OberflÃ¤che 0,4 +/- 0,2 Â°C/Dek und 0,1 +/- 0,2 Â°C/Dek in 5,6 km HÃ¶he (500 mbar). Die ErwÃ¤rmung ist vermutlich ein WÃ¤rme-Insel-Effekt der Wetter- und Forschungs-Station.
Wie schon wiederholt erwÃ¤hnt, die Station Sonnblick misst seit vielen Jahren unter vergleichbaren Bedingungen :
http://www.zamg.ac.at/histalp/dataset/station/csv.php?c=AT&s=_127
Andere Stationen mit (cum grano salis)
Konstanten Bedienungen
Niwot Ridge LTER, die oberen Stationen
Und sicherlich weitere Stationen des LTER Netzwerks.
MfG
Ketterer
Die Sensoren werden heute mit einer standardisierten Grundtoleranz bis herunter auf Klasse AA von Â±0,1 K bei 0Â°C (oder noch besserâ€¦) angeboten, die die fÃ¼r sich durchaus auch bringen, aber mit der â€Konfektionierungâ€œ gehen die eigentlichen Probleme los, da wissen die allermeisten Anwender wirklich nicht, was sie tun. Neben den Fragen der Ankopplung an das MeÃŸobjekt gibt es auch noch die Problematik mit den parasitÃ¤ren Thermospannungen (durch vielfÃ¤ltige Materialkombinationen und Temperaturgradienten im Messkreis), die man nur mit einem gewissen Aufwand eliminieren kann, da nÃ¼tzt der hÃ¶chstauflÃ¶sende AD-Wandler nix, da muÃŸ man sich was einfallen lassen.
Wenn ich also in Meteorologie / KlimagedÃ¶ns Temperaturangaben mit zwei Stellen nach dem Komma sehe, da stehen auch mir die Haare zu Berge, die â€Verursacherâ€œ solcher Angaben kÃ¶nnen nur messtechnische Volltrotttel sein â€¦.
Es gab noch eine weitere, sogar durch ein peer review gelaufene VerÃ¶ffentlichung:
â€Analysis of the impacts of station exposure on the U.S. Historical Climatology Network temperatures and temperature trendsâ€œ
Souleymane Fall, Anthony Watts, John Nielsenâ€Gammon, Evan Jones, Dev Niyogi, John R. Christy, Roger A. Pielke Sr.
https://doi.org/10.1029/2010JD015146
Dort wird auch diskutiert, welche Auswirkungen die Stationsklassifizierung auf den Trend von Minimal-, Maximal- und Durschnittstemperatur hat.
Das setzt allerdings ein WissenschaftsverstÃ¤ndnis und eine Ethik bei den Akteuren voraus, die erkennbar nicht (mehr) vorhanden sind â€“ vom Publikum und den Staatsmedien ganz zu schweigen.
Ungeachtet dessen: sehr verdienstvoll, uns an Ihrer Expertise teilnehmen zu lassen, Herr Limburg!
Rainer Facius
Aber das interessiert die sog. â€Klimaforscherâ€œ nicht. Die nehmen das, was fÃ¼r ihre Vorstellungen paÃŸt und nicht das, was man nehmen muÃŸ, was also korrekt ist.