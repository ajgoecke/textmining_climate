Bild rechts: Die lineare Regression kleinster Quadrate des Trends der globalen monatlichen Temperaturanomalie nach RSS zeigt keine globale ErwÃ¤rmung seit 18 Jahren und 8 Monaten, also seit Januar 1997, obwohl ein Drittel aller anthropogenen Antriebe wÃ¤hrend der Periode des Stillstands Eingang fand.
Wie immer aber eine Warnung hinsichtlich des gegenwÃ¤rtigen El NiÃ±o. Wahrscheinlich wird die Temperaturzunahme, die normalerweise mit einem El NiÃ±o einhergeht, den Stillstand irgendwie unterbrechen, gerade pÃ¼nktlich zum Klimagipfel in Paris, obwohl ein nachfolgendes La NiÃ±a den Stillstand wiederherstellen und verlÃ¤ngern dÃ¼rfte.
Zu einer globalen Temperaturspitze aufgrund der thermohalinen Zirkulation, der die warmen Wasser des tropischen Pazifiks um die ganze Welt transportiert, kommt es in einem El NiÃ±o-Jahr gewÃ¶hnlicherweise im nordhemisphÃ¤rischen Winter.
Allerdings entwickelt sich ein oder zwei Jahre nach einem El NiÃ±o normalerweise â€“ aber nicht immer â€“ eine La NiÃ±a, wobei sich zuerst die OberflÃ¤che des Ozeans abkÃ¼hlt und danach die Lufttemperatur, was die globale Temperatur wieder auf ihre Normalwerte bringt.
Abbildung 1a: Wassertemperatur-Index fÃ¼r die Nino 3.4-Region des tropischen Ostpazifiks. Er zeigt einen Anstieg in Richtung einer Spitze, zu der es normalerweise im nordhemisphÃ¤rischen Winter kommt. Vorerst verlÃ¤ngert sich der Stillstand noch weiter, aber nicht mehr lange, und das wÃ¤rmere Wasser im Pazifik wird durch die thermohaline Zirkulation auf der ganzen Welt verbreitet, was zu einer vorÃ¼bergehenden WÃ¤rmespitze der globalen Temperaturen fÃ¼hrt.
Die Stillstandsperiode von 18 Jahren und 8 Monaten ist der am weitesten zurÃ¼ck reichende Zeitraum in den Satelliten-Temperaturaufzeichnungen, den man betrachten kann und der immer noch einen Null-Trend zeigt. Das Anfangsdatum ist nicht cherry picked, es ist berechnet. Und die Graphik bedeutet nicht, dass es so etwas wie eine globale ErwÃ¤rmung nicht gibt. Geht man noch weiter zurÃ¼ck, zeigt sich eine geringe ErwÃ¤rmungsrate.
Der UAH-Datensatz zeigt den Stillstand fast genauso lang wie der RSS-Datensatz. Allerdings zeigen die vielfach verÃ¤nderten Temperatur-DatensÃ¤tze am Boden eine geringe ErwÃ¤rmungsrate.
Abbildung 1b: Der lineare Regressionstrend kleinster Quadrate in der mittleren monatlichen Temperaturanomalie nach GISS, HadCRUT4 und NCDC zeigt eine globale ErwÃ¤rmung mit einer Rate Ã¤quivalent zu etwas Ã¼ber 1Â°C pro Jahrhundert im Zeitraum des Stillstands von Januar 1997 bis Juli 2015.
Wenn man bedenkt, dass ein Drittel des Strahlungsantriebs von 2,4 W/mÂ² aus allen anthropogenen Quellen wÃ¤hrend des Stillstands-Zeitraumes stammt, ist eine ErwÃ¤rmungsrate von etwas mehr als 1Â°C pro Jahrhundert nicht wirklich alarmierend. Allerdings hat es die Studie, die das vermeintliche Fehlen des Stillstands zeigte, sorgfÃ¤ltig vermieden zu erwÃ¤hnen, wie gering die ErwÃ¤rmung in den terrestrischen DatensÃ¤tzen tatsÃ¤chlich ist â€“ selbst nach all den zahlreichen Manipulationen.
Wie immer muss man hier aber aufpassen: Nur weil es wÃ¤hrend der letzten Jahrzehnte keine oder nur eine geringe ErwÃ¤rmung gegeben hat, kann man daraus nicht folgern, dass die ErwÃ¤rmung fÃ¼r immer zum Stillstand gekommen ist. Die Trendlinien messen nur das, was geschehen ist; sie sagen nicht voraus, was geschehen wird.
Der Stillstand â€“ so politisch nÃ¼tzlich er auch fÃ¼r alle sein kann die wÃ¼nschen, dass die â€offizielleâ€œ wissenschaftliche Gemeinschaft seiner Pflicht zum Hinterfragen nachkommt â€“ weit weniger wichtig ist als die immer grÃ¶ÃŸer werdende Diskrepanz zwischen den Prophezeiungen der globalen Zirkulationsmodelle und der beobachteten RealitÃ¤t.
Die Diskrepanz zwischen den Modellprophezeiungen im Jahre 1990 (Abbildung 2) und 2005 (Abbildung 3) einerseits und die tatsÃ¤chlichen Messungen andererseits vergrÃ¶ÃŸert sich immer weiter. Falls sich der Stillstand noch ein wenig weiter fortsetzt, wird die ErwÃ¤rmungsrate in dem Vierteljahrhundert seit dem ersten IPCC-Zustandsbericht aus dem Jahr 1990 unter ein Ã„quivalent von 1Â°C pro Jahrhundert absinken.
Abbildung 2: Echtzeit-Projektionen der ErwÃ¤rmung mit einer Rate Ã¤quivalent zu 2,8 (1,9; 4,2) K pro Jahrhundert, erstellt mit â€substantiellem Vertrauenâ€œ des IPCC im Jahre 1990 fÃ¼r die 307 Monate von Januar 1990 bis Juli 2015 (orangefarbener Bereich und rote Trendlinie) im Vergleich zu gemessenen Anomalien (dunkelblau) und Trend (hellblau) mit einem Ã„quivalent von nur 1 K pro Jahrhundert, gemittelt aus den Satelliten-DatensÃ¤tzen von RSS und UAH.
Abbildung 3: Vorhergesagte TemperaturÃ¤nderung von Januar 2005 bis Juli 2015 mit einer Rate Ã¤quivalent zu 1,7 (1,0; 2,3) K pro Jahrhundert (orangefarbener Bereich mit dicker roter Best Estimate-Trendlinie), verglichen mit den beobachteten, nahe Null liegenden Anomalien (dunkelblau) und Trend (hellblau) der realen Welt, gebildet aus dem Mittel der Temperaturanomalien in der unteren TroposphÃ¤re nach RSS und UAH.
Die Liste der Kernfakten Ã¼ber die globale Temperatur (unten) sollte man jedem vorlegen, der weiter daran glaubt, dass â€“ um es mit den Worten von Mr. Obamas Twitterei auszurÃ¼cken â€“ â€die globale ErwÃ¤rmung real, vom Menschen verursacht und gefÃ¤hrlichâ€œ ist.
In HintergrÃ¼nde werden die Quellen der IPCC-Vorhersagen aus den Jahren 1990 und 2005 erlÃ¤utert. Auch wird gezeigt, dass sich die Ozeane den ARGO-Daten zufolge mit einer Rate Ã¤quivalent zu weniger als 0,25Â°C pro Jahrhundert erwÃ¤rmen.
Kernfakten zur globalen Temperatur
â— Der RSS-Satelliten-Datensatz zeigt keinerlei globale ErwÃ¤rmung seit 223 Monaten von Januar 1997 bis Juli 2015 â€“ also wÃ¤hrend Ã¼ber der HÃ¤lfte der 439 Monate langen Satelliten-Aufzeichnungen.
â— Es gab keine ErwÃ¤rmung, obwohl ein Drittel aller anthropogenen Antriebe seit 1750 erst nach Januar 1997 freigesetzt wurden, also wÃ¤hrend des Stillstands der globalen ErwÃ¤rmung.
â— Der gesamte RSS-Datensatz von Januar 1979 bis heute zeigt eine globale ErwÃ¤rmung mit einer wirklich nicht alarmierenden Rate von lediglich 1,2Â°C pro Jahrhundert.
â— Seit 1950, also seit dem Zeitpunkt, an dem ein menschlicher Einfluss auf das Klima zum ersten Mal theoretisch mÃ¶glich ist, lag der Trend der globalen ErwÃ¤rmung unter einem Ã„quivalent von 1,2Â°C pro Jahrhundert.
â— Der globale ErwÃ¤rmungstrend seit dem Jahr 1900 ist Ã¤quivalent zu 0,75Â°C pro Jahrhundert. Dies liegt deutlich innerhalb der natÃ¼rlichen VariabilitÃ¤t und dÃ¼rfte mit uns nicht viel zu tun haben.
â— Zur hÃ¶chsten ErwÃ¤rmungsrate Ã¼ber 15 Jahre oder mehr seit 1950 kam es wÃ¤hrend der 33 Jahre von 1974 bis 2006. Die Rate lag Ã¤quivalent zu 2,0Â°C pro Jahrhundert.
â— Zum Vergleich, die ErwÃ¤rmung im Temperatur-Datensatz aus Mittelengland wÃ¤hrend der 40 Jahre von 1694 bis 1733, also lange vor der industriellen Revolution, erreichte ein Ã„quivalent zu 4,33Â°C pro Jahrhundert.
â— Im Jahre 1990 lag die Mittelfrist-Prognose des IPCC Ã¤quivalent zu 2,8Â°C pro Jahrhundert, also um zwei Drittel hÃ¶her als seine jÃ¼ngste Prognose von 1,7Â°C pro Jahrhundert.
â— Der ErwÃ¤rmungstrend seit 1990, dem Jahr des 1. IPCC-Zustandsberichtes, ist Ã¤quivalent zu 1Â°C pro Jahrhundert. Das IPCC hat eine zweieinhalb mal so hohe Rate prophezeit.
â— Damit die zentrale Prognose des IPCC von einer ErwÃ¤rmung um 1Â°C von 1990 bis 2025 doch noch eintrifft, mÃ¼sste es im nÃ¤chsten Jahrzehnt zu einer ErwÃ¤rmung von 0,75Â°C kommen, also einem Ã„quivalent zu 7,5Â°C pro Jahrhundert.
â— Obwohl das IPCC seine ErwÃ¤rmungsprognose im Kurzfristzeitraum zurÃ¼ckgefahren hat, sagt es langfristig bis zum Jahr 2100 bei Business as Usual immer noch eine ErwÃ¤rmung um 4,8Â°C voraus.
â— Die vom IPCC vorhergesagte ErwÃ¤rmung um 4,8Â°C bis zum Jahr 2100 liegt deutlich mehr als zweimal so hoch wie die hÃ¶chste ErwÃ¤rmungsrate Ã¼ber mehr als 15 Jahre, die seit dem Jahr 1950 gemessen worden ist.
â— Die IPCC-Vorhersage von 4,8Â°C bis 2100 liegt viermal so hoch wie der gemessene globale ErwÃ¤rmungstrend, seit wir diesen theoretisch hÃ¤tten beeinflussen kÃ¶nnen, also seit 1950.
â— Die Ozeane erwÃ¤rmen sich den ARGO-Bojen zufolge mit einer Rate von lediglich 0,02Â°C pro Jahrzehnt, Ã¤quivalent zu 0,23Â°C pro Jahrhundert oder 1Â°C in 430 Jahren.
HintergrÃ¼nde
Unsere jÃ¼ngste Graphik zeigt die lineare Regression kleinster Quadrate des Trends der monatlichen globalen mittleren Temperaturen, der so weit zurÃ¼ckreicht, wie es mÃ¶glich ist und immer noch einen Null-Trend findet. Das Anfangsdatum ist nicht cherry picked, um mit der Temperaturspitze des El NiÃ±o des Jahres 1998 zusammenzufallen. Stattdessen ist er berechnet, um den lÃ¤ngsten Zeitraum mit einem Null-Trend zu finden.
Die Tatsache des langen Stillstands ist ein Hinweis auf die sich stetig vergrÃ¶ÃŸernde Diskrepanz zwischen Prophezeiung und Wirklichkeit bei den Temperaturmessungen.
Die Satelliten-DatensÃ¤tze sind allgemein deutlich weniger unzuverlÃ¤ssig als andere DatensÃ¤tze, zeigen sie doch den GroÃŸen El NiÃ±o klarer als alle anderen DatensÃ¤tze. Dieses Ereignis verursachte wie seine beiden VorgÃ¤nger wÃ¤hrend der letzten 300 Jahre eine verbreitete Korallenbleiche, was eine unabhÃ¤ngige Verifikation erlaubte, dass Satelliten-DatensÃ¤tze mehr als alle anderen in der Lage sind, derartige Fluktuationen ohne kÃ¼nstliches Herausfiltern abzubilden.
Terrestrische Temperaturen werden mit Thermometern gemessen. Korrekt in lÃ¤ndlichen Gebieten aufgestellte Thermometer abseits von menschlichen WÃ¤rmequellen zeigen ErwÃ¤rmungsraten, die unter jenen liegen, die verÃ¶ffentlicht werden. Die Satelliten-DatensÃ¤tze basieren auf Referenzmessungen mittels der genauesten derzeit verfÃ¼gbaren Thermometer â€“ Platin-Widerstands-Thermometer, welche eine unabhÃ¤ngige Verifikation der Temperaturmessungen ermÃ¶glichen, indem man mittels ins Weltall gerichteter Spiegel die kosmische Hintergrundstrahlung heranzieht, die etwa 1% des Gefrierpunktes von Wasser betrÃ¤gt oder einfach 2,73 Grad Ã¼ber dem Absoluten Nullpunkt liegt. Es waren Messungen von minimalen Variationen dieser kosmischen Hintergrundstrahlung, die es der NASA ermÃ¶glichten, das Alter des Universums zu berechnen: 13,82 Milliarden Jahre.
Die RSS-Graphik (Abbildung oben rechts) ist akkurat. Die Daten wurden monatlich direkt von der RSS-Website heruntergeladen. Ein Computer-Algorithmus liest sie aus dem File heraus und plottet sie automatisch mittels einer fortschrittlichen Routine, die automatisch das Bildformat des Datenfensters an beide Achsen anpasst, um die Daten der Klarheit halber im maximal mÃ¶glichen MaÃŸstab zu zeigen.
Der letzte monatliche Datenpunkt wird visuell untersucht um sicherzustellen, dass er korrekt positioniert ist. Die hellblaue, Ã¼ber die gezackte dunkelblaue Linie gelegte Trendlinie zeigt die Daten nach der linearen Regression kleinster Quadrate sowie die den Schnittpunkt mit der Y-Achse und die Neigung der Linie.
Das IPCC und die meisten anderen Agenturen verwenden lineare Regression, um globale Temperaturtrends zu berechnen. Prof. Phil Jones von der University of East Anglia verlangt dies in einer der Klimagate-E-Mails. Das Verfahren ist angemessen, weil die globalen Temperaturaufzeichnungen nur wenig Autoregression zeigen, werden doch die Sommertemperaturen auf der einen HemisphÃ¤re durch die Wintertemperaturen auf der anderen HemisphÃ¤re kompensiert. Daher wÃ¼rde ein AR(n)-Modell Ergebnisse erzeugen, die sich ein wenig vom Trend der kleinsten Quadrate unterscheiden.
Dr. Stephen Farish, Professor fÃ¼r epidemiologische Statistik an der University of Melbourne hat freundlicherweise die ZuverlÃ¤ssigkeit des Algorithmusâ€˜ verifiziert, mit dem der Trend in der Graphik und der Korrelationskoeffizient berechnet werden. Dieser ist sehr klein, und obwohl die Daten hoch variabel sind, verlÃ¤uft der Trend flach.
Bei RSS selbst ist jetzt ernsthaftes Interesse an der LÃ¤nge des GroÃŸen Stillstands aufgekommen. Dr. Carl Mears, der leitende Forschungswissenschaftler bei RSS, diskutiert dieses hier.
Die Ergebnisse von Dr. Mears werden in Abbildung T1 zusammengefasst:
Abbildung T1: Output von 33 IPCC-Modellen (tÃ¼rkis) im Vergleich mit der globalen TemperaturÃ¤nderung nach RSS (schwarz) von1979 bis 2014. Die vorÃ¼bergehenden AbkÃ¼hlungen durch die VulkanausbrÃ¼che des El Chichon (1983) und des Pinatubo (1991) zeichnen sich ab, ebenso wie die WÃ¤rmespitze des GroÃŸen El Nino von 1998.
Dr. Mears schreibt:
â€Die Leugner mÃ¶gen es zu vermuten, dass der Grund fÃ¼r die Diskrepanz zwischen Modellen und Beobachtungen eine Art Problem mit der zugrunde liegenden Modellphysik ist, und sie weisen alle anderen Formen der ErklÃ¤rung zurÃ¼ck. Dies verleitet sie zu der Schlussfolgerung, sehr wahrscheinlich irrig, dass die langfristige SensitivitÃ¤t des Klimas viel geringer ist als ursprÃ¼nglich gedachtâ€œ.
Dr. Mears rÃ¤umt die wachsende Diskrepanz zwischen RSS-Daten und den Modellen ein, fÃ¼hrt diese allerdings auf Rosinenpickerei des Anfangsdatums der Graphik der globalen Temperatur zurÃ¼ck:
â€JÃ¼ngst war in einer Anzahl von Artikeln der Mainstream-Presse zu lesen, dass es nur eine geringe oder gar keine Ã„nderung der global gemittelten Temperatur wÃ¤hrend der letzten beiden Jahrzehnte gegeben zu haben scheint. Deswegen stellt man uns viele Fragen mit dem Tenor ;Ich habe diese Graphik auf einer Leugner-Website gesehen. Sind das wirklich Ihre Daten?â€˜ WÃ¤hrend einige dieser Berichte den Endzeitpunkt ,cherry-pickedâ€˜ haben, um ihre Beweise noch stÃ¤rker aussehen zu lassen, gibt es kaum einen Zweifel daran, dass die ErwÃ¤rmungsrate seit Ende der neunziger Jahre geringer ist als von den meisten IPCC-Simulationen des historischen Klimas im AR 5 vorhergesagt. â€¦ Die Leugner mÃ¶gen es wirklich, die Trends im Jahre 1997 beginnen zulassen, so dass das gewaltige ENSO-Ereignis von 1997/1998 am Beginn ihrer Zeitreihe liegt, was zu einer linearen Anpassung mit der kleinstmÃ¶glichen Neigung fÃ¼hrtâ€œ.
TatsÃ¤chlich wird die vom GroÃŸen El NiÃ±o 1998 verursachte Temperaturspitze aus zwei GrÃ¼nden fast vollstÃ¤ndig bei der Berechnung des linearen Trends ausgeglichen: dem Fehlen einer Ã¤hnlichen Spitze beim El NiÃ±o 2010 und der schieren LÃ¤nge des Stillstands selbst.
[Was ist das fÃ¼r ein â€Wissenschaftlerâ€œ, der alle anderen als â€Leugnerâ€œ bezeichnet, die lediglich zu anderen Erkenntnissen gekommen sind als er selbst? Anm. d. Ãœbers.]
Komischerweise bevorzugt Dr. Mears die terrestrischen DatensÃ¤tze vor den Satelliten-DatensÃ¤tzen. Das UK Met.-Office jedoch verwendet die Satellitendaten, um seine eigenen terrestrischen Daten zu kalibrieren.
Die LÃ¤nge des Stillstands der globalen ErwÃ¤rmung, so signifikant sie auch daherkommt, ist weit weniger wichtig als die stetig und rasch weiter zunehmende Diskrepanz zwischen den von den Modellen prophezeiten Temperaturtrends und der weit weniger ausgeprÃ¤gten TemperaturÃ¤nderung der realen Welt. Der El NiÃ±o dieses Jahres kann sich durchaus noch in diesem Jahr verstÃ¤rken, was die LÃ¤nge des GroÃŸen Stillstands reduziert. Allerdings setzt sich die Verbreiterung des Grabens zwischen Prophezeiung und Beobachtung weiter fort.
Quellen der IPCC-Projektionen in den Abbildungen 2 und 3.
Im Ersten Zustandsbericht des IPCC wurde prophezeit, dass die Temperatur bis zum Jahr 2025 um 1,0 Â°C (0,7; 1,5Â°C) steigen wÃ¼rde, was Ã¤quivalent ist zu einer Rate von 2,8Â°C (1,9; 4.2Â°C) pro Jahrhundert. In der Executive Summary wurde die rhetorische Frage gestellt: â€Wie viel Vertrauen haben wir in unsere Prophezeiungen?â€œ Das IPCC verwies auf einige Unsicherheiten (Wolken, Ozeane usw.), kam aber dennoch zu dem Ergebnis:
â€Nichtsdestotrotz haben wir substantielles Vertrauen, dass die Modelle zumindest die groben Umrisse des Klimawandels prophezeien kÃ¶nnen â€¦ Es gibt Ã„hnlichkeiten zwischen Ergebnissen der gekoppelten Modelle mittels einfacher ReprÃ¤sentationen des Ozeans und jenen komplizierteren Beschreibungen, und unser VerstÃ¤ndnis der Differenzen, die es auch gibt, bestÃ¤rkt uns in dem Vertrauen in unsere Ergebnisseâ€œ.
Dieses â€substantielle Vertrauenâ€œ war substantielles Ãœber-Vertrauen. Und zwar weil die Rate der globalen ErwÃ¤rmung seit 1990 â€“ die wichtigsten â€groben Umrisse des Klimawandelsâ€œ, die die Modelle vermeintlich prophezeien sollten â€“ inzwischen weniger als halb so groÃŸ ist wie vom IPCC damals prophezeit.
Im Jahre 1990 schrieb das IPCC Folgendes:
â€Auf der Grundlage der gegenwÃ¤rtigen Modelle prophezeien wir:
Bei Treibhausgas-Emissionen unter dem Szenario A (Business as Usual) gibt es eine Rate der Zunahme der globalen mittleren Temperatur wÃ¤hrend des nÃ¤chsten Jahrhunderts um etwa 0,3Â°C pro Jahrzehnt (mit einer Unsicherheits-Bandbreite von 0,2Â°C bis 0,5Â°C pro Jahrzehnt). Dies ist grÃ¶ÃŸer als wÃ¤hrend der letzten 10.000 Jahre aufgetreten. Dies wird zu einer wahrscheinlichen ErwÃ¤rmung um etwa 1Â°C Ã¼ber dem jetzigen Wert bis zum Jahr 2025 und um 3Â°C vor dem Ende des nÃ¤chsten Jahrhunderts fÃ¼hren. Der Anstieg wird nicht gleichmÃ¤ÃŸig verlaufen infolge des Einwirkens anderer Faktoren (S. Xii).
SpÃ¤ter hieÃŸ es vom IPCC:
Die Zahlen unten basieren auf hoch auflÃ¶senden Modellen, skaliert, um mit unserer Best Estimate der globalen mittleren ErwÃ¤rmung von 1,8Â°C bis zum Jahr 2030 konsistent zu sein. FÃ¼r Werte, die konsistent mit anderen SchÃ¤tzungen des globalen Temperaturanstiegs sind, sollten die Zahlen unten um 30% reduziert werden fÃ¼r die niedrige SchÃ¤tzung oder um 50% erhÃ¶ht werden fÃ¼r die hohe SchÃ¤tzung (S. xxiv).
Der orangefarbene Bereich in Abbildung 2 reprÃ¤sentiert die mittelfristige IPCC-SchÃ¤tzung unter Szenario A einer kurzfristigen ErwÃ¤rmung, d. h. 1,0Â°C (0,7; 1,5) bis zum Jahr 2025.
Die vom IPCC prophezeite globale ErwÃ¤rmung wÃ¤hrend der 25 Jahre ab 1990 bis zur Gegenwart differiert wenig um eine gerade Linie (Abbildung T2)
Abbildung T2: Historische ErwÃ¤rmung von 1850 bis 1990 sowie prophezeite ErwÃ¤rmung von 1990 bis 2100 unter dem IPCC-Szenario A (IPCC 1990, S. xxii)
Weil diese Differenz zwischen einer geraden Linie und dem leichten Sprung aufwÃ¤rts der von IPCC im Zeitraum 1990 bis 2025 prophezeiten ErwÃ¤rmungsrate so klein ist, kann man das noch in anderer Weise betrachten. Um die zentrale SchÃ¤tzung von 1 K seit 1990 bis 2025 zu erreichen, mÃ¼sste es eine doppelt so starke ErwÃ¤rmung wÃ¤hrend der nÃ¤chsten zehn Jahre geben, als es wÃ¤hrend der letzten 25 Jahre der Fall war. Das ist unwahrscheinlich.
Aber ist der Stillstand vielleicht der Tatsache geschuldet, dass CO2-Emissionen nicht annÃ¤hernd so stark gestiegen sind wie die IPCC-Prophezeiung unter Szenario A im Jahre 1990 benannt? Nein: Die CO2-Emissionen sind ziemlich deutlich Ã¼ber die unter Szenario A prophezeite Menge gestiegen (Abbildung T3)
Abbildung T3: CO2-Emissionen durch fossile Treibstoffe usw. im Jahre 2012 aus Le QuÃ©rÃ© et al. (2014) im Vergleich zur Karte der â€anthropogenen Kohlendioxid-Emissionenâ€œ in Milliarden Tonnen Kohlenstoff pro Jahr. Quelle: IPCC 1990
Daher haben sich die CO2-Emissionen seit 1990 als viel nÃ¤her Szenario A entsprechend erwiesen als an jedes andere Szenario, weil trotz all des Geredes um Reduktionen von CO2-Emissionen die Tatsache besteht, dass die Rate der Ausweitung des Verbrennens fossiler Treibstoffe in China, Indonesien, Indien, Brasilien usw. die geringen Reduktionen in westlichen LÃ¤ndern bis heute weit in den Schatten stellen.
Es stimmt schon, die Methankonzentration ist nicht so gestiegen wie im Jahre 1990 prophezeit, weil die Methan-Emissionen, obwohl weitgehend ungesteuert, einfach nicht so steigen, wie die Modelle prophezeit hatten. Auch hier war also die Grundlage aller Prophezeiungen abwegig.
Das Gesamtbild ist klar. Szenario A ist das Emissions-Szenario aus dem Jahr 1990, dass der Menge der beobachteten CO2-Emissionen am NÃ¤chsten liegt.
Abbildung T4: Methan-Konzentration wie in vier IPCC-Zustandsberichten prophezeit, zusammen mit der beobachteten Konzentration, welche sich entlang der unteren Grenze der Geringst-Prophezeiung bewegt. Diese Graphik erschien in der vorlÃ¤ufig-endgÃ¼ltigen Version von IPCC (2013), wurde jedoch auf mysteriÃ¶se Weise aus der verÃ¶ffentlichten Endversion entfernt, vermutlich weil das IPCC einen solchen direkten Vergleich zwischen absurd Ã¼bertriebenen Prophezeiungen und der wenig aufregenden Wirklichkeit nicht zeigen wollte.
Um genau zu sein, ein Vierteljahrhundert nach 1990 betrÃ¤gt die globale ErwÃ¤rmung bis jetzt 0,27Â°C, was Ã¤quivalent zu etwa Ã¼ber 1Â°C pro Jahrhundert ist â€“ wenn man es ausdrÃ¼ckt als linearer Regressionstrend kleinster Quadrate des Mittels von RSS und UAH der monatlichen mittleren globalen Temperaturanomalien. Die zentrale SchÃ¤tzung des IPCC von 0,71Â°C, also Ã¤quivalent zu 2,8Â°C pro Jahrhundert, welche unter Szenario A im Jahre 1990 mit â€substantiellem Vertrauenâ€œ prophezeit worden war, war dreimal zu hoch gegriffen. TatsÃ¤chlich liegt die tatsÃ¤chliche Rate sichtlich deutlich unter selbst der kleinsten SchÃ¤tzung.
Im Jahre 1990 war die zentrale Prophezeiung der kurzfristigen ErwÃ¤rmungsrate des IPCC um zwei Drittel hÃ¶her als dessen heutige Prophezeiung. Damals war es ein Ã„quivalent von 2,8Â°C pro Jahrhundert, heute ist es ein solches von 1,7Â°C pro Jahrhundert â€“ und wie Abbildung T5 zeigt, hat sich selbst das als eine substantielle Ãœbertreibung erwiesen.
ErwÃ¤rmt sich der Ozean?
Eine oft herangezogene ErklÃ¤rung des GroÃŸen Stillstands lautet, dass das gekoppelte Ozean-AtmosphÃ¤re-System weiterhin WÃ¤rme akkumuliert hat mit etwa der von den Modellen prophezeiten Rate, aber das die WÃ¤rme wÃ¤hrend der letzten Jahrzehnte durch die Ozeane aus der AtmosphÃ¤re herausgefiltert worden ist. Und man hypothetisiert angesichts des Umstands, das die oberflÃ¤chennahen Schichten weit weniger ErwÃ¤rmung zeigen als von den Modellen prophezeit, dass das, was man die â€fehlende WÃ¤rmeâ€œ nennt, irgendwie in die kaum vermessenen Tiefen unter 2000 m diffundiert ist, von wo sie irgendwann in Zukunft wieder freigesetzt wird.
TatsÃ¤chlich ist unbekannt, ob sich der Ozean erwÃ¤rmt: jede der automatisierten ARGO-Bojen fÃ¼hrt nur drei Messungen pro Monat durch fÃ¼r eine Wassermenge von 200.000 kmÂ³ â€“ grob etwa ein 100.000 Quadratmeilen [ca. 259.000 kmÂ²] groÃŸes Gitterquadrat mit einer KantenlÃ¤nge von 316 km und einer Tiefe von 2000 m. Im Klartext, die Ergebnisse einer so geringen AuflÃ¶sung sind nicht viel besser als Raten. (Willis Eschenbach drÃ¼ckt es so aus: es ist etwa das Ã„quivalent zu versuchen, eine einzelnes Temperatur- und Salzgehalts-Profil an einem einzigen Punkt im Oberen See einmal pro Jahr zu erstellen).
UnglÃ¼cklicherweise scheint es, als ob ARGO den Ozean-Datensatz seit Dezember 2014 nicht mehr aktualisiert hat. Allerdings haben wir 11 ganze Jahre mit Daten. Die Ergebnisse zeigt Abbildung T5. Falls die ARGO-Messungen richtig sind, ist die ErwÃ¤rmung der Ozeane Ã¤quivalent zu 0,2Â°C pro Jahrhundert.
Abbildung T5: Der gesamte, fast globale ARGO-Datensatz der Wassertemperatur der Ozeane in den oberen 2000 m von Januar 2004 bis Dezember 2014 (schwarze gezackte Kurve), zusammen mit dem linearen Regressionstrend kleinster Quadrate, berechnet aus den Daten vom Autor (grÃ¼ner Pfeil).
Und schlieÃŸlich, obwohl die ARG-Bojen die TemperaturÃ¤nderung in den Ozeanen direkt messen, Ã¤ndert die NOAA die TemperaturÃ¤nderung kÃ¼nstlich in Zetajoule der Ã„nderung des ozeanischen WÃ¤rmegehaltes, wodurch die Ã„nderung viel grÃ¶ÃŸer aussieht.
Die erschreckend klingende Ã„nderung des WÃ¤rmegehaltes von 260 ZJ im Zeitraum 1970 bis 2014 (Abbildung T6) ist Ã¤quivalent zu lediglich 0,2 K pro Jahrhundert globaler ErwÃ¤rmung. All jene â€Hiroshima-Bomben von WÃ¤rmeâ€œ, von denen die extremistischen Klima-Websites reden, sind in Wirklichkeit ein kaum wahrnehmbarer Nadelstich. Der Ozean und dessen WÃ¤rmekapazitÃ¤t sind viel grÃ¶ÃŸer als so Mancher wahrhaben will.
Abbildung T6: Ã„nderung des ozeanischen WÃ¤rmegehaltes von 1957 bis 2013 in Zetajoules vom NODC Ocean Climate Lab der NOAA: http://www.nodc.noaa.gov/OC5/3M_HEAT_CONTENT, wobei die Werte des WÃ¤rmegehaltes zurÃ¼ck konvertiert wurden zu den Ã„nderungen der Wassertemperatur in den Ozeanen in Kelvin, wie sie ursprÃ¼nglich gemessen worden sind. Die Konversion der NOAA der minimalen ErwÃ¤rmungszahlen in Zetajoule, kombiniert mit einer Ã¼bertriebenen vertikalen Achse der Graphik, hat die Auswirkung, dass eine sehr kleine Ã„nderung der Wassertemperatur viel signifikanter aussieht als sie ist.
Die RÃ¼ck-Konversion der Ã„nderung des ozeanischen WÃ¤rmegehaltes in TemperaturÃ¤nderungen enthÃ¼llt eine interessante Diskrepanz zwischen den NOAA-Daten und denen des ARGO-Systems. WÃ¤hrend des Zeitraumes der ARGO-Daten von 2004 bis 2014 implizieren die NOAA-Daten eine ErwÃ¤rmung der Ozeane um 0,05Â°C pro Jahrzehnt, Ã¤quivalent zu 0,5Â°C pro Jahrhundert â€“ oder eine fast doppelt so hohe Rate wie von ARGO gemessen.
Zwar hat ARGO den besser aufgelÃ¶sten Datensatz, aber da die AuflÃ¶sung aller Ozean-DatensÃ¤tze sehr gering ist, muss man all diese Ergebnisse mit Vorsicht betrachten. Was man sagen kann ist, dass aufgrund solcher Beweise, wie sie diese DatensÃ¤tze erbringen kÃ¶nnen, die Differenz zwischen dem zugrunde liegenden ErwÃ¤rmungstrend des Ozeans und dem der AtmosphÃ¤re nicht statistisch signifikant ist. Dies zeigt, dass falls sich die â€fehlende WÃ¤rmeâ€œ wirklich in den Ozeanen versteckt, sie auf magische Weise den Weg in tiefe Schichten gefunden hat, ohne die oberen Schichten auf ihrem Weg zu erwÃ¤rmen. Ebenfalls aufgrund dieser Daten gibt es keine Beweise fÃ¼r eine rapide oder katastrophale ErwÃ¤rmung der Ozeane.
AuÃŸerdem gibt es bis heute keine empirische, theoretische oder numerische Methode, komplex oder einfach, die erfolgreich mechanistisch spezifiziert hat, wie entweder die durch die anthropogene Anreicherung der AtmosphÃ¤re mit Treibhausgasen verursachte ErwÃ¤rmung die Tiefsee erreicht hat, ohne den WÃ¤rmegehalt der durchdrungenen oberen Ozeanschichten wesentlich zu verÃ¤ndern, oder wie die WÃ¤rme vom Grund der Ozeane eventuell irgendwie wieder die Klimabedingungen nahe der OberflÃ¤che beeinflussen kann, die fÃ¼r das Leben auf dem Festland der Erde relevant sind.
Die meisten Ozean-Modelle, die zur Berechnung der gekoppelten allgemeinen Modell-SensitivitÃ¤t herangezogen worden sind, kÃ¶nnen die meisten der physikalischen Prozesse nicht auflÃ¶sen, die fÃ¼r die WÃ¤rmeaufnahme durch die Ozeane relevant sind. Ultimativ verlangt der Zweite Hauptsatz der Thermodynamik, dass sich jedwede in der Tiefsee akkumulierte WÃ¤rme via verschiedener Diffusionsprozesse auflÃ¶sen muss. Es ist nicht plausibel, dass irgendwelche durch die Tiefsee aufgenommenen WÃ¤rmemengen plÃ¶tzlich die oberen Ozeanschichten erwÃ¤rmen und damit auch die AtmosphÃ¤re.
Falls die â€Tiefseeâ€œ-ErklÃ¤rung fÃ¼r den Stillstand korrekt wÃ¤re (und diese ist lediglich eine von Dutzenden Anderen, die prÃ¤sentiert worden sind), haben die komplexen Modelle dabei versagt, dies korrekt abzubilden: anderenfalls wÃ¤re die wachsende Diskrepanz zwischen den prophezeiten und beobachteten atmosphÃ¤rischen ErwÃ¤rmungsraten nicht so signifikant geworden, wie es der Fall ist.
Warum waren die Modell-Prophezeiungen Ã¼bertrieben?
Im Jahre 1990 prophezeite das IPCC â€“ unter seinem Szenario A (Business as Usual) â€“ dass es vom Beginn der industriellen Revolution an bis zur Gegenwart einen Strahlungsantrieb von 4 Watt pro Quadratmeter geben wÃ¼rde, der durch den Menschen verursacht ist (Abbildung T7):
Abbildung T7: prophezeite, vom Menschen verursachte Strahlungsantriebe (IPCC 1990).
Allerdings hat sich das IPCC von 1995 an entschlossen, aufgrund dÃ¼rftiger Beweise zu vermuten, dass anthropogene Aerosole â€“ meist RuÃŸ durch Verbrennung â€“ die Erde abschattet, und zwar stark genug, um einen starken negativen Strahlungsantrieb auszulÃ¶sen. Auch hat es inzwischen verspÃ¤tet erkannt, dass seine projizierten Zunahmen der Methan-Konzentration wilde Ãœbertreibungen waren. Als Folge dieser und anderer Ã„nderungen schÃ¤tzt es jetzt, dass der gesamt-anthropogene Antrieb der industriellen Ã„ra lediglich 2,3 W/mÂ² betrÃ¤gt oder etwas weniger als halb so groÃŸ ist wie laut dessen Prophezeiung im Jahre 1990.
Abbildung T8: Gesamt-anthropogene Antriebe von 1750 bis 1950, 1980 und 2012 (IPCC 2013).
Allerdings kÃ¶nnte selbst das noch eine erhebliche Ãœbertreibung sein, betrÃ¤gt doch die Best Estimate des tatsÃ¤chlichen gegenwÃ¤rtigen Strahlungs-Ungleichgewichtes an der Obergrenze der AtmosphÃ¤re (TOA) lediglich 0,6 W/mÂ² (gesamter natÃ¼rlicher und anthropogener Antrieb); Abbildung T9:
Abbildung T9: Energiebudget-Diagramm fÃ¼r die Erde nach Stephens et al. (2012).
Kurz gesagt, der grÃ¶ÃŸte Anteil des vom IPCC prophezeiten Antriebs ist entweder eine Ãœbertreibung oder hat bereits zu der TemperaturÃ¤nderung gefÃ¼hrt, welcher auch immer, die er verursachen sollte. Es gibt nur wenig globale ErwÃ¤rmung in der Pipeline als Folge unserer EmissionssÃ¼nden in Vergangenheit und Gegenwart.
Es kann auch sein, dass das IPCC und die Modelle unablÃ¤ssig die KlimasensitivitÃ¤t Ã¼bertrieben haben. Eine jÃ¼ngst erschienene Studie zu dieser Frage von Monckton of Brenchley et al. (2015), kam auf eine KlimasensitivitÃ¤t im Bereich von 1Â°C pro Verdoppelung des CO2-Gehaltes (man gehe zu www.scibull.com und klicke dort auf â€Most Read Articlesâ€œ). Die Studie identifizierte Fehler in der Art und Weise, wie die Modelle mit Temperatur-RÃ¼ckkopplungen umgegangen waren, sowie deren VerstÃ¤rkung, welche fÃ¼r zwei Drittel der Gleichgewichts-ErwÃ¤rmung verantwortlich waren, die das IPCC prophezeit hatte.
Prof. Ray Bates hat jÃ¼ngst in Moskau eine Studie verÃ¶ffentlicht, in der er auf der Grundlage der Analyse von Lindzen & Choi (2009, 2011) zu dem Ergebnis gekommen war (Abbildung T10), dass Temperatur-RÃ¼ckkopplungen insgesamt negativ sind. Demzufolge hat er die Schlussfolgerungen sowohl von Lindzen & Choi (1990) als auch von Spencer & Braswell (2010, 2011) bestÃ¤tigt, denen zufolge die KlimasensitivitÃ¤t unter â€“ und vielleicht betrÃ¤chtlich unter â€“ 1Â°C pro CO2-Verdoppelung liegt.
Abbildung T10: RealitÃ¤t (Mitte) im Vergleich zu 11 Modellen. Aus Lindzen & Choi (2009).
Eine zunehmende Anzahl begutachteter Studien kommt zu dem Ergebnis, dass die KlimasensitivitÃ¤t deutlich unter den 3Â°C (1,5; 4,5) pro CO2-Verdoppelung liegt, die zum ersten Mal im Charney-Report aus dem Jahr 1979 fÃ¼r die Academy of Sciences in den USA kolportiert worden war. Hierbei handelt es sich bis heute um die Best Estimate des IPCC.
Auf der Grundlage der bis heute vorliegenden Beweise gibt es daher keine wissenschaftliche Basis, Ã¼berhaupt irgendwelche MaÃŸnahmen zu ergreifen, um die CO2-Emissionen abzuschwÃ¤chen.
Und zum Schluss: Wie lange wird es dauern, bevor die Freiheits-Uhr 20 Jahre ohne jede globale ErwÃ¤rmung erreicht? Falls das geschieht, wird die Klimaangst unhaltbar werden.
Ãœbersetzt von Chris Frey EIKE
Link: http://wattsupwiththat.com/2015/09/04/the-pause-lengthens-yet-again/
MfG
Ich finde es zunÃ¤chst mal gut, dass Stephens et al. 2012 die experimentellen Unsicherheiten deutlich machen.
Das IPCC hingegen hat sich dabei auf Wild et al 2013 http://tinyurl.com/nstnwrf verlassen. Die bekommen geringere Unsicherheiten raus. Etwa 6 W/m2.
Indem sie mit dem SB-Gestz und einem epsilon kalkulieren, dass ich in ihre Paper nicht gefunden habe. Trenberth und Kiehl benutzten 1.
Fehler durch die Ungenauigkeiten in der Bestimmung von epsilon, werden sehr oberflÃ¤chlich weggebÃ¼gelt.
â€Some uncertainty might be introduced in the determination of the upward thermal flux through the specification of the sur- face emissivity epsilon, a value close to 1. However, this flux is not overly sensitive to the exact choice of epsilon, since, if epsilon is chosen to be lower than 1, the associated reduction in the upward thermal flux is largely compensated for by an additional upward component, stemming from the non- absorbed (upward reflected) part of the downward thermal radiation.â€œ
Das IPCC ignoriert also die experimentellen Unsicherheiten und nimmt statt dessen kÃ¼nstliche Werte. Das zeigt wie oberflÃ¤chlich da gearbeitet wird.
Es ist also aus Stephens et al. 2012 bekannt, dass die experimentellen Unsicherheiten im Bereich der OberflÃ¤chenenergiebilanz bei 10 â€“ 20 W/m2, aber das IPCC nimmt lieber eine Kalkulation aus den CMIP5 Modellen statt das Experiment, Wild et al 2013 die eine Unsicherheit von 6 W/m2 vorgaukeln.
Zitat: Da muss man verdammt aufpassen was Messung und was Rechnung ist.
Ja, das ist richtig.
Das Problem von ERBE (Earth Radiation Budget Experiment) und CERES (Clouds and the Earthâ€™s Radiant Energy System) ist, dass die Messgenauigkeit nicht in dem Bereich liegt, der eigentlich erforderlich wÃ¤re, um genaue Aussagen Ã¼ber die Energiebilanzen zu erhalten. Aus den Daten werden dann irgend welche Antriebe berechnet, die im grunde gar nicht existent sind.
Ein Problem der ganzen Messungen ist die Tatsache, dass die Strahldichten innerhalb des Gesichtsfeldes inhomogen (Fresnel-Reflexion) und verschieden spektral verteilt (Interferenz) sind, je nach Bereich der Erde. Dies bedeutet, dass eine iterative Methode zur Bestimmung der wirklichen Signale notwendig ist. Es sind zwar 12 Bereichs-Faktoren (Land, Ozean, bewÃ¶lkt usw.) definiert, diese sind aber viel zu grob gewÃ¤hlt.
An den ganzen Energiebilanzen-Schemas stÃ¶rt mich, dass diese â€Bildchenâ€œ in keinster Weise die â€grobeâ€œ RealitÃ¤t abbilden. Mit relativ einfachen physikalischen Vergleichen kÃ¶nnten sie diese â€Bildchenâ€œ in das Reich der Mythen und Fabeln verbannen.
Mfg
Werner Holtz
â€Stephens et al. (2012) verwenden die puren Messwerte von ERBE/CERES. Da wird nichts physikalisch umgerechnet oder die Gegebenheiten andere Messwerte berÃ¼cksichtigt, und der Emissionsgrad von e=0,96 gilt fÃ¼r die terrestrische IR-Strahlung.â€œ
Lieber Holtz,
ich habe mich vor einiger Zeit mit den Ceres -Daten beschÃ¤ftigt. Da muss man verdammt aufpassen was Messung und was Rechnung ist. Z.B. ergeben sich die auf die ErdoberflÃ¤che einfallenden und emittierten EnergieflÃ¼sse aus Modell-Rechnungen. Vergleichen Sie mal die sich die aus dem von der ErdoberflÃ¤che emittierten IR-Energiefluss ergebenden Temperaturen mit den z. B. von HADCRUT verÃ¶ffentlichten Temperaturen ortsaufgelÃ¶st.
â€Reflektierte Strahlung kann nicht absorbiert bzw. emittiert werden, sie mindert den Energieumsatz.â€œ
Sehr geehrter Herr Holtz,
betrachten Sie einen Mikrowellenofen. Die Mikrowelle wird an den WÃ¤nden reflektiert. Stellt man ein Wasserglas hinein wird die Mikrowelle durch das Wasser absorbiert. Entsprechend ist IR-Strahlung zwischen ErdoberflÃ¤che und AtmosphÃ¤re eingeschlossen. Wenn die EmissivitÃ¤t E von ErdoberflÃ¤che und AtmosphÃ¤re kleiner 1 ist, wird wegen A= E und A+T+R= 1 die IR-Strahlung nicht total absorbiert, sondern auch reflektiert. Diese eingeschlossene IR-Strahlung wÃ¤chst nicht unbegrenzt an, also muÃŸ sie irgendwo im KÃ¤fig wieder absorbiert werden. Andererseits geht diese Strahlung nicht in die Energiebilanz ein, da sie eingeschlossen ist.
Zitat: Das reflektierte IR-Licht wird letztlich schnell absorbiert â€¦
Reflektierte Strahlung kann nicht absorbiert bzw. emittiert werden, sie mindert den Energieumsatz.
Stephens et al. (2012) verwenden die puren Messwerte von ERBE/CERES. Da wird nichts physikalisch umgerechnet oder die Gegebenheiten andere Messwerte berÃ¼cksichtigt, und der Emissionsgrad von e=0,96 gilt fÃ¼r die terrestrische IR-Strahlung.
Die Frage ist eigentlich, welches ist die korrekte homogene Erd-OberflÃ¤chentemperatur, also die Temperatur bei homogener Verteilung der Energie Ã¼ber den Globus.
Wenn man fÃ¼r die homogene Erd-OberflÃ¤chentemperatur 288K ansetzt, und ganz einfach die AtmosphÃ¤re mal als idealen Strahler (â€schwarzen Strahlerâ€œ) betrachtet, dann ergibt sich eine maximale Abstrahlung der AtmosphÃ¤re in den Weltraum von 195 W/m^2. Bei der Annahme der homogenen Erd-OberflÃ¤chentemperatur von 292,5K erhÃ¤lt man unter den gleichen Bedingungen fÃ¼r die maximale Abstrahlung der AtmosphÃ¤re in den Weltraum einen Wert von 207 W/m^2.
Schon diese einfache Betrachtung zeigt mir, dass da etwas nicht korrekt sein kann. Stephens et al. (2012) bauen hier einfach â€Messwerteâ€œ zusammen, die nicht zu den gegebenen Bedingungen passen.
Die Strahlung der AtmosphÃ¤re (Gase+Wolken) in den Weltraum in dieser Darstellung Ã¼bertifft die eines idealen (â€schwarzenâ€œ) Strahlers AtmosphÃ¤re: 220 W/m^2 (AtmosphÃ¤re) + 20 W/m^2 (direkte Abstrahlung der OberflÃ¤che).
Mfg
Werner Holtz
Mir ist noch ein Argument eingefallen, warum Stephens et al in ihrer Bilanzrechnung eine EmissivitÃ¤t = 1 verwenden. Es werden in ihrer Rechnung 398 W/m2 emittiert und 346 W/m2 kommen aus der AtmosphÃ¤re wieder zurÃ¼ck. In der Bilanzrechnung mÃ¼sste man also auch berÃ¼cksichtigen, dass IR-Strahlung von der AtmosphÃ¤re reflektiert wird (â€IR-Albedoâ€œ). Das fÃ¼hrt zu Mehrfach-Reflexionen der IR-Strahlung zwischen AtmosphÃ¤re und OberflÃ¤che. AuÃŸerdem geht auch die EmissivitÃ¤t der AtmosphÃ¤re in die Rechnung ein. Das reflektierte IR-Licht wird letztlich schnell absorbiert, macht aber die Energiebilanz komplizierter. Deshalb rechnet man wohl mit der EmissivitÃ¤t = 1.
Sie haben Recht. Mit der von mir vorgeschlagenen Korrektur kann man nur 1,2 K der von Ihnen festgestellten Differenz von 5 K erklÃ¤ren. Die Hauptdiskrepanz folgt offensichtlich aus der Nicht-BerÃ¼cksichtigung der EmissivitÃ¤t von 0,96. Warum Stephens et al. dies vernachlÃ¤ssigen bleibt mir auch ein RÃ¤tsel.
Zitat: Ich habe dies fÃ¼r den HADCRUT4 Datensatz fÃ¼r 2014 gemacht und erhalte 395 W/m2 fÃ¼r die EmissivitÃ¤t von 1. FÃ¼r die mittlere OberflÃ¤chentemperatur fÃ¼r 2014 erhalte ich 287,7 K.
Es zeigt sich doch eine Diskrepanz zwischen Temperatur und Strahlungsleistung der OberflÃ¤che. Wenn man annimmt, daÃŸ das Stefan-Boltzmann-Gesetz den korrekten Zusammenhang zwischen Strahlung und Temperatur wieder geben soll.
1. T = [395/(1*5,67*10^-8)]^0,25 = 288,9 K oder T = [395/(0,96*5,67*10^-8)]^0,25 = 291,9 K
2. E = 1*5,67*10^-8*287,7^4 = 388,5 W/m^2 oder E = 0,96*5,67*10^-8*287,7^4 = 372,9 W/m^2
3. Diskrepanz: dT = 288,9-287,7 = 1,2K bzw. dT = 291,9-287,7 = 4,2K ; dE = 395-388,5 = 6,5 W/m^2 bzw. dE = 395-373 = 22 W/m^2
Die Messungen vom Emissonsgrad fÃ¼r die Erd-OberflÃ¤che ergeben einen flÃ¤chen-gewichteten Mittelwert von e=0,96. Die Messungen der LW Abstrahlung der Erd-OberflÃ¤che mittels ERBE/CERES ergibt einen 5-Jahres-Mittelwert von 395,58 W/m^2 (Alle Berechnungen der ERBE/CERES Messungen werden mit einem Emissionsgrad von 1 durchgefÃ¼hrt.). Wenn also das Stefan-Boltzmann-Gesetz GlÃ¼tigkeit besitzen soll, mÃ¼ssen sie Temperatur und Strahlungsleistung in Einklang bringen, sonst sind alle daraus folgenden Interpretationen fehlerhaft bzw. entziehen sich der ÃœberprÃ¼fung anhand physikalischer GesetzmÃ¤ÃŸigkeiten.
Wie heiÃŸt es: Wer miÃŸt, miÃŸt Mist.
Hier zwei frei-verfÃ¼gbare Papers (Einfach per Google suchen):
+ Outgoing Longwave Radiation due to Directly Transmitted Surface Emission
+ Calculation of radiative fluxes from the surface to top of atmosphere based on ISCCP and other global data sets: Refinements of the radiative transfer model and the input data
+ und die ERBE/CERES Messwerte als Text-Datei finden Sie auf: https://eosweb.larc.nasa.gov/sse/
Mfg
Werner Holtz
gerade die von der AGW-Sekte praktizierte S&B-Rechnerei ist Ã¼belste Kindergartenphysik, bei â€Expertenâ€œ also vorsÃ¤tzlicher Betrug, das skandalÃ¶se K&T-Strahlungsbilanz-Modell EINGESCHLOSSEN, das auch der Schwindler Stephens et al. tatsÃ¤chlich 2012 immer noch verwendet!
Wenn einer schon mit PLanck und der S&B-Formel rechnen will, darf er die 4. Potenz doch nicht ignorieren, die eine Rechnung mit einer â€Durchschnittstemperaturâ€œ VERBIETET. Darauf hat nicht nur GERLICH deutlich hingewiesen (HÃ¶ldersche Ungleichung).
Der noch viel grÃ¶ÃŸere Unsinn, siehe Pesch und Holz, wirklich eine Belewidigung des gesunden Menschenverstandes, ist die â€WÃ¤rmestrahlungâ€œ AUS der AtmosphÃ¤re, die nach K&T ja tatsÃ¤chlich stÃ¤rker sein soll, als die Sonneneinstrahlung, ein wirklicher BrÃ¼ller!!!!!!!
Manchmal glaube ich, dass Menschen Dinge glauben, gerade weil sie so extrem paradox sind.
Es soll also diese ErdatmosphÃ¤re, in der wir alle leben,
und die gesetzmÃ¤ÃŸig mit dem Abstand von der ErdoberflÃ¤che immer â€dÃ¼nnerâ€œ und KÃ„LTER wird,
stÃ¤rker strahlen als die Sonne !!!!
Gerade, wenn man schon mit S&B rechnet, was bei Gasen natÃ¼rlich falsch ist,
muss einer solchen Strahlung eine Temperatur der Strahlungsquelle entsprechen, die man nun mal NICHT messen kann. Damit ist diese Gegenstrahlung lÃ¤ngst, lÃ¤ngst, lÃ¤ngst WIDERLEGT!
Deshalb das physikalisch richtige Urteil:
VerstoÃŸ gegen den ersten (Energie aus dem NICHTS) und gegen den zweiten Hauptsatz der Thermodynamik (von kalt nach warm),
vereinfacht:
WÃ¤rme aus der KÃ¤lte.
KINDERGARTEN fÃ¼r die doofe Masse, einschl. Politiker und Journalisten
und vorsÃ¤tzlicher Betrug von â€Expertenâ€œ.
Man kann weder alles wissen, noch alles erklÃ¤ren. In einer sachlichen Diskussion ist es daher NICHT notwendig einer (falschen) Theorie
eine richtige Theorie gegenÃ¼berzustellen.
Sondern es gilt zunÃ¤chst nur zu klÃ¤ren, ob eine behauptete Theorie richtig oder falsch ist.
Die CO2-Treibhaustheorie ist mit Sicherheit falsch, sowohl durch simple empirische ÃœberprÃ¼fung, als auch in der theoretisch physikalischen Konstruktion, die dem 1. und 2. HS der Thermodynamik widerspricht.
Das Wort â€Treibhauseffektâ€œ ist bereits falsch, weil die TemperaturerhÃ¶hung im GÃ¤rtnerhaus BEI SONNENEINSTRAHLUNG auf der Blockierung der KonvektionskÃ¼hlung beruht.
Wer seriÃ¶s diskutieren will, muss also zunÃ¤chts diesen Begriff fÃ¼r die AtmosphÃ¤re strikt ablehnen.
Gerade in suggestiven Diskussionen wird gerne der Trick eingesetzt fehlende inhaltliche Tatsachen durch Wortkonstruktionen, bevorzugt in englischer Sprache zu ersetzen.
Deshalb muss man auch falsche â€Begriffeâ€œ strikt ablehnen,
das fÃ¤ngt an bei dem Begriff â€Treibhauseffektâ€œ der AtmosphÃ¤re, bzw. irgendwelcher Spurengase darin, bzw. irgend einem nicht messbaren menschlichen Anteil dieser Spurengase darin.
Aber Denken tut weh, Nachquatschen ist immer bequemer ğŸ™‚
mfG
ich glaube Stephens et al. (2012) haben Recht. Man darf nicht die mittlere OberflÃ¤chentemperatur in das SB-Gesetz einsetzen, sondern man muÃŸ fÃ¼r jedes einzelne OberflÃ¤chenelement die emittierte Leistung nach SB berechnen und dann Ã¼ber alle OberflÃ¤chenelemente summieren. Ich habe dies fÃ¼r den HADCRUT4 Datensatz fÃ¼r 2014 gemacht und erhalte 395 W/m2 fÃ¼r die EmissivitÃ¤t von 1. FÃ¼r die mittlere OberflÃ¤chentemperatur fÃ¼r 2014 erhalte ich 287,7 K.
in 2. sollte natÃ¼rlich 1997 statt 2007 stehen.
Von 1979 bis 2003/2004 ein AufwÃ¤rtstrend um etwa 0,4-0,5Â°C. Danach kippt die Trendlinie und zeigt fÃ¼r die restlichen 11 Jahre ein Minus von etwa 0,2Â°C an.
Jetzt hat man mehrere MÃ¶glichkeiten zur Interpretation.
1. 2004 war der Kipppunkt ins negative
2. ab etwa 2007 Stillstand (wird hier bei EIKE am ehesten vertreten)
3. ein seit 1979 ungebrochener AufwÃ¤rtstrend, allerdings um etwa 0,2Â° flacher verlaufende Linie (AGW-Version)
Alle 3 Aussagen sind richtig. Es kommt lediglich darauf an, was man damit begrÃ¼nden will.
MfG
welches katastrophale wissenschaftliche Niveau diese â€Klimaexpertenâ€œ haben, ist schon bemerkenswert.
WÃ¤rme aus der KÃ¤lte.
â€Abbildung T9: Energiebudget-Diagramm fÃ¼r die Erde nach Stephens et al. (2012).â€œ
Was hat man denn da zusammengeschustert. Versucht man mal wieder den â€Treibhaus-Effektâ€œ zu retten.
LW Up Surface = 398 W/m^2 macht eine Temperatur von 292,5K (beim Emissionsgrad von 0,96) und nicht 288K bzw. laut NASA 287,7K (Global Mean Surface Air Temperature). Wow! Fast FÃ¼nf Grad Unterschied!
Damit scheinen alle Temperaturmessungen der OberflÃ¤che (auch Satellitenmessungen) Ã¼berhaupt nicht zu stimmen â€“ Klasse Wissenschaft.
Mfg
Werner Holtz
319 W/mÂ² aus dem NICHTS! Wie kann man nur solche Graphiken zeichnen und sich dann auch noch â€Wissenschaftlerâ€œ nennen?? Klimakirche eben, da gibt es bei Bedarf auch einmal ein Wunder wenn die HauptsÃ¤tze der Thermodynamik stÃ¶ren. Und das im 21. Jahrhundert, es ist zum Heulenâ€¦.
Mir ist ganz Ã¼bel.
Viele GrÃ¼ÃŸe
Peter
P.S. Wenn ich einen solchen BlÃ¶dsinn verbreitet hÃ¤tte, wÃ¤re sicherlich der blaue Wagen vorgefahren.