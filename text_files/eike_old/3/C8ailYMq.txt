§D.1 Klimamodelle sind seit dem AR4 verbessert. Die Modelle reproduzieren die beobachtete Temperaturverteilung in räumlicher Größenordnung von Kontinenten sowie deren Trends über viele Jahrzehnte einschließlich der rascheren Erwärmung seit Mitte des 20. Jahrhunderts sowie der Abkühlung unmittelbar nach großen Vulkanausbrüchen (sehr hohes Vertrauen).
§D.2 Beobachtete und modellierte Studien der Temperaturänderung, Klima-Rückkopplungen sowie Änderungen des Energiehaushaltes der Erde zusammen schaffen Vertrauen in die Größenordnung der globalen Erwärmung als Reaktion auf Antriebe in Vergangenheit und Zukunft.
Keines dieser Statements entspricht der Wahrheit, wie die inzwischen berüchtigte Divergenz zwischen CMIP5 und dem Stillstand beweist (siehe unten Abbildung 7). Der CO2-Gehalt ist weiter gestiegen, die Temperatur nicht.
Die interessante Frage lautet: Warum ist das so? Ein ursächlicher Grund ist so fundamental unlösbar, dass man sich aus gutem Grunde fragen kann, warum die Multimilliarden Dollar schwere Klimamodell-„Industrie“ jemals ohne Nachfrage darauf eingehen konnte. (1)
GCMs sind das Klima-Äquivalent der den Ingenieuren vertrauten Modellen für die Finite-Elemente-Analyse (FEA). FEA wird in diesen Tagen für fast alles genutzt, um Brücken, Flugzeuge, um Motorkomponenten zu konstruieren ([FEA unterstützt Lösungen für [Material-] Spannungen, Dehnung, Biegung, Hitze, Alterung, …).
In der FEA Technik [[ im Sinne von Berechnungs-Verfahren]] werden die Eingangsparameter durch wiederholtes Messen der tatsächlichen Materialien im Labor bestimmt.
Auch die nicht-linearen „unlösbaren“ Parameter der Navier-Stokes-Fluiddynamik (Luftströme bei Flugzeugen werden drag-modelliert mit Hilfe der CFD-Untergruppe von FEA ) werden durch Prüfung im Windkanal verifiziert.
(wie Auto- / Flugzeug-Designer es mit 1:1 und/oder skalierten Modellen tun).
Hinsichtlich des Klimas der Erde ist dies nicht möglich.
GCMs überziehen die Welt mit abgesteckten Gitterquadraten (die ,finite elements‘ der Ingenieure). Jedes Gitterquadrat hat einen Satz Anfangsbedingungen. Dann wird eine Änderung angebracht (wie z. B. zunehmender CO2-Gehalt nach IPCC RCP8.5). Das entspricht der zunehmenden Stresskomponente einer Brücke durch zunehmenden Verkehr oder die zusätzliche Erwärmung durch Reibung an einem Flugzeug, das immer schneller fliegt. Das GCM berechnet dann, wie sich die Eingangswerte mit der Zeit ändern (2). Die Berechnungen basieren auf etablierter Physik wie der Clausius-Clapeyron’schen Wasserdampfgleichung, Strahlungstransport durch das Frequenzband (d. h. Treibhauseffekt) oder die Navier-Stoke’schen Gleichungen zur Dynamik von Flüssigkeiten für konvektive Zellen.
Das CMIP5-Archiv verwendet UCAR zufolge bis zu 30 atmosphärische Schichten, bis zu 30 Ozeanschichten und Zeitschritten von 30 Minuten.
Die horizontale räumliche Auflösung der CMIP5-Modelle ist typischerweise ~2.5° Breite/Länge am Äquator (etwa 280 km). Die feinste horizontale CMIP5-Auflösung betrug ~1,1° oder etwa 110 km. Dieses Limit ist den Rechenleistungen der Computer geschuldet. Eine Verdoppelung der Auflösung durch eine Halbierung des Gitterquadrates (xy) vervierfacht die Anzahl der Quadrate. Es halbiert auch in etwa den Zeitschritt aufgrund der Courant-Friedrichs-Lewy-Bedingung CFL. (Die Erklärung von CFL für numerisch gelöste partielle Differantialgleichungen würde zu weit vom Thema dieses Beitrags wegführen). Die Verdoppelung der Auflösung auf ein ~55 km-Gitternetz ist ~4 X 2 so rechenintensiv für die Computer. Die University Corporation for Atmospheric Research UCAR sagt, dass die GCM-Faustregel einer verdoppelten räumlichen Auflösung gleich ist einer 10 mal so hohen Anforderung an die Computer. Eine Größenordnung pro Verdoppelung der Auflösung.
Die räumliche Auflösung moderner numerischer Wettervorhersage-Modelle ist notwendigerweise viel feiner. Der neueste, 2012 installierte Wetter-Supercomputer des UK Met.-Office und deren assoziierte Modelle verwenden eine grobe Auflösung von 25 km (NAE) für Prädiktanten wie Druckgradienten {Wind} und frontale Grenzschichten sowie eine Feinauflösung (UKV) von 1,5 km für Parameter wie Niederschlag (für regionale Überschwemmungs-Warnungen). Die Website des UKMO zeigt stolz dieses Beispiel:
Dies ist möglich, weil die UKMO-Wettermodelle nur das Wetter für das Gebiet von UK und nur ein paar Tage im Voraus simulieren – und nicht den Planeten für viele Jahrzehnte im Voraus. Die Simulation von ΔT bis zum Jahr 2100 mit dem ,groben‘ Wettermodell mit 25 km des UKMO liegt um zwei Größenordnungen (≈4x4x2x2x[10/8]) jenseits der derzeitigen Fähigkeiten. Die Simulation von ΔT mit einer Auflösung von 1,5 km bis zum Jahr 2100 zur Erfassung tropischer konvektiver Zellen (sowie deren ,Eschenbach‘-Konsequenzen) liegt um 7 Größenordnungen (110-55-27-13-7-3-1,5) jenseits derzeitiger Computer-Fähigkeiten. Die heutigen besten Supercomputer brauchen für einen einzigen GCM-Lauf rund 2 Monate (fünfzig aufeinander folgende Tage pro UCAR sind typisch). Ein einziger Lauf bei einer Auflösung von 1,5 km würde 1,4 Millionen Jahre dauern. Darum heißt es im AR5 WG1 im Kapitel 7 (hinsichtlich Wolken bei §7.2.1.2):
„Wolkenbildungs-Prozesse umfassen Größenordnungen von kleinsten Wolken-Kondensationskernen bis zu Wolkensystemen mit einer Ausdehnung von tausenden Kilometern. Diese Bandbreite der Größenordnungen kann unmöglich mit numerischen Gleichungen von Computern aufgelöst werden, und das wird noch Jahrzehnte lang so bleiben“.
Das fundamental unlösbare Problem der Auflösung in den GCMs wird sehr gut illustriert in Gestalt eines Gewitterclusters bei seinem Weg über Arizona. 110 X 110 Quadrate sind die feinstmögliche computertechnische Auflösung in CMIP5. Nutzlos zur Auflösung konvektiver Prozesse.
Grundlegende Klimaprozesse wie tropische konvektive Zellen (Gewitter) mit der damit assoziierten Freisetzung latenter Wärme in der oberen Troposphäre, von wo die Wärme leichter in den Weltraum entweichen kann, sowie dem damit verbundenen Niederschlag, der Wasserdampf aus der Luft entfernt und damit dessen Rückkopplung erniedrigt, können einfach von GCMs nicht simuliert werden. Klimaphänomene kleiner als das Gitterquadrat können nicht von der Physik simuliert werden. Sie müssen parametrisiert werden.
Und das ist das zweite unlösbare Problem. Es ist unmöglich, etwas zu parametrisieren ohne Kenntnis der Zuordnungen (wie viel der beobachteten Änderung in der Vergangenheit ist Treibhausgasen geschuldet, und wie viel der ,natürlichen‘ Variation?). Die Zuordnung der Parameter im AR5(CMIP5 des IPCC) war hauptsächlich AGW geschuldet. Dazu in der SPM:
§D.3 Diese Beweise für den menschlichen Einfluss haben seit dem AR4 zugenommen. Es ist extrem wahrscheinlich, dass menschlicher Einfluss die dominante Ursache der beobachteten Erwärmung seit Mitte des 20. Jahrhunderts ist.
Die CMIP5-Parametrisierungen wurden in zwei grundlegenden Schritten bestimmt. Seit dem Jahr 2002 hat DoE das CAPT-Programm gesponsert, welches multiple Kurzfrist-Vergleiche zwischen den GCMs bei der Modellierung einiger Tage (bei grober Auflösung) und deren Brüdern, den numerischen Wettervorhersagemodellen sowie dem tatsächlich eingetretenen Wetter anstellt. Die vorgegebene Logik lautet, dass die kurzfristige Divergenz zwischen GCMs und Wettermodellen einer falschen Parametrisierung geschuldet sein muss, von denen die Wettermodelle nicht so viele brauchen (3). Dies funktioniert gut bei ,schnellen‘ Phäomenen wie einer irrtümlich von GCMs modellierten Aufsplitterung der ITC in zwei innerhalb von zwei Tagen, aber nicht bei ,langsamen‘ Phänomenen wie Änderungen der hochtroposphärischen Feuchtigkeit oder der Wolkenbedeckung mit steigendem CO2-Gehalt mit der Zeit.
Der zweite Schritt bestand darin, längerfristige beobachtete Daten zu verschiedenen Zeiten mit den Parametrisierungs-Ergebnissen zu vergleichen und die Parameter zu ,frisieren‘, um die Beobachtungen über längere Zeiträume zu reproduzieren. Dieses Verfahren wurde angewendet von der NOAA MAPP CMIP5-Arbeitsgruppe (4). Es ist sehr schwierig, Faktoren zu frisieren wie Änderungen der Wolkenbedeckung, Albedo, Ozean-Wassertemperaturen oder die sommerliche arktische Meereis-Ausdehnung. Hiervon gibt es nämlich kaum längerfristige beobachtete Daten zum Vergleich. Und das Tuning erfordert immer noch die Annahme einer Verbindung der Zuordnungen zwischen den Prozessen (Modelle), dem gewünschten Phänomen-Output (z. B. Wolkenbedeckung, arktisches Meeres) sowie Beobachtung.
CMIP5-Parametrisierungen wurden frisiert, um Temperatur-Nachhersagen in bestmöglicher Weise von 2005 bis zurück zum Jahr 1975 durchzuführen (die verpflichtende Nachhersage über drei Jahrzehnte), erklärt durch das experimentelle Design der CMIP5-Modelle selbst (5). Dies geht eindeutig hervor von der ,Gottheit der Anpassung‘.
[Original: „CMIP5 parameterizations were tuned to hindcast temperature as best possible from 2005 back to about 1975 (the mandatory three decade hindcast), explained by the CMIP5 experimental design itself.[5] This is circumstantially evident from the ‘goodness of fit’“]
Die Hypothese von hauptsächlich anthropogenen Ursachen bedeutet, dass die GCMs falsch parametiridert worden sind (vor dem Hintergrund des Stillstands). Darum laufen sie derzeit heiß, wenn sie ,natürliche‘ Änderungen einer Abkühlung wie z. B. von 1945 bis 1975 hinweg rechnen. Graphisch wurde dies im Jahre 2010 von Dr. Akasofu aufbereitet, ehemaliger Leiter des International Arctic Research Center – und vom IPCC komplett ignoriert (6).
Akasofus einfacher Gedanke erklärt auch, warum sich das arktische Eis erholt, was die Alarmisten immer mehr alarmiert. Die Eiskarten des Dänischen Wetteramtes DMI und Larsens Durchfahrt der Nordwest-Passage im Jahre 1944 legen einen natürlichen Zyklus der Eisentwicklung in der Arktis nahe, mit einem Minimum in den vierziger Jahren und einem Maximum in den siebziger Jahren. Allerdings konnte das arktische Eis nicht gut erfasst werden bis zum Beginn der Vermessung mittels Satelliten im Jahre 1979, etwa um den Zeitpunkt eines möglichen natürlichen Spitzenwertes. Die gesamte beobachtete Aufzeichnung bis zum Jahr 2013 könnte einfach nur der abnehmende Zweig einer natürlichen Eis-Variation sein. Die Erholung des Eises hinsichtlich Ausdehnung, Volumen und mehrjährigem Eis seit dem Jahr 2012 könnte der Beginn eines natürlichen 35 Jahre langen Eisbildungszyklus‘ sein. Aber die GCM-Zuordnung ist hauptsächlich AGW geschuldet.
Fast niemand ist willens, über das fundamental unlösbare Problem der GCMs zu diskutieren. Ungeeignete Klimamodelle wären eine sehr schlechte Nachricht für alle, die glauben, dass die Klimawissenschaft settled ist.
—————————————
References:
[1] According to the congressionally mandated annual FCCE report to Congress, the US alone spent $2.66 billion in 2014 on climate change research. By comparison, the 2014 NOAA NWS budget for weather research was $82 million; only three percent of what was spent on climate change. FUBAR.
[2] What is actually calculated are values at cell corners (nodes), based on the cell’s internals plus the node’s adjacent cells internals.
[3] Philips et. al., Evaluating Parameterizations in General Circulation Models, BAMS 85: 1903-1915 (2004)
[4] NOAA MAPP CMIP5 Task Force white paper, available at cpo.NOAA.gov/sites.cop/MAPP/
[5] Taylor et. al., An Overview of CMIP5 and the Experimental Design, BAMS 93: 485-498 (2012).
[6] Akasofu, On the recovery from the Little Ice Age, Natural Science 2: 1211-1224 (2010).
Link: http://wattsupwiththat.com/2015/08/09/the-trouble-with-global-climate-models/
Übersetzt von Chris Frey EIKE. Ich bedanke mich bei meinem Übersetzer-Kollegen Herrn Andreas Demmig für seine Hilfe.
am Sonntag, 16.08.2015, 09:16
#2: Dr. John Mohammed sagt am Samstag, 15.08.2015, 10:30:
„Begeistert bin ich auch von dem Kommentar von Gerald Pesch, besonders über seine treffende Aussage „CO2 Moleküle strahlen praktisch nicht (Stossdeaktivierung)“. Dieser Aussage stimme ich 100 %ig zu !“
So ungefähr ist das auch, mit dem kleinen aber feinen Unterschied, dass dieses an der Erdoberfläche nur deshalb so ist, andere Arten des Energietransportes jederzeit immer möglichen Transport durch elektromagnetische Strahlung sehr weit überwiegen, so dass der Strahlungstransport in Bodennähe eine ganz untergeordnete Rolle spielt. Es ist aber gemessen mit naturwissenschaftlicher Präzision und Richtigkeit ganz sicher nicht Null. Ganz sicher ist sein Vorzeichen aber negativ (kühlend) und nicht positiv (wärmend).
#########################################“,
Nur mit nach oben abnehmender Materiedichte werden die materiegebundenen Energieflüsse immer schwächer und zum Weltall hin bleibt zu 100 % nur noch die elektromagnetische Energieleitfähigkeit (blöderweise Strahlung genannt) übrig.
Und hier dominiert die Strahlung der CO2 Deformationsschwingung mit dem Bandenschwerpunkt um 15 µ herum mit all ihren kurzwelligeren Obertönen und Kombinationsschwingungen.
Jedermann kann das Gesagte experimentell nachprüfen, indem er ein IR-Fernthermometer zur Hand nimmt (18 Euro beim Discounter) und nach unten die Bodentemperatur und nach oben die Lufttemperatur misst.
Ein Beispiel von vor meiner Haustüre ist hier: http://tinyurl.com/kgje845
Diese „Strahlungstransfermessungen“ (genauer Berechnungen aus gemessenen Strahlungstemperaturdifferenzen) zeigen, die Atmosphäre ist kein Treibhaus, sondern ein Kühlhaus.
Die Schichtung der Ozeantemperaturen zeigt demgegenüber wie ein Treibhaus aussieht, das ist es oben wärmer als unten. In der Atmosphäre müsste es eigentlich aus physikalischen Gründen genau wie im Ozean oben wärmer sein als unten, weil die warme Luft aufsteigt und die kalte absinkt. Es ist aber in der Atmosphäre genau umgekehrt – und zwar über den adiabatischen Gradienten hinaus! Deshalb Kühl- nicht Treibhaus!
Es ist schon makaber dass die Atmosphäre durch CO2 gekühlt wird, und das auch physikalisch plausibel darstellbar und messbar !! ist, Politik und Medien jedoch das Gespenst der Klimaerwärmung durch CO2 durch alle Schichten der Gesellschaft huschen lassen. Alle machen mit bei der „Klimarettung“, ohne auch nur einen Schimmer einer Ahnung zu haben was das eigentlich sein soll. Das erinnert mich an folgendes:
http://bit.ly/1PfTjSd
„Begeistert bin ich auch von dem Kommentar von Gerald Pesch, besonders über seine treffende Aussage „CO2 Moleküle strahlen praktisch nicht (Stossdeaktivierung)“. Dieser Aussage stimme ich 100 %ig zu !“
So ungefähr ist das auch, mit dem kleinen aber feinen Unterschied, dass dieses an der Erdoberfläche nur deshalb so ist, andere Arten des Energietransportes jederzeit immer möglichen Transport durch elektromagnetische Strahlung sehr weit überwiegen, so dass der Strahlungstransport in Bodennähe eine ganz untergeordnete Rolle spielt. Es ist aber gemessen mit naturwissenschaftlicher Präzision und Richtigkeit ganz sicher nicht Null. Ganz sicher ist sein Vorzeichen aber negativ (kühlend) und nicht positiv (wärmend).
Nur mit nach oben abnehmender Materiedichte werden die materiegebundenen Energieflüsse immer schwächer und zum Weltall hin bleibt zu 100 % nur noch die elektromagnetische Energieleitfähigkeit (blöderweise Strahlung genannt) übrig.
Und hier dominiert die Strahlung der CO2 Deformationsschwingung mit dem Bandenschwerpunkt um 15 µ herum mit all ihren kurzwelligeren Obertönen und Kombinationsschwingungen.
Jedermann kann das Gesagte experimentell nachprüfen, indem er ein IR-Fernthermometer zur Hand nimmt (18 Euro beim Discounter) und nach unten die Bodentemperatur und nach oben die Lufttemperatur misst.
Ein Beispiel von vor meiner Haustüre ist hier: http://tinyurl.com/kgje845
Diese „Strahlungstransfermessungen“ (genauer Berechnungen aus gemessenen Strahlungstemperaturdifferenzen) zeigen, die Atmosphäre ist kein Treibhaus, sondern ein Kühlhaus.
Die Schichtung der Ozeantemperaturen zeigt demgegenüber wie ein Treibhaus aussieht, das ist es oben wärmer als unten. In der Atmosphäre müsste es eigentlich aus physikalischen Gründen genau wie im Ozean oben wärmer sein als unten, weil die warme Luft aufsteigt und die kalte absinkt. Es ist aber in der Atmosphäre genau umgekehrt – und zwar über den adiabatischen Gradienten hinaus! Deshalb Kühl- nicht Treibhaus!
„Mich würde interessieren, wie gut die Modelle das globale Temperaturprofil der Erdoberfläche und der Atmosphäre im monatlichen Mittel beschreiben können.“
Sehr löblich Ihr Interesse. Die Antwort lautet: Die können es gar nicht. Der Vergleich Hindcast mit gemessener Realität ergibt nicht akzeptable Differenzen, die auf systematischen Modellfehlern beruhen.
„Die Konvektion dominiert den Wärmetransport zur Tropopause und CO2 Moleküle strahlen praktisch nicht (Stoßdeaktivierung) unter den Bedingungen der unteren Troposphäre. Man kann den in der Klimakirche verkündeten Erderwärmungseffekt durch CO2 nicht beweisen, egal mit welcher Methode, weil es ihn schlicht nicht gibt!“
Lieber Herr Pesch,
das stimmt!
Aber:
„Glauben ist seliger als Wissen“
Und darüber hinaus:
„wer es glaubt, wird selig“…
Schönen Feiertag
der Artikel über Klimamodelle von Rud Istvan gehört zu den besten anschaulichen Beschreibungen über Klimamodelle, die ich je gelesen habe !
Begeistert bin ich auch von dem Kommentar von Gerald Pesch, besonders über seine treffende Aussage „CO2 Moleküle strahlen praktisch nicht (Stossdeaktivierung)“. Dieser Aussage stimme ich 100 %ig zu ! Herr Pesch, sehen Sie sich dazu auch einmal auf der Website scilogs Klimalounge beim
Artikel „Keine Verlangsamung der globalen Temperatur“ vom 4.8.2015 die Antwort von Stefan Rahmstorf vom 12.8.2015 auf eine Frage von mir an.
Die Antwort von Herrn Rahmstorf lässt vermuten, dass er in Thermodynamik noch einiges zu lernen hat. Doch machen Sie sich selbst ein Bild davon !
Großes Lob auch an die Readaktion,die es erfolgreich geschafft hat, Trolle/Trollinen und
Wichtugtuer(innen) in diesem Forum in die Schranken zu weisen . Mein Respekt für diesen Erfolg !
Und da fängt der Unsinn schon an. „Treibhauseffekt“ hat nichts, aber auch gar nichts, mit irgendeinem Strahlungstransport zu tun. Die Konvektion dominiert den Wärmetransport zur Tropopause und CO2 Moleküle strahlen praktisch nicht (Stoßdeaktivierung) unter den Bedingungen der unteren Troposphäre. Man kann den in der Klimakirche verkündeten Erderwärmungseffekt durch CO2 nicht beweisen, egal mit welcher Methode, weil es ihn schlicht nicht gibt!