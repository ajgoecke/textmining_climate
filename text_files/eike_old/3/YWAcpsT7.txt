Ein ganz süßes Video
Sicherlich haben Sie schon einmal, liebe Leserin, ein ganz süßes Video wie dieses https://www.youtube.com/watch?v=HjzjSvY0vlU mit ein paar Freundinnen geteilt, und Sie lieber Leser haben den Clip eines attraktiven Models an Ihre Kumpels verschickt. Da kann man dann fragen, wie sich solche kleinen Geschenke über die Menschheit ausbreiten, wenn jeder der Bescherten genau so weitermacht.
Nach 10 Tagen wird daraus eine Zahl aus elf Einsen; das sind im Klartext 11 Milliarden.
Modelle sind eigenwillig
Wir haben also versucht, die Wirklichkeit durch Mathematik abzubilden. Dazu haben wir angenommen, dass der Zuwachs an neuen Betrachtern des Videos proportional ist zur Zahl der existierenden. So etwas wird durch eine „Exponentialfunktion“ beschrieben, weil man den Zuwachs als 10 hoch n schreiben könnte, wobei n, die Anzahl der Tage, als „Exponent“ bezeichnet wird.
Dieses simple Computermodell zeigt uns etwas Interessantes:
Es gibt da Annahmen, die das Endresultat extrem beeinflussen: Wir gehen davon aus, dass unsere Freunde und Freundesfreunde täglich teilen. Würden sie aber nur am Wochenende teilen, dann hätten wir nach 10 Tagen nur gut hundert Bescherte, nicht elf Milliarden! Das ist ein Unterschied.
Wir haben behauptet, dass jeder 10-mal teilt. Bei 5 „Shares“, so wie es uns WhatsApp maximal zugesteht, wären wir bei rund 10 Millionen gelandet. Auch noch eine große Zahl, aber immerhin nur ein Tausendstel der vorherigen.
Die Zeitspanne – täglich oder nur am Sonntag – und die Zahl der Shares sind die „Parameter“ unseres Modells, und wenn wir die nicht ganz genau kennen, dann wird das Resultat nicht nur falsch, es kann grotesk unsinnig werden.
Die innere Logik
Aber nicht nur die Parameter des Modells müssen stimmen, auch die innere Logik des Vorgangs muss durch die Mathematik richtig abgebildet sein. Wir müssen bedenken, dass über kurz oder lang die einen oder anderen antworten: „Ja, super Clip, kannt´ ich aber schon.“ Die dürfen wir dann nicht mehr zählen.
In unserem Modell hatten wir das nicht berücksichtigt. Es waren also die Parameter unsicher und die Logik falsch. Als Folge davon war das Resultat unbrauchbar.
Sie sagen, das sei trivial? Dann sagen Sie das bitte auch Herrn Neil Ferguson, Professor am Imperial College London, und seinen deutschen Kollegen, die mit ihren fürchterlichen mathematischen Modellen in Sachen Corona berühmt und berüchtigt wurden. Vermutlich lagen auch deren Annahmen zur intrinsischen Logik der Epidemie daneben, oder die Parameter waren ungenau, oder beides. Vielleicht haben auch sie „täglich“ angenommen, wenn „wöchentlich“ richtig gewesen wäre. Und heraus kam ein abstruses Ergebnis.
Eine Maschine voller Schräubchen
Keine Sorge, wir werden jetzt nicht das einzig richtige Computermodell zur Ausbreitung einer Pandemie entwickeln, wir wollten nur aufzeigen, wie leicht so ein Modell uns total in die Irre führen kann.
Noch komplizierter ist vermutlich der Mechanismus unseres Klimas. Da gibt es neben CO2 so viele andere Einflussfaktoren, welche alle in unserem Modell auftauchen müssen und deren relativer Einfluss durch die jeweiligen „Parameter“ abgebildet wird. Da sind das stratosphärische Ozon, der Wasserdampf durch CH4, der Albedo-Effekt durch Wolken, die langlebigen Kondensstreifen und noch einige andere Kandidaten, die alle in die Energiebilanz des Planeten eingreifen (hier nachzulesen https://think-again.org/product/grun-und-dumm/).
Ein mathematisches Modell dafür wird eine recht komplizierte Maschine, an der viele Schrauben sind – die Parameter –, an denen wir so lange drehen, bis die Maschine das tut, was wir möchten.
Wunschdenken
Hört sich alles logisch an, wir werden aber sehen, dass das Drehen an den Schräubchen, welche die Wirklichkeit abbilden, vertrackt ist. Drehen wir die eine vor, dann muss die andere zurückgedreht werden, etc. Und je mehr Schräubchen an der Maschine, desto willkürlicher kann man sie einstellen, um ein gewünschtes Ergebnis zu bekommen.
In der Wissenschaft unterscheidet man experimentelle und theoretische Physik. Erstere beschäftigt sich damit, im Labor gemessene Daten den Gleichungen gegenüberzustellen, die von Theoretikern ersonnenen wurden. Mathematische Modelle sind hier das alltägliche Werkzeug. Man malt eine Kurve mit den Messwerten und vergleicht sie mit der Kurve der theoretischen Berechnungen. Dann „schraubt“ man an den Parametern, bis die beiden Kurven möglichst gleich sind.
Ein Elefant mit vier Schrauben
Enrico Fermi, der vermutlich größte Experimentalphysiker aller Zeiten, erklärte seinen Studenten, dass Modelle nur ganz wenige Stellschrauben haben dürfen. Er verbildlichte das mit den Worten: „Gib mir ein Modell mit drei Schrauben, und ich zeichne dir eine Kurve, die aussieht wie ein Elefant; und mit vier Schrauben wedelt der mit dem Schwanz.“
Willkommen in der Welt der Klimaforscher, die mit mega-komplexen Modellen und riesigen Computern so lange an den Schräubchen drehen, bis das rauskommt, was Politik und Geldgeber wünschen. Da wedelt der Elefant dann nicht nur mit dem Schwanz, sondern der Schwanz mit dem Elefanten, falls das gewünscht wird.
Das ganze Modellieren funktioniert also nur, wenn der Wissenschaftler unvoreingenommen ist, denn Wissenschaft ist eine Funktion der absichtslosen Wahrheit. Diese Neutralität aber ist dank der Einmischung von Politik in die Wissenschaft verloren gegangen; Corona und Klima sind nur die zwei auffallendsten Beispiele dafür.
Computermodell der Macht
Und nun zu einem Phänomen, zu dem wir mehr als genug Erfahrung haben, um ein zuverlässiges Computermodell zu bauen: Es ist das Phänomen der Macht. Hier sind Parameter und intrinsische Logik gut bekannt, und die Dynamik wird präzise durch die eingangs erwähnte Exponentialfunktion beschrieben.
Die Geschichte zeigt, dass je mächtiger eine Person ist, desto größer wird ihr Verlangen nach noch mehr Macht. Und da Macht nicht vergeben wird, sondern genommen, so hat der Mächtige eine sehr gute Position: Er kann sich ja nehmen, was er will.
Betrachten Sie herausragende historische Figuren und Sie werden sehen, wie sich deren Macht alle drei oder fünf Jahre „verdoppelt“ hat. Dieses grenzenlose Wachstum konnte dann nur durch eine Katastrophe gestoppt werden – wie etwa Waterloo.
Deshalb muss das exponentielle Wachstum der Macht, wenn schon unvermeidlich, so doch zeitlich begrenzt werden; etwa auf zwei Amtsperioden. Diese Randbedingung, ein wesentlicher Bestandteil der Verfassung der USA, hat dafür gesorgt, dass es dort seit 1776 nie zu einem „Meltdown“ der Demokratie kam. Niemals konnte ein machtbesessener Präsident so willkürlich in das Schicksal von Millionen eingreifen, wie das im Europa der vergangenen 250 Jahre die Regel war.
Ein Gedankenexperiment
Aber nicht nur in der Geschichte, auch in der Gegenwart gibt es Fälle von grenzenloser Macht. Erdogan, Putin, Zuckerberg sind Namen, die uns da sofort einfallen; ja, und in Deutschland regiert die „mächtigsten Frau der Welt“. In 15 Jahren hat ihre Macht exponentiell zugenommen und im Februar 2020, mit dem Absetzen des gewählten Thüringer Ministerpräsidenten, eine Höchstmarke erreicht.
Lassen Sie uns, in einem Gedankenexperiment – Gedanken sind bekanntlich frei – ein mathematisches Modell für die politische Zukunft Deutschlands entwerfen. Die Exponentialfunktion lässt ja ein noch stärkeres Anwachsen von Merkels Macht für die nächsten Jahre erwarten. Was wird geschehen?
Sie wird keine Welle der Beliebtheit verstreichen lassen, ohne daraus Kapital zu schlagen. Ihr aktuelles „Corona High“ wird sie nutzen, indem sie vor Ende 2020 Neuwahlen organisiert – mit ihr selbst als einziger Kandidatin. Eine schwarz/rot/rote oder schwarz/rot/grüne Koalition bringt es dann auf eine 2/3-Mehrheit.
Eine neue Fahne
Es wird neue Ministerien geben, reichlich besetzt mit Expertinnen sozialistischer Prägung: Eines für Faktensicherung, ein anderes für Demokratie, mit angehängten „NGOs“. Regierungskritischen Zeitungen und Blogs wird das Leben durch bürokratische Schikanen unmöglich gemacht. Der Bundestag bleibt zwar noch bestehen, zur Arbeitsplatzbeschaffung für hilfreiche Persönlichkeiten, hat aber keine Funktion mehr.
Ohne parlamentarische Opposition, ohne kritische Medien kann endlich praktiziert werden, was lange angestrebt war: Es wird durchregiert. Die Verfassung wird nur respektiert, falls sie der großen Transformation nicht im Wege steht, ansonsten wird sie angepasst. Die Demokratie in Deutschland hat damit den „Tipping Point“ überwunden und es geht vorwärts – und abwärts – in die sozialistische Vergangenheit. Die neuen strategischen Partner werden Türkei, Iran und Russland sein; Polen, England und USA bleiben Feindstaaten. Und die verhasste schwarz-rot-goldene Fahne wird endlich ersetzt durch ein Banner mit sozialistisch-ökologischen Symbolen.
Soweit also ein mathematisches Gedankenexperiment in Sachen Exponentialfunktion. Es hat keinen Bezug zur Realität, denn Frau Merkel hat ihr Ausscheiden aus der Politik bereits angekündigt, und nur ein Schelm würde an ihren Worten zweifeln.
Dieser Artikel erschien zuerst bei www.think-again.org und im Buch „Grün und Dumm“ https://think-again.org/product/grun-und-dumm/
Warum PASCAL und Niklaus Wirth da eine spezielle Rolle spielten, ist hier mal unwichtig. Eines meiner ersten komplexeren Programme 1986, sollte bestimmte (gewünschte) Ergebnisse produzieren, was es aber böserweise oft gar nicht tat. Ergo sann ich auf „intelligente Abhilfe“ und schuf eine Art „versteckte Nische“ die man als Insider diskret „aufrufen“ konnte um so ein paar zusätzliche Parameter, ergänzend zu den für Alle sichtbaren „offiziellen“ Eingabedaten einzuspeisen. Einer der „Zusätze“ war eine Art „WmDaWab-Eingabefrage“ – Vulgo:“Was möchtest Du als Wunschergebnis ausgegeben bekommen?“ und dazu noch ein paar intelligent optimierte „Feinheiten“ für Optik und Außendarstellung der Berechnungen, wie etwa die gewünsche Dauer von immergleichen (sinnlosen) Berechnungsschleifen nach dem Start, mit völlig bedeutungslosen Parametern aber dies dann auch bis zu 6 Stunden lang, unermüdlich immer wieder. Hauptsache irgendwelche neugierigen Ahnungslose, würden bei Interesse den Rechner „schuften und glühen“ sehen, bei seinen immensen (Müll-)Berechnungen. Heraus kam nachher als „offizielles Berechnungsergebnis“ und nach langen Stunden Rechenzeit, natürlich IMMER genau das, was ich vorher selbst diskret als „WmDaWab“ dafür als Wunscherhalt eingegeben hatte…
Das war 1986 gewesen, ist also 34 Jahre her. Dankt hier denn irgendwer, die HEUTIGEN Programmierer, seien darin DÜMMER, als ich selbst es damals, also 1986 gewesen bin???
Werner Eisenkopf
Vielleicht haben Klimatologen, die ich dazu angefragt habe, ja was dazu zu sagen, aber aus deren crickets geht nichts hervor. Wie hier in Fäden sehen ist die Klimazahlenhuberei-Fangemeinde „wird ja nix eingegeben“ gernegratis Kenntnislos Überzeugt.
Liebe Kenntnislos Überzeugte, wenn ihr wieder einen Anfall bekommt dann benutzt die Mantra: der Computer rechnet was eingegeben wurde, der Computer rechnet was eingegeben wurde; … nur Scharlatane wollen, was ausgegeben wird, nicht eingegeben wurde. Im Gegenteil: was ausgegeben wird steht in programmierter Relation zu dem was eingegeben wurde (selbst Judith A. Curry will die Konsequenzen des fundamentalen Computer-Prinzips nicht diskutieren).
P.S. ich gebe Fachleuten der Kenntnislosen Überzeugung auf ihrem Fachgebiet keine Auskunft. Und euer infantiles Geschwätz, nur die Anderen hätten keine Ahnung weil ja Beweislastumkehr (sieht aus wie abgetippt von €gratis€grünen€ spektrum.de und/oder €Dauer€hetzer€ heise.de), das könnt ihr euch an die Backe kleben /dingens.
Das Zitat wird aber John von Neumann zugeordnet!
Fermi hat es nur gerne wiedergegeben. xxxxxxxxxx Denn schließlich ist John v. Neumann ja der erste Klimamodellierer. Er hat als Erster einen Computer dazu eingesetzt, die Navier-Stokes-Gleichung für die Atmosphäre zu lösen, um damit Wetter und Klima vorherzusagen. Und sein Modell hatte weit mehr als nur 3 Parameter!
„Es ergibt keinen Sinn, präzise zu sein, wenn man überhaupt nicht weiß, wovon man spricht.“
„Wenn Leute nicht glauben, dass Mathematik einfach ist, dann nur deshalb, weil sie nicht begreifen, wie kompliziert das Leben ist.“
Bitte hier nur unter vollem Klarnamen posten, siehe Regeln.
Dem Vernehmen nach versagen bei solchen Tests sämtliche Klimamodelle kläglich.
Weil nur, wenn solche Tests treffsicher wären, hätte Beschäftigung mit diesen Modellen Sinn. Ansonsten ist es nichts anderes als Kaffeesudlesen mit hohem technischem Aufwand.
Ihre Aussage zu Güte von Modellen ist nur zum Teil richtig. Modelle müssen permanent mit der Wirklichkeit verglichen und angepasst werden, ansonsten sind diese reine Spielerei.
Der Grund dafür ist, dass man die Wirklichkeit NICHT in mathematischen Formeln darstellen kann. Es ist ein Irrglaube, man müsse nur die Naturgesetze richtig einprogrammieren und es käme das richtige Ergebnis raus. Es ist aber nur so, dass man die Auswirkungen von Naturprozesse mit mathematischen Mitteln nachgeahmt werden, d.h. im bestimmten engen Bereich liegen Natur und Simulation eng beieinander.
Bei Klimasimulationen wird zuerst die Vergangenheit nachgeahmt und locker eine Übereinstimmung erzielt, keine Herausforderung für einen guten Programmierer. Ist aber Spielerei, wie die nächsten Jahren zeigen. Ist ein Modell im Jahr 2000 fertiggestellt, stimmt es bis dato, aber von 2000 bis 2020 passt gar nichts mehr. Weil es eben Spielerei ohne Naturwissenschaftliche Grundlage ist.
Z.B. habe ich Klima und CO2 Verläufe der letzten 200 Jahre und spiele so lange mit den Formeln rum, das beides in der Simulation unter Ausschluß von kosmischen Einfüssen passt. Nun behaupte ich mal aber ketzerisch, CO2 Einfluß ist nahe Null und alle Schwankungen kommen von der Sonne. Ab sofort stimmt gar nichts mehr, obwohl die Vergangenheit richtig dargestellt wird. Da es keine gemessene Zusammenhänge beim Klima oder Epidemie-Verläufe gibt, sind diese Modelle reine sinnlose Spielerei. Und das Erschreckende, dass Milliarden glauben, dass ein überdimensionierter Taschenrechner die Zukunft voraussagen kann. Dann lieber Wanga.
Im Maschienenbau z.B. bekannte Prozesse zu simulieren und damit unzählige Realtests zu sparen ist äußerst sinnvoll. Aber Nichts geht in Serie ohne Test nur anhand von Simulation, da es doch nur eine Annäherung ist.
Unbekanntes zu Simulieren ist wie betont Spielerei und dient nur denen, die damit zu Ruhm und Ehre kommen, anstatt mit ehrlicher Arbeit bescheiden zu leben.
Nein, Herr Strasser, das ist viel komplexer, als Sie vermuten. Man vergleicht Modelle ja ständig mit Beobachtungen in den sogenannten Hindcasts. Die Temperaturverläufe der Vergangenheit wird von allen Modellen gut reproduziert, sonst hätten sie die Aufnahme in den IPCC-Pool (CMIP) nicht geschafft.
Aber die Frage, ob ein Modell gut ist, die ist viel komplexer. Z.B. hat das neue britische Modell für den CMIP6 einen zu hohen Wert der Klimasensitivität geliefert, da ist es schlechter als andere Modelle. Gleichzeitig liefert es besser Werte für Niederschläge als diese anderen Modelle. Welches Modell ist nun besser?
„Dem Vernehmen nach versagen bei solchen Tests sämtliche Klimamodelle kläglich.“
Ach? Wo vernimmt man das denn? Auch hier gilt: Die Wahrheit ist komplexer.
@ Herr Georgiv
„Bei Klimasimulationen wird zuerst die Vergangenheit nachgeahmt und locker eine Übereinstimmung erzielt, keine Herausforderung für einen guten Programmierer.“
Mag sein, es gibt aber eine Reihe weiterer Tests, die ein Modell gleichzeitig überstehen muss. Eine der schwierigeren Hürde ist z.B., dass ohne Forcing die Temperatur stabil (abgesehen von natürlichen Schwankungen) bleiben muss. Wäre die Rekonstruktion vergangener Temperaturen der einzige Test, dann hätten Sie natürlich recht, aber die Praxis ist nun einmal ganz anders als von Ihnen vermutet.
Herr Dreßen, dass die Software die ach so komplizierte Test übersteht, hat erstmal nur eine Auswirkung: dass die teuren Programmierer nicht durch Praktikanten im 4. Semester ersetzt werden,die das hinkriegen würden. Nicht mehr und nicht weniger!
Denken Sie bitte nach, was ein erfolgreicher Softwaretest bedeutet!
Dass ein Berechnungdurchlauf ein exakt definiertes Ergebnis erbringt, oder? Erklären Sie mir bitte, wie man das Hauptergebnis einer Software testet, die eine zukünftige Entwicklung einer Pandemie oder gar die Temperatur in hundert Jahre berechnet? Wie einfach muss man gestrickt sein um zu glauben, dass eine stupide Maschine, die Zeile für Zeile vorgeschriebene Befehle ausführt, die Zukunft voraussagen kann?
Wenn das Endergebnis (bei Klimasimulationen Temperatursteigerungen in xx Jahren) nicht mit der Wirklichkeit verglichen werden können, da diese in ferner Zukunft liegt, erfüllen sie die Erwartungen des Auftraggebers, also eine Erwärmung von xx Grad. Geben Sie mir den Auftrag, und ich erstelle Ihnen eine Simulation die alle Tests besteht und eine Abkühlung von 2 Grad bei einer CO2 Verdopplung vorhersagt.
Früher waren die Gebrüder Grimm dafür zuständig, heute Simulationen!
Auftrag erteilt. Hat bislang noch niemand geschafft, ich bin also sehr neugierig.
Sie müssten mir dazu schon einen niedrigen 7-stelligen Betrag zu Verfügung stellen und 10 Jahre Zeit dazu.
Mein Wissen über Codes ist beschränkt, meine Hauptaufgabe in den letzten Jahrzehnten war aber, die Künstler der komplexen Codes zu erden und zu trimmen, dass ihr Kunstwerk verdammt noch mal dieses Ergebnis liefern soll, wofür der Kunde bezahlt. Die Konkurenz haben wir meilenweit überholt.
Mit einer guten Mannschaft stelle ich ihnen jeden Unsinn als Code zu Verfügung.
„Auftrag erteilt“
Da Sie so locker diesen Satz schreiben, muss ich davon ausgehen, dass Ihnen der Aufwand für eine komplexe Simulation nicht so richtig klar ist.
Zu Spitzenzeiten war ich verantwortlich, dass 30 Programmierer und Tester jeden Monat entlohnt werden. Dazu brauch man eine ganze Menge zufriedene Kunden.
Die Simulationen der Klima-Märchen-Onkel müssen nur eine Erwärmung anzeigen und schon fließen die Millionen, die für die Mannschaft erforderlich sind. Sobald keine Erwärmung mehr errechnet wird, muss die Mannschaft entlassen werden.
Das ist der einzige Grund, warum es immer wärmer wird!
„Es hat niemand versucht, da es dafür kein Geld gibt.“
Nein, damit machen Sie es sich aber sehr einfach: Staaten, die vom Export fossiler Energieträger abhängig sind, hätten ein großes Interesse daran, andere Vorhersagen, die in Richtung Entwarnung zeigen, vorzeigen zu können. Im Grunde sind es sogar alle Staaten der Welt, Klimapolitik zu betreiben wird von jeder Regierung der Welt als Last, nicht als Lust, empfunden.
Ich denke eher, das Grundproblem ist eher, dass es einfach nicht geht. Alles Geld der Welt kann nun mal die Physik nicht ersetzen. Sie können sich ja mal schlau machen, welche Tests Modelle bestehen müssen, um in den IPCC-Pool aufgenommen werden zu können. Hören Sie auch mal den Klimaentwicklern zu: Man ist glücklich und stolz, wenn die Entwicklung eines Modells geklappt hat und die Tests erfolgreich bestanden wurden. Welche Klimasensitivität dabei herauskommt, das weiß man dann erst hinterher. Sie haben ja sicherlich verfolgt, wie überrascht die britischen Modellierer jetzt bei ihrem neuen Modell über den hohen Wert waren, man kann auch sagen, unangenehm überrascht.
“ Hören Sie auch mal den Klimaentwicklern zu: Man ist glücklich und stolz, wenn die Entwicklung eines Modells geklappt hat und die Tests erfolgreich bestanden wurden. “ Soll ich mich totlachen! Wie ich schon schrieb, ich habe Jahrzehnte stolze Entwickler gefaltet, wenn die Projekte eben etwas anderes als erwarten berechnet haben, also die Test nicht bestanden haben.
„Alles Geld der Welt kann nun mal die Physik nicht ersetzen“
Was haben die Computerspielereien über Klima mit Physik zu tun. NICHTS.
“ Welche Klimasensitivität dabei herauskommt, das weiß man dann erst hinterher. Sie haben ja sicherlich verfolgt, wie überrascht die britischen Modellierer jetzt bei ihrem neuen Modell über den hohen Wert waren, man kann auch sagen, unangenehm überrascht. “
Ha ha ha, es ist ja nicht auszuhalten! Es ist genau die Sensivität rausgekommen, die man Code direkt oder indirekt reingesteckt hat. Wenn man so naiv ist und glaubt, dass die Physik im Code wirkt, dann ist Ihnen nicht zu helfen. Denken Sie mal darüber nach, warum im digitalen Zeichentrickfilm, im Gegensatz zum analogen Film, der Stein nicht immer nach unten fällt, sondern manchmal auch nach oben fliegt? Weil der Entwickler es so will! Beim analogen Film wirkt auch keine Schwerkraft, sondern die Wirkung wird dokumentiert. Beim digitalen dagegen kann man jeden Humbug vortäuschen. Man wäre bei ernsteren Themen dann nicht so blöd um die Steine nach oben fliegen zu lassen, das wäre zu auffällig.
Aber ein digitaler Zeichentrickfilm und eine Simulation sind … im Prinzip identisch! Es wird genau dass angezeigt, was gewollt und im Code gesteckt ist. Man kann sich unterhalten lassen.
Wenn man glaubt, es wird die Zukunft von Prozessen, dessen ganze Komplexität nicht im entferntesten erforscht sind, im Computer richtig angezeigt werden…
Dann lieber an den Weihnachtsmann glauben, dass macht mehr Spaß und ist eher realistisch!
Mein Angebot steht, Sie müssten nur die Finanzierung von sagen wir mal, 5 Mio unterschreiben, dann können Sie Endtemperatur im Rahmen von plus/minus 5 Grad zu heute selbst bestimmen, und das Modell bekommen Sie 2030!
Man merkt, dass Sie keine Ahnung haben, wovon Sie reden. Bleiben wir doch mal beim Beispiel des britischen Modells:
Es war ein Rätsel, warum es eine unplausibel hohe Klimasensitivität zeigt. Die Entwickler-Community hat viel Zeit und Arbeit investiert, um die Unterschiede und Gründe herauszubekommen. Heißester Kandidat ist, dass jüngere Satellitendaten gezeigt haben, dass das Eis-Wasser-Verhältnis kleiner ist als gedacht. Das ist man im Modell dementsprechend angepasst.
Und nun fragen Sie sich doch mal selbst:
1.) Sie tun ja so, als könne man beliebig Dinge verändern. An diesem Beispiel sehen Sie, dass man keine Wahl hat, man ist gezwungen, bei den realen Daten zu bleiben.
2.) Glauben Sie wirklich, jemand hätte geahnt, welche Auswirkung diese so unscheinbar wirkende Änderung auf die Klimasensitivität des Modells hat? Da unterschätzen Sie die Komplexität der Modelle aber gewaltig.
Apropos Komplexität:
Die Realität ist komplexer als Sie meinen. Sie geben sich mit einfachen Antworten zufrieden (z.B. „es könne nur das rauskommen, was man reingesteckt hat“, der Geldgeber bestimmt das Ergebnis etc.). Das mag Ihnen helfen, der Überforderung durch zu komplexe Zusammenhänge zu entkommen, der Nachteil ist aber, dass Sie sich immer weiter von der wirklichen Welt entfernen.
Zitat: „Die Temperaturverläufe der Vergangenheit wird von allen Modellen gut reproduziert, sonst hätten sie die Aufnahme in den IPCC-Pool (CMIP) nicht geschafft.“
Wie sind denn nun die Modelle wirklich in dieser Hinsicht? Auch auf Rahmsdorfs Seiten habe ich viel (fast nur) Gutes gelesen. Die Kritik meint trotzdem, sie versagen dabei völlig. Und die Modelle, die diese Rekonstruktion einigermaßen hinbekommen, seien mit verschiedenen *Co2- Sensitivitäten* je nach Epoche gefittet???
Auf jeden Fall kann meiner bescheidenen Logik nach eines einfach NICHT WAHR sein:
Dass die bisherigen CMIP Modelle (bestens vom IPCC geprüft, gewogen und für gut befunden) *alles sehr gut* reproduzieren konnten… und die neuen Modelle (die der Aufnahme in den nächsten AR Report, der absehbarerweise zumindest in den *Summarys* noch alarmistischer als die anderen daherkommen wird, harren), mit der jetzt *überraschend hohen* Klimasensitivität ebenfalls die selben Klimaverläufe der Vergangenheit ausgeben.
Hier beisst sich doch die Katze in den Schwanz. Ich denke fast, die sollen mal die Rechenleistung der Supercomputer eine Weile nicht beanspruchen… Spart Energie, hätte zumindest einen wirklichen Nutzen…
Also man hat gemerkt, dass Mist rauskommt, und hat nach dem Willen der Community etwas verändert, bis diese mit dem Ergebnis zufrieden war.
Genau das wird bei jeder Software gemacht, bei öffentlich geförderten muss die Öffentlichkeit mit dem Ergebnis zufrieden sein, in der freien Wirtschaft der zahlende Kunde. Und es wird immer so lange an den Schrauben gedreht, bis es passt.
Wer kommt aber auf die Idee dass der Computer das Ergebnis gewollt hat? Unfug, die Öffentlichkeit oder der Kunde haben das Ergenis erwartet, und die Entwickler sind keine Götter, die die Schöpfung wiederholt haben, auch wenn sie das glauben. Sie machen aus der Software das, was von ihnen erwartet wird.
ja, diesen Widerspruch hier habe ich auch schon bemerkt. Da gibt es die einen, die behaupten, die Modelle würden scheitern, die beobachteten Temperaturverläufe zu reproduzieren. Und dann gibt es diejenigen, die sagen, es sei keine Kunst, dass die Verläufe reproduziert werden, das könne man ja durch Schrauben an den Parametern ja immer hinkriegen.
Ja, beides zusammen kann nicht richtig sein. Aber das sollten die Herren hier dann schon unter sich ausmachen. (Ich persönlich bin sogar der Ansicht, dass beides falsch ist.)
„Und die Modelle, die diese Rekonstruktion einigermaßen hinbekommen, seien mit verschiedenen *Co2- Sensitivitäten* je nach Epoche gefittet???“
Das ist natürlich doppelter Unsinn. Erstens, weil keine Sensitivitätswerte eingegeben werden, und zweitens, weil an einem fertigen Modell nichts mehr geändert wird, schon gar nicht epochenweise.
Das Grundproblem ist halt, dass es derzeit kaum möglich ist zu entscheiden, welche Modelle gut (gut in dem Sinne, dass auch zukünftige Klimaverläufe gut vorhergesagt werden) und welche schlecht.
Die anthropogenen Treibhausgase wirken erwärmend, die anthropogenen Aerosole abkühlend, das hat die Erwärmung in Teilen kompensiert. Man kann die Nettowirkung also grundsätzlich auf mehrere Arten reproduzieren: Hohe Erwärmungswirkung der THG gepaart mit hoher Abkühlungswirkung oder niedrigere Erwärmungswirkung dert THG mit niedriger Abkühlungswirkung durch Aerosole.
Da die Emissionen von Aerosolen inzwischen von Satelliten recht genau erfasst wird, wird man spätestens in 20 Jahren wissen, welche Klasse von Modellen besser ist. Bis dahin fließen einfach alle Modelle in die Projektionen ein.
Kompensiert der antropogene Aerosolausstoß wirklich die (THG induzierte) Erwärmung in Teilen? Das würde nur gelten, wenn Co2- Ausstoß und Aerosole beide gemeinsam steigen würden.
1.) Im Gegensatz zu den Co2 Werten der Atmosphäre hat sich aber der Ausstoß z. B. von Schwefeldioxid als Aerosolbildner in D. von 5,5 Mio Tonnen in 1990 auf 315 000 Tonnen in 2017(?) vermindert. Eine Reduktion um 94%!!! (Quelle UBA)
Ich darf wohl einigermaßen sicher davon ausgehen, dass diese Entwicklung für alle Industrieländer gleichermaßen anzusetzen ist.
2.) Das Argument, dass das Wachstum Chinas durch den dort evtl. erhöhten So2- Ausstoß im genannten Zeitraum die So2 Einsparungen der westlichen Welt (über)kompensiert, stelle ich in Frage, weil die Lebensdauer des So2 in der Atmosphäre nur wenige Tage beträgt und *chinesisches Co2* also keine globale Wirkung haben kann.
Wenn Aerosole, hier So2, als so klimawirksam verstanden werden, dass man sich sogar als Geoengeneering- Maßnahme ebendiesem zur Abkühlung bedienen möchte, es sich in wirklich relevanten Größenordnungen genau in ebenjenem Zeitraum minderte, in dem uns die größten Erwärmungsraten gezeigt werden… was bleibt dann noch fürs Co2 übrig???
Um zum Eingangszitat zurückzukehren: Liegt hier nicht für die letzten 30 Jahre in den Modellen ein fundamentaler Trugschluß vor, müsste es nicht so sein, dass die Erwärmung durch den zurückgehenden Aerosolausstoß gefördert wird, aber so in den Modellen gar nicht wirkt?
Nirgends fand ich bisher Hinweise darauf, dass die Abnahme des So2 in irgendeiner Hinsicht auf die Klimawirkung wirklich quantifiziert wurde…
Bei physikalischen Modellsimulationen geht es nicht darum, mit irgendeinem „passenden“ Algorithmus vorgegebene Ergebnisse zu reproduzieren. Das Modell muss natürlich auf den Naturgesetzen besieren. Das Ziel ist es schliesslich, die Beobachtungen nicht nur zu reproduzieren, sondern auch die wesentlich dafür verantwortlichen Prozesse und deren Zusammenspiel zu quantifizieren.
Man kann Klimadaten auch durch neuronale Netze reproduzieren, das sind nur keine physikalischen modelle, denn man lernt dabei nichts über die Physik des Klimasystems.
Daher ist es ein völliges Aberglauben, unbekannte Zusammenhänge in einer Software richtig einzusetzen und damit neue Erkenntnisse zu gewinnen. Es ist wirklich absurd, daran zu glauben.
Wären die Zusammenhänge der natürlichen Klimaschwankungen wissenschaftlich eindeutig bekannt und mathematisch abbildbar (
nicht wie das Wetter, das aufgrund Zufälle unberechenbar ist), dann gäbe es eine Basis über Simulationen zu diskutieren.
Bei unüberprüfbare Klimasimulationen reduziert sich die Frage, glaubt man am digitalen Weihnachtsmann oder nicht. Sicher, ich habe deutlich mehr als andere Sterbliche mit trimmen von komplexen Programmen zu tun gehabt. Aber aus meiner Sicht müsste der gesunde Menschenverstand ausreichen um zu verstehen, dass eine mit Unwissen gefüllte Maschine keinen neuen Zusammengang nachweisen kann, sonder nur Datensalat.
Wenn Sie daran glauben wollen, bitte!
18. JUNI 2020 UM 23:40
Sie sind offenbar kein Naturwissenschaftler. Modelle werden überall den in Naturwissenschaften neben Experimenten eingesetzt, um Erkenntnisse über komplexe Zusammenhönge zu finden. Dass man solche Modelle davor anhand bekannter Zusammenhänge testen muss und davor die Programmierung selbst, ist selbstverständlich, denn garbage in – garbage out ist kaum das Ziel eines Modellierers. Daher halte ich dies Argument, welches immer genannt wird, für reichlich flachbrüstig und zeigt nur Mangel an Argumenten an.
Die Existenz von validen Wettermodellen und Zirkulationdmodelle widerlegt ihre Behauptung. Sie müssen schon inhaltlich und konkret argumentieren, ihre simple Sichtweise ist unzureichend. Ihre Kenntnisse beschränken sich wohl nur auf irgendwelche komplexe Programme, aber mit physikalischen Modellen kennen sie sich nicht aus. Beschäftigen Sie sich erstmal damit.
„Die Existenz von validen Wettermodellen und Zirkulationdmodelle“ widerlegt nicht, sondern bestätigt meine Behauptung.
Frage: Was unterscheidet Wettermodelle von Klimamodelle grundsätzlich?
Antwort: Die physikalischen Grundlagen der Wetteränderungen sind bestens erforscht, beim Klima weiß man nicht einmal wieviel Faktoren überhaupt das Klima beeinflüssen.
Schlussfolgerung: Klimamodelle sind grundsätzlich Unfug, wenn man einen prozess nicht kennt, kann man seinen Verlauf nicht programmieren, sondern nur Mäerchen erzählen! ODER?
Zweiter Punkt mit 2 Fragen: Wie groß ist die Wahrscheinlichkeit, dass ein Simulation von heute das Wetter vom 15.7. genau vorhersagt? Wie groß ist die Wahrscheinlichkeit, dass die Simulation vom 14.7. das Wetter vom 15.7. genau vorhersagt? Einmal fast Null und einmal sehr hoch? Warum? Die langfristge Simulation basiert ausschließlich auf Simulationsdaten bei einem Prozess, der augrund Zufälle nicht reproduzierbar ist, daher ist das Ergebnis Müll. Bei der Verfolgung der Wetterentwicklung werden simulierte Daten durch echt Gemessene ersetzt und damit ist die kurzfristige Prognose nah an der Wahrheit.
Das Problem ist eher, dass Naturwissenschaflter nicht wissen, wie der Code funktioniert und glauben bei partielln Übereinstimmung die Physik im code ist richtig programmiert, und die Entwickler wissen nicht, wie die Natur funktioniert und glauben bei partieller Übereinstimmung, sie haben die Schöpfung wiederholt. Beides ist Quatsch. Simulation kann man nur wie ein digitaler Trickfilm betrachten, der mit den gleichen Mitteln reelLe Vorgänge nachahmt. Es gibt zig vernüftige Anwendungen dafür, zum Beispiel die physikalische Strömunssimulation zu nutzen anstatt ständig den Windkanal zu nutzen.
Aber letztendlich physkalisch unbekannte Prozesse wie Klimaänderung der Vergangenheit nachzuspielen und dann erzählen, man hat die Ursachen gefunden und gar bewiesen, ist reiner Selbstbetrug und wer es glaubt, ist selbst schuld.
Ich werde Ihnen noch Beispiele schreiben über sinnvolle und vllig sinnlose Nutzung der gleichen Simulation, brauche aber Zeit…
MfG
„Mist“ ist wieder so brutal vereinfachend, die Realität ist komplexer. Das britische Modell liefert bessere Niederschlagsmuster als die meisten anderen Modelle. Die Klimasensitivität ist dagegen hoch, und man hat in einem Paper nachgewiesen, dass die TCR (transiente Klimasensitivität) außerhalb des Bereiches liegt, die durch die Daten der beobachteten Erwärmung nahegelegt werden. (Das trifft übrigens auch für das hier vielgelobte russische Modell zu, das ist zu kalt).
Würden Sie solche Modelle nun herausnehmen wollen? Das würde dann aber bedeuten, dass die Bänder der Klimaprojektionen schmaler werden, dass mehr Sicherheit der Prognosen dargestellt wird als besteht. In diesem Sinne ist es gut, dass Modelle, die die Bänder verbreitern und die Unsicherheiten vergrößern, mit einfließen und dargestellt werden. Es ist zwar unwahrscheinlicher, dass das russische oder das britische Modell die Zukunft korrekt darstellen, aber es ist auch nicht ausgeschlossen.
Die Welt der Klimamodeller ist ganz schön komplex, nicht wahr?
Für die britischen und russischen Modellierer sind die Ergebnisse Hinweise darauf, dass ihre Modelle verbessert werden können und sollen. Die Arbeit geht weiter, und die kritischen Stellschrauben liegen in der Mikrophysik der Wolkenentstehung.
19. JUNI 2020 UM 15:24
„Frage: Was unterscheidet Wettermodelle von Klimamodelle grundsätzlich?
Antwort: Die physikalischen Grundlagen der Wetteränderungen sind bestens erforscht, beim Klima weiß man nicht einmal wieviel Faktoren überhaupt das Klima beeinflüssen.“
Sie haben echt keine Ahnung vom Thema und behaupten was Ihnen passt. Von Softwareentwicklern darf man mehr abverlangen. Peinlich.
Klimamodelle unterscheiden sich von der implementierten Physik grundsätzlich nicht von Wettermodellen. Sie laufen nur länger als Wettermodelle.
„Wie groß ist die Wahrscheinlichkeit, dass … Einmal fast Null und einmal sehr hoch? Warum? Die langfristge Simulation basiert ausschließlich auf Simulationsdaten bei einem Prozess, der augrund Zufälle nicht reproduzierbar ist, daher ist das Ergebnis Müll.“
Was Sie „Müll“ nennen, ist ein Datenpunkt fürs Klima. Natürlich ist klar, dass die Wettervorhersage mit der Laufzeit im Phasenraum vom wahren Wetterzustand wegdiffundiert. Aber man betrachtet ja beim Klima sowieso nur die Phasenraumverteilung der Wetterzustände, also die Statistik ohne Berücksichtigung, ob die Zeitfolge auch stimmt. Das Klima wird also dann reproduziert, wenn die berechneten Wetterzustände unabhängig von ihrer zeitlichen Reihenfolge zum statistischen Ensemble des Klimazustands gehören.
„Bei der Verfolgung der Wetterentwicklung werden simulierte Daten durch echt Gemessene ersetzt und damit ist die kurzfristige Prognose nah an der Wahrheit.“
Die Messungen fliessen in einen Gesamtoptimierungsprozeß mit den simulierten Daten ein. Aber es gibt zu jedem Zeitpunkt wesentlich mehr simulierte Daten als Messdaten. Messdaten verschieben nur die Simulationstendenzen.
„Das Problem ist eher, dass Naturwissenschaflter nicht wissen, wie der Code funktioniert“
Hä? Die Modellierer, die typischerweise Naturwissenschaftler sind, haben den code doch selbst programmiert.
„und glauben bei partielln Übereinstimmung die Physik im code ist richtig programmiert, und die Entwickler wissen nicht, wie die Natur funktioniert und glauben bei partieller Übereinstimmung, sie haben die Schöpfung wiederholt.“
So ein Quatsch. Sie haben keine Ahnung, wie physikalische Modellbildung gemacht wird.
Der einzige Grund, warum Sie beide soviel Wind machen können, ist dass für diese Form von Computer-Spielerei das Steuergeld nur so hinterhergeschmissen wird. Müsste man damit Geld wirklich verdienen anstatt nur einzusammeln, dann könnten Sie nicht mehr mit Computer spielen, sondern sich die Hände dreckig machen mit unangenehmer Arbeit, oder hätten so eine ätzende Persönlichkeit wie mich vor der Nase, die wirklich den Göttern gut beibringen kann, was beim Programm hinten rauskommen muss.
Spielen Sie weiter und glauben Sie Ihre Spielerei!
19. JUNI 2020 UM 22:39
Sich auf den gesunden Menschenverstand berufen heißt ja nichts weniger als thematisch überfordert und am Ende zu sein.
Diesen Glauben könnten Sie den Leuten doch relativ „einfach“ nehmen: Einige Modelle sind im Quellcode verfügbar. Zeigen Sie doch einfach, dass da keine physikalischen Grundlagen drin sind, sondern nur eine Erwärmung fest reinprogrammiert ist. Das wäre viel überzeugender als einfach nur Behauptungen aufzustellen.
„Da fühlen sich zwei IT-Gottheiten, bzw deren Propheten mächtig auf den Schlips getreten,…“
Ich bitte Sie. Wie man hier nachlesen kann, haben nur Sie sich als „IT-Gottheit“ aufgeplustert. Schade, dass Sie als Experte leider nur Bauchgefühl und Vorurteil zur Sache beitragen konnten. Da ihr letzter Beitrag nur noch aus ad-hominems besteht, beende ich die Diskussion mit Ihnen.
Sie behaupten ja, Klimamodellierer seien geradezu glücklich über die hohen Werte der ECS mancher neuer Modelle.
Der Guardian hat kürzlich einen Artikel publiziert, wo die Meinung vertreten worden ist, diese neuen Modelle gäben Anlass zur Vermutung, dass die Klimasensitivität höher sei als bisher gedacht. Nach ihrer Denkweise müssten die Modellierer damit doch sehr zufrieden sein, oder?
Dann gleichen Sie bitte mal ihre Ansichten über Modellierer mit der Realität ab. Hier bei climatefeedback.org kritisieren führende Modellierer den Artikel als „irreführend“ und begründen auch warum:
https://climatefeedback.org/evaluation/article-in-the-guardian-misleads-readers-about-sensitivity-of-climate-models-by-narrowly-focusing-on-single-study-jonathan-watts/
Wie gesagt, die echte Welt ist ganz anders als Sie inzwischen so meinen.
vielen Dank für Ihre Expertise!
Aber für mich muß ein Modell, welches sich Modell nennen darf, in der Lage sein, die bereits geschehene Realität mit rel. guter Güte nachzuvollziehen!
Gute Güte bedeutet, die wesentlichen Einflüsse müssen in ihrer Auswirkung passen. Natürlich kann es sein, daß man erkennt, viele Dinge fehlen. Aber dann ist es eben kein Modell, sondern ein Ausprobierspielzeug faktischen ohne Wert!
ich glaube schon, dass die Modelle die Vergangenheit ganz gut abbilden. Das Problem ist eher die Definition, was ist Vergangenheit und was ist Zukunft.
Komplexe Software entsteht nicht über Nacht und die Entwicklung endet nie. Es kommen immer neue Erkenntnisse, Anforderungen und neue technische Möglichkeiten dazu, also werden immer neue Funktionen angepappt und Details verändert. Bis man nach 15-20 Jahren merkt, dass das Ursprungskonzept für die heutigen Anforderungen nicht mehr ausreicht. Normalerweise müsste dann mit den neuen Erkenntnissen, Erfahrungen und Möglichkeiten eine neue Software ab Code-Zeile 1 neu entwickelt werden. Es bedeutet aber die Verdopplung der Ausgaben, die alte muss ja noch gepflegt werden. Also wird häufig weiter „gefummelt“.
Etwas überspitz vielleicht aber durchaus real: Um den Klimaverlauf bis 2020 exakt abzubilden, muss man heute anfangen, da man im Dezember anfangen zu entwickeln, da kennt man den genauen Verlauf. Das Modell ist 2030 fertig, bis 2020 ist alles richtig abgebildet, von 2020 bis 2030 wird Grütze angezeigt. Weil heute niemand weiß, wie es sich in den nächsten 10 Jahren verhält.
Daher zeigen die Modelle rückwirken 200-300 Jahren das Klima ganz gut an, die letzten 20-30 Jahren eher Grütze mit ständigen nachfummeln.
Damit die „wesentliche Einflüsse passen“, muss man diese Einflüsse und deren exakte qualitative und quantitative Wirkung kennen. Kann man gar nicht, beim Klima beruht alles auf Annahmen. Wenn die IT-Götter mal den Verlauf ganz gut treffen, glauben sie wirklich, die Einflüsse richtig hinterlegt haben.
Ist aber Quatsch. Man kann die Wirkung nur genau erklären, wenn man die Ursachen exakt kennt. Wenn man die Wirkung irgendwie nachgeahmt hat, bedeutet das nicht, dass die Nachahmung auf die gleichen Ursachen beruht, sondern das der Entwickler Talent hat. Es ist ein philosophisches Problem der Betrachtung von Simulationen. Wenn man lange genug Kundenwünsche in Anforderungen für Softwareentwickler „übersetzt“ hat, betrachtet man automatisch eine jede Software wie ein Knetekasten, mit dem man jede vom Kunden gewünschte Form kneten kann. Das hat dann Nachweis von physikalischen Zusammenhänge nicht zu tun.
MfG
Peter Georgiev
Es wäre interessant zu wissen in welchem Zusammenhang Enrico Fermi dies gesagt hat. Tatsache ist dass Elefanten existieren die mit dem Schwanz wedeln. Um dies zu erklären reichen wahrscheinlich zwei Parameter nicht aus. Fazit: Man sollte nicht unnötigerweise versuchen die Welt zu erklären.
das können sie. Wenn auch mit einer Unsicherheit von 1:100.000.000.
Modelle sind milde ausgedrückt Quatsch. Sie können einem helfen bestimmte dinge besser zu verstehen, aber die Realität bilden die nie ab. Wer es genau wissen will, der schaue sich die Wettermodelle an und deren Vorhersagequalitäten.
Das Klima kann man übrigens mit Modellen ohnehin nicht bestimmen. Dazu hat man schlichtweg viel zu wenige Daten. In den USA ca. bei 1880 beginnend, in Deutschland ca. um 1870 mit ein paar Ausnahmen.
Und daß man ne Wettervorhersage brauchte merkte man als die spanische Kriegsflotte im Kanal vom Schlechtwetter zerstört wurde.
Weil Trenberth/Kiehl beschreiben ein Perpetuum Mobile der x-ten Art, welches es in der Wirklichkeit nie geben kann!
https://www.ipcc.ch/site/assets/uploads/2018/02/Fig2-11-1-1024×673.jpg
Heute im knappen Text nur zweimal falsch:
1. Das Trenberth/Kiehl Diagramm ist offensichtlich kein Perpetuum Mobile (weder 1. noch 2. Art). Beweis durch Zusammenrechnen der Energieflüsse (1.) und der daraus ableitbaren Entropien (2.)
2. Das Trenberth/Kiehl Diagramm geht in kein Klimamodell ein.
PS: Haben Sie eigentlich auch Angst vor Logarithmentafeln oder anderen Zahlenwerken oder was bewirkt Ihr Problem mit diesem trivialen Diagramm, welches seit knapp 100 Jahren Standard der Meteorologie ist?