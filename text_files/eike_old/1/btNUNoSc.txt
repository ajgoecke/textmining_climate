Der Punkt ist, dass ein computerbasiertes mathematisches Modell irgendeines Prozesses oder Systems nutzlos ist, solange es nicht validiert worden ist. Die Validierung eines solchen Modells muss das Austesten jeder einzelnen Gleichung darin enthalten sowie das Studium jedes einzelnen Parameters, um deren Genauigkeit auf statistischer Grundlage zu erkennen mittels einer Palette von Wahrscheinlichkeits-Verteilungen auf numerischer Grundlage, Standardabweichungen, Korrelationskoeffizienten und Vertrauensbereich. Das Endstadium ist ein sorgfältiger Test der Fähigkeit des Modells, die Folgen von Änderungen der Modellparameter über die gesamte gewünschte Bandbreite zu prognostizieren.
Als Reaktion auf meine Anmerkung, dass kein Modell je validiert worden ist, änderten sie den Titel zu „Klimamodelle – Evaluierung“ nicht weniger als 50 mal. Es gibt nicht einmal in irgendeiner IPCC-Publikation eine Beschreibung, was getan werden muss, um ein Modell zu validieren.
Anstatt validiert zu werden mittels des traditionellen Gebrauchs mathematischer Statistik werden die Modelle „evaluiert“ allein auf der Grundlage der Meinung derjenigen, die sie erfunden haben.
In den Anfängen wurde das Austesten eines Modells ,Nachhersage‘ genannt. Man datierte den Startzeitpunkt zurück mit einer bekannten Ausgangslage und ließ es von da an laufen, um festzustellen, ob es diese Entwicklung nachvollziehen konnte. War das der Fall, konnte man auch dessen realen Prognosen für die Zukunft vertrauen. So wie ich es verstehe wurde dieses Verfahren als Validierung bekannt, und die Modellierer proklamierten Erfolg, weil sie die Variablen so lange bearbeitet hatten, bis das Modell tatsächlich die Situationen der Vergangenheit nachvollziehen konnte. Das Problem war, dass bei dieser ,Nachhersage‘-Korrelation kein Beweis für Ursache und Wirkung erbracht worden ist.
Diese Story der Validierung illustriert, wie das IPCC nicht einmal grundlegende Verfahren durchführt, aber wenn man es dabei ertappt, werden die Ergebnisse solange bearbeitet, bis sie passen, oder man bringt eine neue Terminologie ins Spiel. Meines Wissens werden immer noch keinerlei Validierungen vorgenommen. Die Arbeit von Vincent bietet eine Gelegenheit, auf Kommentare zu meinem letzten Beitrag zu antworten. Er wies darauf hin, dass die Fehler des IPCC alles andere als unbedeutend waren. Sie waren ausgeprägt und unterstreichen, dass kein Vertrauensniveau involviert war. Kritiker meiner Beiträge argumentierten, dass es ausreichende Sicherheit gab, um die Modelle als valide Mittel zu betrachten. Ich wies diese Behauptung in Bausch und Bogen zurück, weil Sicherheit in jeder Phase der AGW-Behauptung unzureichend ist.
Im Rechtssystem wird unterschieden nach Verbrechen aus Affekten und nach Vorsatz, auch die Strafen sind unterschiedlich. Sie werden als zwei separate Verbrechen angesehen wegen der dahinter stehenden Absicht. Die globale Klima-Kampagne beabsichtigte, der Welt zu beweisen, dass die menschliche Erzeugung von CO2 die Ursache für AGW ist. Das belegt den Vorsatz. Es wurde zu einem Verbrechen aus Leidenschaft, nachdem das Verbrechen begangen worden war, weil die Täter es zuließen, sich von ihrer Leidenschaft übermannen zu lassen und allem widerstehen, was die Wahrheit enthüllt. Die betreffenden Leute wussten von Anfang an, dass das, was sie taten, noch weniger als Pseudowissenschaft war, also handelten sie vorsätzlich. Traurigerweise ist es so: wenn sie das nicht wussten – und es gibt viel zu viele, die es nicht wussten – waren sie inkompetent.
Es gibt eine weitere, extrem große Gruppe von Wissenschaftlern aus allen Disziplinen, die niemals die IPCC-Berichte lesen. Diejenigen, die es doch tun, machen die gleiche Erfahrung wie der Meteorologe und Physiker Klaus-Eckart Pulsl:
Ich wurde sehr zornig, als ich entdeckte, dass Vieles von dem, was das IPCC und die Medien uns weismachten, schierer Unsinn war und nicht einmal durch auch nur ein einziges wissenschaftliches Faktum oder irgendeiner Messung gestützt war.
Der Physiker Hal Lewis beschrieb die Arbeit des IPCC so:
Es ist der größte und erfolgreichste pseudowissenschaftliche Betrug, den ich je in meinem langen Leben als Physiker erlebt habe.
Da gibt es keine Unsicherheit.
Anfang der achtziger Jahre, als das Thema Klima zu einer politischen Agenda geworden ist, wurden von allen in dem AGW-Betrug involvierten Personen zwei klare Verpflichtungen gefordert. Die erste war die Übernahme der wissenschaftlichen Verantwortung für das, was erzeugt wurde. Die zweite erfordert sozial-ökonomische Verpflichtungen hinsichtlich der auf dieser Wissenschaft basierenden Politik. Weder wissenschaftlichen noch sozialen oder politischen Verpflichtungen wurde nachgekommen. Schlimmer, sie wurden absichtlich umgangen.
Die Gefahren kristallisierten sich heraus, als der Betrug in die Öffentlichkeit getragen wurde, und zwar mittels Hansens abgekartetem Auftritt vor dem Senatskomitee im Jahre 1988. Ich entschuldige mich nicht dafür, dies immer wieder zu wiederholen und mich zu diesem Thema zu äußern, weil immer noch zu viele versuchen nachzuvollziehen, was da vor sich gegangen war.
In einem Kommentar zu meinem vorigen Artikel [auf Deutsch beim EIKE hier] schrieb Richard Tol:
Das ist alles ziemlich übertrieben. Tim Balls Kernpunkt scheint es zu sein, dass weil wir es nicht genau wissen, wir überhaupt nichts wissen. Es gibt im Leben nur Weniges, dass mit großer Präzision bekannt ist, besonders die Dinge, die eine Rolle spielen.
Die Menschen sind ziemlich gut darin, mit nicht perfekten Informationen umzugehen. Wir wären längst ausgestorben, wenn wir das nicht gewesen wären.
Klimapolitik ist nichts weiter als ein weiterer Fall, Entscheidungen unter Unsicherheiten zu treffen. Wir wissen, wie man das macht.
Dieses törichte Statement bildet die Grundlage dafür, alles zu erklären, was mit der ,Wissenschaft‘ für eine politische Agenda falsch läuft. Diese ,Wissenschaft‘ ist das Werk des IPCC. Erstens, es repräsentiert den umweltlichen Standard-Rückfall, wenn Daten und Wissenschaft unzureichend sind – das Vorsorgeprinzip; der Gedanke, dass es besser ist, überhaupt irgendetwas zu tun, nur für den Fall der Fälle. Nein, das ist es nicht, und zwar aus den Gründen, die ich vor dem kanadischen Parlamentskomitee zu FCKWs und Ozon erläutert habe. Die Wissenschaft schreitet voran mittels Spekulation, obwohl sie es hypothetisieren nennen. Viele Wissenschaftler können jeden Tag viele Spekulationen erzeugen und tun das auch, welche auf wenigen Fakten und Hypothesen beruhen. Heutzutage gieren Medien und Andere nach akademischen Publikationen auf der Suche nach sensationellen, den Globus oder das Leben bedrohenden Spekulationen. Niemals jedoch berichten sie, wenn die Forschungen gescheitert sind.
Meine Frage an die Politiker war, welche Spekulation sie auswählen und ausschlachten würden. Sie können nicht alle heranziehen, und außerdem erweisen sich fast alle als ungerechtfertigt, wenn man sie dem wissenschaftlichen Verfahren des Skeptizismus‘ unterzieht. Ironischerweise macht der Erfolg der Entwicklung das Geld locker, um damit eine verzerrte Liste von Prioritäten zu erstellen. Wir können es uns leisten, dumm zu sein.
Das IPCC wurde unter Federführung des United Nations Framework Convention on Climate Change (UNFCCC) gegründet. Jene Agentur stand unter dem Schirm der Agenda 21 des United Nations Environment Program (UNEP), einer globalen Umweltstrategie für das 21. Jahrhundert. Es war das geistige Produkt von Maurice Strong, welcher es der Welt im Jahre 1992 in Rio de Janeiro vorlegte. Wie alle Endprodukte beruhte sie auf eine Reihe von Vermutungen. Mit der Agenda 21 wurden diese Vermutungen als Prinzipien ausgegeben, welche die Eckpunkte setzten für Maßnahmen und Aktivitäten. Das Prinzip 15 besagt:
Um die Umwelt zu schützen, sollte das Vorsorgeprinzip von den Staaten entsprechend ihren Möglichkeiten weitgehend übernommen werden. Wo es Bedrohungen ernster oder irreversibler Schäden gibt, sollte das Fehlen vollständiger wissenschaftlicher Sicherheit nicht als Begründung dienen, kosteneffektive Maßnahmen zu verschieben, um umweltliche Beeinträchtigungen zu verhindern.
Schauen wir mal genau, was sie damit sagen. Erstens ist es diskriminierend und kontraproduktiv [anti-success]. Es ist nur auf Länder anwendbar, welche sich das leisten können (Fähigkeiten). Wer entscheidet das? Sie tun es! Wir agieren, wenn ernste irreversible Schäden drohen. Wer entscheidet darüber? Sie tun es! Fehlende volle wissenschaftliche Sicherheit ist nicht erforderlich um zu agieren, aber wer entscheidet, was angemessene Sicherheit ist? Sie tun es! Sie gaben sich selbst die Autorität, die Sicherheit zu verwerfen, auf die sich Tol bezog.
Nun wissen wir jetzt, dass die Modelle an jedweden ,Sicherheits‘-Anforderungen scheitern, selbst im grundlegendsten Niveau. Allerdings ist die Sicherheit ungeeignet bei allem was sie sagten und taten und – traurigerweise – immer noch tun. Die Computermodelle sind mathematische Konstrukte, wobei die Erdatmosphäre durch willkürlich gewählte Würfel repräsentiert wird (Abbildung 1):
Für etwa 85% der Oberflächen-Rechtecke gibt es keine Wetterdaten. Die Temperaturdaten werden in einer Höhe zwischen 1 m und 1,25 m über Grund genommen, sind also nicht repräsentativ für die Temperatur über oder unter diesem Niveau. Es gibt praktisch keine Daten über der Oberfläche. Die Datengrundlage zur Konstruktion der mathematischen Modelle repräsentiert die Temperatur in jedwedem Würfel nicht einmal annähernd. Die existierenden Daten genügen in keiner Weise irgendwelchen akzeptablen Niveaus wissenschaftlicher Sicherheit. Die USA beispielsweise haben die beste und mit Sicherheit die teuerste Ausrüstung für die Messung der Temperatur. Anthony Watts hat ermittelt, dass nur 7,9% aller USHCN-Wetterstationen eine Genauigkeit unter 1°C erreichten. Im Zusammenhang: im IPCC-Bericht 2001 heißt es, dass eine Erwärmung um 0,6°C über etwa 120 Jahren nicht natürlich war.
Es gibt nicht eine einzige vom IPCC herangezogene Variable, die auch nur ansatzweise in die Nähe dessen kommt, was eine statistisch signifikante Stichprobe ist. Soweit ich das verstehe, muss das eine Stichprobe sein, die 30% der Datenmenge enthalten muss, um signifikant zu sein [Original: that is a sample needs to be 30% of the population for significance]. Die IPCC-Leute sollten das wissen. Als Mitglieder der Weltwetterorganisation WMO arbeiten sie die ganze Zeit mit 30-Jahre-Mittelwerten. Das Problem ist, dass selbst dieser Zeitraum bedeutungslos ist, wenn es um Klimaaufzeichnungen geht. Eine Stichprobe über 30 Jahre mag statistisch für 100 Jahre signifikant sein, aber nicht für längerzeitliche Aufzeichnungen.
Das IPCC und seine Gründungsmitglieder wissen, dass Sicherheit entscheidend war, also machten sie sich daran, diese herbei zu schummeln. Die Arbeitsgruppe I (WG I) produziert den ,wissenschaftlichen‘ Beweis, dass menschliches CO2 Erwärmung verursacht. Sie repräsentieren nur einen kleinen Anteil der Leute, die sich wissenschaftlich betätigen, und nur sehr wenige von Ihnen haben eine Ausbildung in Klimatologie. Sie sind zumeist nur Spezialisten in einem kleinen Teilbereich des Klima-Puzzles. Ihre Ergebnisse werden ohne Hinterfragen übernommen und bilden die Grundlage für die Arbeit der Arbeitsgruppen II und III. Dies bedeutet, dass sie lediglich auf die Auswirkung und die erforderliche Abschwächung schauen, die notwendig für die Sicherheit ist, dass es eine Erwärmung geben wird.
Die Personen in der WGI sind sich der Begrenztheit der Daten bewusst. Ganz zu Anfang des 5. Zustandsberichtes gibt es einen Abschnitt mit der Überschrift [übersetzt] „Behandlung der Unsicherheiten“ [Treatment of Uncertainties]. Inzwischen weiß man, wie man das macht: mit der Angabe von „Wahrscheinlichkeiten“, die eine ohnehin schon ungenügende Präzision vortäuschen und verbreiten.
Abbildung 2 zeigt die Tabelle der ,Antriebe‘ [forcings] aus dem AR 5. Dies sind die Variablen, welche sie hinsichtlich menschlicher Gründe des Klimawandels für bedeutend halten. Rechts findet sich eine Spalte „Vertrauensniveau“ [Level of Confidence] Wer bestimmt das, und wie wird es gemessen? Es ist standardmäßig eine subjektive Maßzahl, aber das IPCC bevorzugt sie. Nur eine Variable ist markiert als ,Sehr Hoch‘, und zwar das CO2, eine eigennützige Abschätzung, die sich leicht widerlegen lässt. Nur drei der 11 Variablen sind als ,Hoch‘ gelistet. Man betrachte davon lediglich eine, nämlich Aerosole. Kann mir jemand die Daten beschaffen der Anzahl und der Natur der Aerosole in der Atmosphäre ebenso wie deren Volumen und deren zeitlicher Änderung über jedweden Zeitraum?
Die Tabelle ,Antriebe‘ in Abbildung 2 ist modifiziert aus der im AR 3 erschienenen Tabelle (Abbildung 3):
Weiter, die Spalte ist überschrieben worden als LOSU für „Level of Scientific Understanding.” Was in aller Welt ist das außer einer anderen subjektiven Maßzahl. Ich bin nicht überzeugt, dass „Vertrauen“ eine Verbesserung war. Es gab eine marginale statistische Verbesserung, weil die Anzahl der Klassifizierung „hoch“ von 2 von 11 auf 3 von 11 stieg und die Anzahl von „low“ von 4 auf 2 sank. Liest man jedoch den Report, zeigt sich, dass nahezu für jede Variable das LOSU oder Vertrauensniveau niedrig bis nicht existent ist.
Der größte Teil der Öffentlichkeit einschließlich fast aller Medien und der meisten Wissenschaftler haben niemals den Report der WG I gelesen. Dessen einzig mögliches Ziel war es, dass falls er später hinterfragt wird, man sagen kann, dass sie wussten, es gibt ernste Probleme hinsichtlich Daten und Verfahren. Ein Hauptgrund dafür, dass der Report nicht gelesen worden ist, ist der Umstand, dass die Politik absichtlich dessen Veröffentlichung zeitlich erst nach Freigabe der Summary for Policymakers angeordnet hat. Diese SPMs dienten nur dazu, absichtlich Zweifel herunter zu spielen und Sicherheit zu übertreiben. Um wenigstens ein Mindestniveau von Sicherheit vorzuspielen, welches ein Politiker oder Entscheidungsträger verlangt, bevor darüber entschieden wird, muss das vollständige Fehlen von Sicherheit in der Wissenschaft überspielt werden. Man betrachte die von Jones erstellte Graphik mit der globalen Temperaturzunahme um 0,6°C innerhalb von 120 Jahren. Die tatsächlich veröffentlichte Zahl war 0,6°C ±0,2°C. Hätte man dies so ausgedrückt, dass Politiker und die Öffentlichkeit es versteht, hätte man 0,6°C ±33,3% gesagt. Es gibt keine Person oder Gruppe, nicht einmal einen Politiker, der seine Politik auf die Grundlage derartiger Prozentangaben basieren würde. Es gibt strikte Grenzen der tolerierbaren Sicherheit in jeder anderen wissenschaftlichen Forschung – warum nicht in der Klimawissenschaft? Die Frage lautet, welcher Prozentsatz an Sicherheit vernünftig ist. Diese Frage überschreitet die Grenze in der Klimatologie zwischen Wissenschaft und sozial-ökonomischer Politik.
Im Jahre 1989 erlebte ich ein gutes Beispiel auf einer Konferenz in Edmonton zu Klimaprognosen und deren Implikationen für Land- und Waldwirtschaft. Der Klimamodellierer Michael Schlesinger hielt einen Vortrag, in welchem er Klimaprognosen der besten fünf Modelle des Tages miteinander verglich. Er argumentierte, dass diese valide sind, weil sie alle Erwärmung zeigen. Dies war zu 100% vorhersagbar, weil sie allesamt so programmiert waren, dass sie mit einer CO2-Zunahme steigende Temperaturen zeigten. Untersuchte man die Ergebnisse im kontinentalen Maßstab stimmten sie nicht überein. Ein Modell zeigte Abkühlung in Nordamerika, ein anderes Erwärmung. Das Publikum, bestehend aus Entscheidungsträgern aller Art, brauchte größere Sicherheit. Jemand fragte Schlesinger nach der Genauigkeit seiner Vorhersage trockenerer und wärmerer Bedingungen für Alberta. Die Antwort lautete 50%. Der Fragesteller erwiderte, dass diese Angabe für ihn nutzlos war. Sein Ministerium plante Wiederaufforstung und brauchte 95% Sicherheit.
Es gibt noch eine andere Möglichkeit, politische Reaktionen auf Unsicherheit zu testen. Viele US-Senatoren kannten die Probleme mit der Klimawissenschaft des IPCC durch die Arbeit von Senator James Inhofe. Als sie im Jahre 1995 aufgefordert worden waren, das Kyoto-Protokoll zu ratifizieren, machten sie es anders. Sie wollten nicht direkt abstimmen, weil dies hätte dazu führen können, weniger als grün zu erscheinen. Stattdessen schauten sie auf die sozial-ökonomischen Kosten der Umsetzung desselben, indem sie über die Byrd/Hagel Resolution abstimmten. Sie kamen zu dem Ergebnis, dass die Kosten bei Weitem über die Vorteile hinausgehen und stimmten 95 zu Null nicht zu Kyoto ab. Die Senatoren waren sich zu 100% sicher hinsichtlich der Auswirkungen, unabhängig von der Wissenschaft.
Vincent Gray verweist auf eine bedeutende Folgerung in Climate Change 95, in der es heißt:
Nichtsdestotrotz, die Summe der Beweise zeigt, dass es einen merkbaren menschlichen Einfluss auf das globale Klima gibt.
Dieser Kommentar ist ein direktes Zitat des infamen Beitrags von Benjamin Santer zum Kapitel 8. Das Komitee bzgl. Kapitel 8 hatte folgender Bemerkung zugestimmt:
Während Einige aus den hier diskutierten Grundlagen einen signifikanten Klimawandel erkannt haben wollten, gibt es bis heute keine einzige Studie, die den Klimawandel insgesamt oder teilweise anthropogenen Ursachen zugeordnet hat.
In dem Report machte Santer daraus:
Die Gesamtheit statistischer Beweise in Kapitel 8, wenn man sie im Zusammenhang unseres physikalischen Verständnisses des Klimasystems betrachtet, zeigen jetzt einen merkbaren menschlichen Einfluss auf das globale Klima.
Avery und Singer bemerkten dazu im Jahre 2006:
Santer kehrte die ,Klimawissenschaft‘ des gesamten IPCC-Reports in sein Gegenteil und damit auch den politischen Prozess bzgl. globaler Erwärmung! Der ,merkbare menschliche Einfluss‘, den das IPCC vermeintlich enthüllt haben sollte, wurde tausende Male in Medien auf der ganzen Welt zitiert und war der ,Stopper‘ in Millionen Debatten unter Nichtwissenschaftlern.
Dies war das erste ungeheuerliche Beispiel von Korruption und Betrug, mittels derer eine Sicherheit vorgegaukelt worden ist, die in keiner Phase des gesamten IPCC-Verfahrens existierte. Falls man mit seinen Modellen spielen oder Wissenschaft im Labor betreiben will – gut. Aber man muss den Verpflichtungen und Sicherheiten der Wissenschaft genügen. Das IPCC hat das nicht getan. Zieht man jedoch die Ergebnisse dieser ,Wissenschaft‘ heran und präsentiert sie als offizielle Politik, dann ist ein weiterer Satz an Verpflichtungen und Sicherheiten erforderlich. Das IPCC hat auch dem nicht Rechnung getragen.
Es ist unmöglich, das zu korrigieren, was das IPCC gemacht hat, teilweise weil jeder einzelne Report auf den Betrügereien früherer Berichte aufbaute. Das IPCC muss vollständig eliminiert werden, und alle nationalen Wetterdienste müssen zur Datensammlung verpflichtet werden einschließlich der Rekonstruktion von Klimaten der Vergangenheit. Das ist der einzige Weg, auch nur ein Minimum an Sicherheit der Erkenntnisse zu etablieren als Grundlage für Verständnis der Prozesse und damit einer genaueren Prognose.
Link: https://wattsupwiththat.com/2018/06/18/anthropogenic-global-warming-agw-a-premeditated-crime-against-science-justified-with-artificial-certainty/
Übersetzt von Chris Frey EIKE
Also müßte man sich nur mit dem TE (Treibhauseffekt) befassen, ad absurdum führen und die ganze daran hängende Folgediskussion und schlimmen Maßnahmen wären auf einen Schlage obsolet.
Es ist ganz einfach: Treibhausgase sind in der Lage, Wärmeenergie in das Vakuum des Alls zu verlieren und vergrößern dadurch den Fließquerschnitt, verringern die Wärmeflußdichte und damit die Temperatur. Das wäre die Argumentation.
Frage: Was ist aus dem Prozeß in Kalifornien geworden, zu dem u.a. Lord Monckton eine über ein Jahr hin entwickelte Studie eingereicht hat? Der als naturwissenschaftlich zumindest interessierte Richter wollte doch mittels eingereichter Studien zu einem Urteil kommen?