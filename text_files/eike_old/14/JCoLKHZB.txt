Jedes Messergebnis wird unter speziellen Messbedingungen erzielt. Diese sind möglichst konstant zu halten und stets anzugeben. Das Messresultat ist immer mit einer Messunsicherheit (Messabweichung, veraltet: Fehler) behaftet.
Die für Messungen verwendeten Messgeräte wurden im Laufe der Zeit immer genauer.
Problematisch war in der 2. Hälfte des 20. Jahrhunderts der Übergang von Analog-Anzeigen auf Digitalanzeigen. Die digitale Anzeige von Messergebnissen gaukelt oft dem ungeschulten Anwender eine Genauigkeit vor, die nicht vorhanden ist. Liegt beispielsweise die abgelesene Spannung von 12,62 V an den Messpunkten an, die wir ablesen? Oder fahren wir tatsächlich die 88 km/h, die uns der Digital-Tacho anzeigt? Nein, Messungen sind immer mit Messabweichungen behaftet. Im allgemeinen Sprachgebrauch wird für eine Messabweichung meist (noch) der Begriff Messfehler oder kurz Fehler verwendet. Jedoch empfehlt z.B. die DIN-Norm 1319, diesen Begriff durch den der
(Mess-)Abweichung zu ersetzen.
Abb 1: Welcher Waage vertrauen wir mehr? Analog oder Digital…
Bild links: Frank C. Müller, Personenwaage fcm, CC BY-SA 4.0, Bild rechts: Digital-Personenwaage (Bild ohne Lizenz)
1.2.) Kalibrieren und Eichen
Kalibrieren bzw. Eichen (Eichamt) ist der Vergleich einer bezifferten Skala mit einem Normal. Jedes Messgerät (Messeinrichtung), das auf einem physikalischen Messprinzip beruht, muss an ein Sekundär- oder Primär-Normal mit geringerer Messunsicherheit gekoppelt sein. Kalibrierte bzw. geeichte Messgeräte sind somit mit geringeren Messunsicherheiten behaftet. Trotzdem sind Messungen mit diesen Geräten nicht fehlerfrei. Es verbleibt die Messunsicherheit des Gerätes und die Fehleranfälligkeit durch die messende Person.
Als Erkenntnis dieser Einleitung sollte erreicht werden, dass der Leser an fremde Messergebisse kritisch herangehen sollte. Informationen zu den Messgeräten, den Mess-Bedingungen, Anzahl der Messungen, u.s.w. sind unerlässlich. Deutlich ausgedrückt: Ohne Zusatzinformationen sind Messergebisse wertlos! Weitere Informationen in den nächsten Kapiteln. Diese grundlegenden Voraussetzungen werden in den folgenden Kapiteln auf die Zuverlässigkeit der Global-Temperatur-Daten angewendet.
2.) Art der Messabweichungen
2.1.) Zufällige Messabweichungen
Zufällige Messabweichungen entstehen aufgrund nicht beherrschbarer, nicht determinier- ter Einflüsse. Sie sind nicht vorausbestimmbar. Werden die Messungen am selben Messobjekt unter gleichen Bedingungen wiederholt, führen sie zur Streuung der Messwerte. Deshalb lassen sich diese zufälligen Messabweichungen sehr gut mit mathematisch statistischen Methoden ermitteln und eingrenzen. Diese Methode soll jedoch nicht innerhalb dieses Aufsatzes behandelt werden. Es gibt hierzu genügend gute Fachliteratur.
2.2.) Systematische Messabweichungen
Systematische Messabweichungen haben einen konstanten Betrag mit einem bestimmten Vorzeichen oder unterliegen nur einer sehr langsamen Veränderung aufgrund einer Ursache, die die Messgröße nur in eine Richtung verändert. Sie führen zu einer immer gleichen, zeitlich konstanten Differenz des Messwerts vom wahren Wert, d. h. zu einem „falschen“ Messergebnis. Systematische Messabweichungen sind durch Wiederholungen der Messungen unter gleichen Bedingungen nicht erkennbar!
3.) Temperatur-Messungen
Die Bestimmung der genauen Temperatur eines Systems ist trotz der alltäglichen Anwendungen keine triviale Sache – jedenfalls nicht, wenn man es genau nimmt. Im Gegenteil, die genaue Temperatur-Messung ist eine besondere Herausforderung. Eine Vielzahl von Unwägbarkeiten beeinflussen eine exakte Messung: Richtiges Thermometer für die jeweilige Anwendung, Genauigkeit des Thermometers, Sorgfältigkeit der Messung, äußere Einflüsse wie Licht oder Wärmestrahlung, u.s.w.. Einfach formuliert, die möglichen systematischen Fehler sind zahllos. Temperaturmessungen sind häufig falsch.
3.1.) Definition der Temperaturmessung
Sämtliche Verfahren zur Bestimmung der Temperatur eines thermodynamischen Systems durch Vergleich mit einer Temperaturreferenz, z.B. dem Thermometer. Hierfür können im Prinzip jegliche physikalischen Größen (z.B. Länge, elektrischer Widerstand, Strahlungsgrößen) oder chemischen Eigenschaften (z.B. Temperatur-Messfarben) verwendet werden, die sich mit der Temperatur gesetzmäßig ändern.
3.2.) Arten von Thermometer
Die Zahl der unterschiedlichen Arten von Thermometern ist nahezu unüberschaubar. Prinzipiell wird unterschieden zwischen Berührungs-Thermometern und Thermometern, die auf berührungslosen Verfahren beruhen. Die Diskussion zu den unterschiedlichen Arten von Thermometern würde den Rahmen dieses Aufsatzes sprengen.
4.) Temperatur-Messung in der Meteorologie / Klimatologie
Inzwischen nähern wir uns der eigentlichen Fragestellung zur „Zuverlässigkeit der Temperatur-Messdaten der letzten 170 Jahre“. Betrachten wir die Fakten zur Ermittlung der mittleren Global-Temperaturen in diesem Zeitraum. Doch zunächst 2 entscheidende Begriffs-Definitionen.
4.1.) Direkte Messung durch Thermometer
Viel Menschen haben schon eine meteorologische Mess-Station gesehen. Seit dem 17. Jahrhundert (England) werden weltweit regelmäßig verschiedene Parameter des Wetters gemessen. Insbesondere ab dem 19. Jahrhundert sind Mess-Stationen in vielen Ländern weit verbreitet. Hauptzweck ist es, die Daten-Grundlage für die lokale Wetter-Vorhersage zu erlangen. Nicht zu vergessen die Einsätze von Wetter-Ballonen oder Flugzeugen durch Meteorologen. Dadurch wird es möglich, auch Daten von unterschiedlichen Luftschichten der Atmosphäre zu erhalten.
Abb. 2: DWD-Wetterwarte bei Weiden/Opf. (Bild: ohne Lizenz)
4.2.) Indirekte Messung durch Proxy-Daten
Die meisten Leute werden sich bei der aktuellen Klimawandel-Diskussion fragen: Woher kommen eigentlich die Temperatur-Daten aus dem Mittelalter, aus dem Römischen Zeitalter, viele Jahrtausende oder 100 Tausende oder sogar Millionen Jahre zurück? Da gab es keine Thermometer bzw. gar keine Menschen! Wie ist das möglich?
Natürlich wurden die Temperaturen bei den Römern nicht direkt gemessen, sie hatten definitiv noch keine Thermometer. Hier sprechen die Forscher von so genannten Proxy-Daten. Das sind Temperatur-Daten, die durch indirekte Messungen erhalten werden. „Proxy-Daten“ ist ein Sammelbegriff für Informationen, die indirekt Auskunft über vergangene Klimaverhältnisse geben. Proxydaten erhalten wir aus Baumringen und Wachstumsringen in anderen Organismen (wie Korallen), Isotopenmessungen in Eisbohrkernen oder Sedimenten, oder biologische Proben wie etwa Pollenanalysen.
5.) Zuverlässigkeit der Temperatur- Daten
5.1.) Zuverlässigkeit der Proxy-Daten
Proxy-Daten werden grundsätzlich über längere Zeiträume der Erd-Vergangenheit ermittelt.Zum Beispiel bei Baumringen hunderte von Jahren oder Eisbohrkerne 10 Tausende von Jahren. Die große Stärke der Proxy-Daten-Ermittlung liegt in der in sich abgeschlossenen Methodik, der Konsistenz der Daten. Nehmen wir als Beispiel die zeitliche Abfolge der Schichten in Eisbohr-Kernen. Alle Proben werden an einer Stelle eines Gletschers bzw. eines Inlandeis-Panzers entnommen, gleich aufbereitet und mit derselben Methode gemessen. Vorausgesetzt, es werden keine groben Fehler gemacht, sind die Ergebnisse in sich konsistent und zuverlässig. Die besondere Stärke liegt in der guten Genauigkeit der relativen Änderungen innerhalb der jeweiligen Messreihen. Die Ermittlung der absoluten Temperatur im betrachteten Zeitraum ist kaum möglich.
5.2.) Zuverlässigkeit der direkt gemessenen Temperaturen seit 1850
Somit sind wir beim Hauptpunkt dieses Aufsatzes. Die gesamte Diskussion zum Thema „Klimawandel“ und „Erderwärmung“ basiert auf den Temperaturdaten der letzten 170 Jahre. Warum wird immer auf das Jahr 1850 referenziert? Der Zeitraum rund um das Jahr 1850 gilt als der Startpunkt der industriellen Revolution. Sozusagen der Beginn allen Übels bei vielen Protagonisten. Doch wie zuverlässig sind die weltweit direkt gemessenen Daten der letzten 170 Jahre? Das soll in den folgenden Kapiteln behandelt werden.
6.) Pseudo-Wissenschaft. „Gefahr“ durch Klima-Erwärmung.
Eine verbreitete Aussage bei den Physikern lautet „Wer viel misst, misst Mist“. Die Physik legt höchste Maßstäbe in die Ermittlung und Verarbeitung von Mess-Ergebnissen. Deshalb sind Physiker auch besonders kritisch, wenn sie Messungen in anderen Fachgebieten der Naturwissenschaft und Ingenieurs-Wissenschaften beurteilen.
6.1.) Ivar Giaever bezeichnet die Klimawissenschaft als Pseudo-Wissenschaft.
Der hoch angesehene norwegisch amerikanische Physiker und Nobelpreis-Träger Ivar Giaever bezeichnete in dem viel beachteten Gast-Vortrag während des Nobelpreisträger-Treffens von 2012 in Lindau die Klimawissenschaft, basierend auf den seit 1850 ermittelten jährlichen Durchschnitts-Temperaturen der Erde, als Pseudo-Wissenschaft. Die Begründung ist nicht von der Hand zu weisen. Selbstverständlich verunglimpfen die „Klima-Alarmisten“ diese Position und versuchen die Argumente und Beweise von Giaever klein zu reden. Es sei betont, dass die nachfolgend beschriebenen Giaever-Kritikpunkte grundsätzlichen wissenschaftstheoretischen Prinzipien entsprechen. Andere Schlussfolgerungen wurden von ihm durch konsequent logische Denkweise erarbeitet.
Abb. 3: Ivar Giaever, Physik-Nobelpreisträger und Klima-Kritiker [1;2]. Bildlizenz: NTNU Vitenskapsmuseet, Ivar Giæver, verkleinert von Dr. Andreas Karl, CC BY 2.0
Giaevers Hinweise und Kritikpunkte sind vielschichtig, gleichzeitig typisch für einen Physiker, der über den eigenen Tellerrand hinausschaut.
6.2.) Ivar Giaevers Argumente und Beweise.
Außerordentlich bemerkenswert ist die Grundposition, mit der Giaever an die Thematik heran geht. Eigentlich interessierte ihn das Thema Klimawandel nicht. Er wurde gebeten dazu einen Gast-Vortrag in Lindau zu halten. Alleine diese Haltung, den Sachverhalt von außen als neutrale Person zu analysieren, spricht Giaever von Parteilichkeit frei. Er betrachtet die Fakten und Daten als kritischer Physiker, der sein gesamtes Wissenschaftler-Leben seine eigenen und fremde Mess-Ergebnisse bewerten musste. Sein Urteil ist vernichtend. Der Umgang und die Bewertung der gemessenen Temperaturen der Klimaforschung sei eindeutig eine Pseudowissenschaft.
6.1.1.) Grundsätzliche Kritik an der Genauigkeit
Giaevers Grundkritik besteht zunächst in der Angabe von zehntel Graden bei den Temperaturen zur Erderwärmung im Betrachtungs-Zeitraum. Eine angebliche Erwärmung von weniger als 1,5°C in 170 Jahren sei eine höchst erstaunliche Kontinuität des Klimas, völlig unabhängig von den Ursachen der Erwärmung. Die Temperatur-Angabe mit einer Auflösung von 0,1°C ist messtechnischer und physikalischer Unsinn.
Dazu ein treffendes Zitat des Mathematik-Genies Carl Friedrich Gauß: „Durch nichts zeigt sich mathematischer Unverstand deutlicher als durch ein Übermaß an Genauigkeit im Zahlenrechnen.“ Die Naturwissenschaft spricht bei der Verwendung von Messwerten von „Signifikanten Stellen“. Der korrekte Umgang mit Messwerten bezüglich der signifikanten Stellen wird sogar in der DIN 1333 beschrieben. Der Autor dieses Aufsatzes hatte in seinem Studium ein prägendes Erlebnis zu diesem Thema. Der damalige Physik-Professor kündigte im Vorfeld der Physik-Vordiplom-Klausur an: „Prüflinge, die den Rechenweg und das Ergebnis einer Aufgabe korrekt erarbeitet haben, jedoch das Ergebnis mit zu vielen signifikanten Stellen angeben, bekommen Punktabzug.“
6.1.2.) Unterschiedliche Thermometer vor Ort und im Laufe der Zeit
Giaever kritisiert zu Recht, es sei mehr als naiv zu glauben, die auf der Welt verteilten Thermometer (Abb. 3) seien alle von gleicher Qualität und zeigen alle die korrekte Temperatur an. Dazu kommt die technische Entwicklung der letzten 170 Jahre. Selbst die optimistischste Phantasie reicht nicht aus zu glauben, all die weltweit über viele Jahrzehnte tausende Thermometer seien im Laufe der Zeit gleich genau und gleich zuverlässig gewesen.
Abb. 4: Thermometer (17. Jhdt.) und modernes Infrarot-Thermometer. Bildlizenzen: Links: Ferdinand II de‘ Medici [attr.] Museo Galileo, Termometri cinquantigradi inv 85 IF 46774, CC BY-SA 4.0. Rechts: Hedwig Storch, 1024 Pyrometer-8445, CC BY-SA 3.0
6.1.3.) Ungleiche Verteilung der Thermometer auf der Erde
Alleine die Tatsache der ungleichmäßigen Verteilung der meteorologischen Mess-Stationen über die Erdoberfläche reicht aus, die Berechnung einer fiktiven Durchschnitts-Temperatur der Erde als unrealistisch bzw. unmöglich zurückzuweisen. Abbildung 4 zeigt die Häufung der Mess-Stationen auf der nördlichen Erdhalbkugel und den Industrie-staaten. Weite Bereiche Afrikas, Südamerikas, Kanadas, Grönlands und von Sibirien werden nicht abgedeckt. In der Antarktis, die einen großen Einfluss auf das Erdklima hat, gibt es nur wenige Stationen an der Küste.
Abb. 5: Verteilung meteorologischer Mess-Stationen auf der Erde im 21. Jhdt. [3;4]
Abb. 6: Gemeldete Wetterwerte für die Woche vom 2. bis 8. Januar 2000 (Schiffe) [3;4]
Ca. 70% der Erde sind mit Wasser bedeckt. Wie die Forschungen der letzten Jahrzehnte ergeben haben, spielen die Ozeane eine entscheidende Rolle für das Klima der Erde. Entgegen der meteorologischen Bedeutung der Ozeane ist die dort die Abdeckung der Mess-Stationen nur mangelhaft. Sowohl in Abbildung 5 als auch in Abbildung 6 ist zu erkennen, welche riesigen Lücken nicht durch Messungen abgedeckt werden. Abbildung 7 zeigt die zeitliche Entwicklung der über die Erde verteilten Mess-Stationen über einen Zeitraum von 100 Jahren.
Abb. 7: Meteorologische Mess-Stationen von 1885-1985 [3;4]
6.1.4.) Veränderungen im Umfeld von Mess-Stationen und deren Einfluss auf
die gemessenen Temperaturen.
Beispiele: Eine Mess-Station lag 100 Jahre außerhalb einer Stadt, inzwischen ist die Stadt um die Station herum gewachsen. Nahe einer Mess-Station gab es keinen nennenswerten Bewuchs, dann wurde in der Nähe ein Wald aufgeforstet. In 30m Entfernung einer Mess-Station wurde ein Gebäude errichtet. Die Liste solcher Beispiele kann beliebig verlängert werden. Niemand ist in der Lage, die veränderten Einflüsse aus der unmittelbaren Umgebung der Mess-Stationen zu bewerten und einzurechnen. Ein Argument Giaevers, das sicherlich einen großen Einfluss auf den Fehler in der zeitlichen Betrachtung hat.
6.2.) Giaevers philosophische Grundsatz-Kritik zur Klima-Diskussion
Ivar Giaever begnügt sich nicht mit den reinen Sachargumenten oder den Beweisen, die er vorgelegt hat. Er blickt tiefer, macht sich grundsätzliche Gedanken zur Klima-Debatte. Seine im Folgenden beschriebenen Überlegungen haben den Autor dieses Aufsatzes am meisten beeindruckt und gleichzeitig überzeugt.
Giaever blickt auf die Welt als Gesamtes und auf sich als Person. Alles und jeder ist betroffen von Veränderung und Alterung, das kann niemand bestreiten. Er fragt, warum in den meisten Bereichen der politischen und gesellschaftlichen Diskussion die Dinge sich nur zum Negativen bewegen, immer wieder negativ, negativ. Niemand kann bestreiten, dass die Entwicklung der Menschheit seit 1850 einen positiven Weg genommen hat. Die Menschen leben angenehmer, sicherer, Hungern weniger und werden älter. Was soll so negativ an der Temperatur-Entwicklung sein? Wer sagt uns, was die „Optimale Temperatur auf der Erde ist? Wie bereits gesagt, alle Klimaforscher referenzieren auf die Temperatur um 1850. Es ist absolut unwahrscheinlich, dass ausgerechnet diese Temperatur optimal gewesen sein soll. Im Gegenteil, die Klimatologen sprechen vom Römischen Temperatur-Optimum und dem Mittelalterlichen Temperatur-Optimum.
6.3.) Ergänzende Informationen zur Klimaerwärmung (CO2-Einfluss)
Die Zeit um 1850 gilt als das Ende der „Kleinen Eiszeit“, eine Kaltperiode über ca. 400 Jahre innerhalb der aktuellen Eiszeit. Anders formuliert: Um 1850 begann eine Warmphase innerhalb der aktuellen Eiszeit. Gleichzeitig begann um das Jahr 1850 auch die industrielle Revolution (s. unter 5.2.). Folgt nach einer Kaltperiode eine Warmzeit, so muss sich die Erde auch ohne Zutun des Menschen erwärmen. Eine einfache und logische Schlussfolgerung. Schließlich wurde die kleine Eiszeit ja nicht vom Menschen verursacht. Das IPCC (Klimawandel-Organisation der UNO) behauptet jedoch seit 2016, dass die gesamte Erderwärmung seit 1850 ausschließlich auf die Wirkung des anthropogenen CO2 zurückzuführen ist. Mit massiven Auswirkungen auf die Wirtschaft und Energie-Politik der jeweiligen Staaten. Leider ist ausgerechnet Deutschland der Protagonist dieser unsäglichen Ideologie.
Abb. 8: Veröffentlichung von Patrica Adams [5]
Was die „Klima-Alarmisten“ der religiös anmutenden Glaubensgemeinschaft übersehen, ist die Entwicklung in den entscheidenden Ländern Indien und China. Beide haben dem Pariser Klima-Abkommen zugestimmt, sind jedoch berechtigt bis 2050 soviel CO2 auszustoßen, wie sie wollen. Patricia Adams hat kürzlich einen Aufsatz mit dem treffenden Titel „THE RED AND THE GREEN CHINA´S USEFUL IDIOTS“ veröffentlicht.
Mit der von der Politik ausgerufenen Energiewende in Deutschland zerstören wir unsere sichere und unabhängige Energie-Versorgung. Hierzu gibt es viele Veröffentlichungen von unabhängigen Wissenschaftlern, Ökonomen und Ingenieuren. Am bekanntesten ist wohl der ehemalige Chef des ifo-Instituts Prof. Hans-Werner Sinn. Er warnt seit vielen Jahren vor dem Umbau der deutschen Strom-Versorgung hin zu einem Übermaß an regenerativer Stromerzeugung.
6.4.) Stimmen anderer Wissenschaftler
Es gibt auch Wissenschaftler, die sich in Veröffentlichungen kritisch zur Problematik der Ermittlung der mittleren Lufttemperatur der Erde in den letzten 170 Jahren geäußert haben. Verstärkt wird diese Kritik durch geachtete wissenschaftliche Stimmen, welche die Methodik der Temperatur-Ermittlung, das Verständnis des Klimas und den Ausblick auf die Zukunft des Klimas durch die Klimawissenschaftler mit ihren aufwändigen Computer-Modellen in Frage stellen. Dazu 2 ausgewählte Bespiele.
6.4.1.) Fundamentalkritik durch das Genie Freeman Dyson
Freeman Dyson (s. Abb. 9, links), der geniale Mathematiker und Physiker lehrte über 40 Jahre Jahre lang an der Princeton University am Lehrstuhl für Theoretische Physik. Sein Vorgänger auf diese Position war Albert Einstein. Leider ist Freeman Dyson am 28.02.2020 im Alter von 96 Jahren verstorben. In Amerika, sowie in naturwissen-schaftlichen Kreisen, galt Freeman Dyson als Genie. Wegen seiner freundlichen und humorvollen Art war er ein „Wissenschafts-Superstar“. In Interviews wusste er immer zu überraschen und sorgte für Heiterkeit. Andererseits war er kein Mensch, der gemütlich auf der Mainstream-Welle schwamm (s. Abb. 9, rechts).
Abb. 9 Freeman Dyson (2007) am Institute of Advanced Study. Bildlizenz: Monroem, Freeman dyson, CC BY-SA 3.0
Bereits Ende der 60er-Jahre und Anfang der 70er-Jahre war Dyson an den ersten Computer-Modellrechnungen zur Darstellung und Voraussage des Wetters beteiligt. Kein Wissenschaftler auf der Welt kann diese jahrzehntelange Erfahrung auf diesem Gebiet vorweisen. Kaum jemand kennt die Stärken und Schwächen der Klima-Vorhersage besser als er. In vielen Interviews der letzten 15 Jahre wurde er zu seiner Position in der Diskussion zum Klimawandel befragt. Seine Haupt-Thesen stellte er folgendermaßen (hier vereinfacht) dar: 1.) Die Wissenschaft ist noch nicht annähernd in der Lage die Komplexität des Wetters und des Klimas zu verstehen. Viele Fragen zur Wolkenbildung, der Rolle der Ozeane, Einflüsse der Sonne, usw. sind ungeklärt. 2.) Versuche, das Klima über längere Zeiträume vorauszusagen, müssen zwangsläufig scheitern. 3.) Der Einfluss des anthropogenen CO2 zur Erderwärmung der letzten 170 Jahre wird überbewertet. Dagegen werden die positiven Effekte der gestiegenen CO2-Konzentration in der Atmosphäre nicht ausreichend berücksichtigt.
6.4.2.) Positive Auswirkungen der erhöhten CO2-Konzentration
Die USDA (U.S. Department of Agriculture) meldet seit vielen Jahren eine stetig steigende weltweite Getreideernte. Diese Meldungen beschränken sich nicht auf nur auf die weltweite Weizenernte (s. Abb. 10) sondern sind für alle wichtigen Getreidearten wie Reis, Gerste, Mais oder Soja zu beobachten. Wo sind die Auswirkungen der Hitzeperioden, Trockenperioden, Stürmen und Hochwasser, wie sie von den Klima-Alarmisten propagiert werden? Fakt ist, CO2 ist der mit Abstand beste Pflanzendünger.
Abb. 10 Weltweite Weizenernte der letzten 20 Jahre (Quelle: USDA)
Abb. 11 Bild aus einer NASA-Studie von 2016 „Carbon Dixoide Fertilisation, Greening Earth“ [6]
Eine NASA-Studie aus 2016 belegt eindeutig den Düngungs-Effekt durch vermehrtes CO2 in der Atmosphäre. Die Satelliten-Daten (s. Abb. 11) zeigen einen unerwarteten und höchst erstaunlichen Effekt. Die Erde wird grüner! Dank der erhöhten CO2-Konzentration wachsen die Pflanzen besser und schneller, die Wüsten der Erde werden kleiner.
Die Weltbevölkerung steuert bald auf 8 Mrd. Menschen zu. Deshalb sollte die Welternährungs-Organisation der UNO (FAO) eine Studie in Auftrag geben, wieviel Menschen die Erde im Jahr 1850 mit einer CO2-Konzentration von 280 ppm im Vergleich zu heute mit einer CO2-Konzentration von 410 ppm hätte ernähren können. Eine spannende Frage, zumal das Jahr 1850 als „gute“ vorindustrielle Zeit gilt. Die Klima-Alarmisten fordern doch die Rückkehr zu den CO2-Werten der Atmosphäre vor 170 Jahren.
6.4.3.) Bewertung der Temperaturdaten durch Pat Frank
Unterhalb des Radars der Öffentlichkeit wird der der Druck im „Schnellkochtopf“ der Klimawandel-Diskussion immer größer. Die klimakritischen, nicht IPCC-konformen wissenschaftlichen Papers sind von Jahr zu Jahr in größerer Anzahl zu vermelden. Bisher schaffen es jedoch die gleichgeschalteten Medien, besonders in Deutschland, diese Informationen gut unter den Teppich zu kehren. Nur Leute, die bewusst nach nicht klima-alarmistischen Informationen suchen, werden fündig. Ansonsten ist die Meinungs-Hoheit fest in der Hand der Mainstream-Medien.
Abb. 12 Bewertung der Messfehler bei den durchschnittlichen weltweiten Temperaturen nach Pat Frank
Was in diesem Aufsatz eher qualitativ bewertet wurde, hat Pat Frank bereits 2010 und 2011 in 2 interessanten Veröffentlichungen quantifiziert. Er untersuchte die vorliegenden Temperaturdaten nach mathematisch physikalisch Prinzipien auf Zuverlässigkeit. Seine Ergebnisse sind in einem Diagramm zusammengefasst (s. Abb.12).
Quellen: Energy & Environment · Vol. 21, No. 8, 2010; Pat Frank; Uncertainty in the global average surface air temperature index: representative lower limit
Energy & Environment · Energy & Environment · Vol. 22, No. 4, 2011; Pat Frank „Imposed and Neglected Uncertainty in the Global Average Surf
Die Schlussfolgerungen von Pat Frank zu den Fehlergrößen der Temperatur-Messungen in den letzten 170 Jahren sind logisch und nachvollziehbar. Dabei kommt es nicht darauf an, ob der Fehlerbalken 0,46K (s. Abb. 12) groß ist, etwas größer oder kleiner. Entscheidend ist die Erkenntnis, dass die Darstellungen in allen wissenschaftlichen Veröffentlichungen und in den Medien deutlich fehlerbehaftet sind.
Abb. 13: The black line shows the global annual average land-surface air temperature anomalies from 1856 to 2013 from the CRUTEM4 dataset (data here in this format). The grey area shows the 95% confidence range on the annual averages. [7]
Mit großer Wahrscheinlichkeit sind die einheitlichen Fehlerbalken von Pat Frank über den gesamten Zeitraum von 170 Jahren nicht durchgehend realistisch. Die messtechnischen Entwicklungen und bessere Verteilung der Mess-Stationen über die Erde im Laufe der Zeit werden von ihm nicht berücksichtigt. Realistischer dürfte die Abschätzung des englischen Metoffice-Instituts sein (Abb. 13). Die Fehlerbalken sind im 19. Jahrhundert groß und werden im Laufe der Zeit kleiner. Eine Darstellung, die wesentlich besser die Argumente in den vorhergehenden Kapiteln dieses Aufsatzes berücksichtigen.
7.) Schlussfolgerungen
Die Grundlage aller Argumente und Schlussfolgerungen des IPCC, der Klima-wissenschaftler und Klima-Alarmisten sind die verfügbaren weltweiten Durchschnitts-Temperaturen seit 1850. Die Zeit zu Beginn der Industrialisierung ist sozusagen der „Absolute Nullpunkt“ für die Temperatur-Entwicklung auf unserer Erde. Dieser Bezugspunkt wurde gewählt, weil ab dort die Menschheit relevant zum Anstieg der CO2-Konzentration in der Atmosphäre beigetragen hat. Der so genannte anthropogene CO2-Anteil. Was nützten starke Mauern, massive Zwischendecken und ein festes Dach, wenn das Fundament eines Gebäudes zu schwach ausgelegt ist? Fakt ist, die verfügbaren Temperaturdaten (Fundament) sind fehlerbehaftet. Die Fehlergröße bewegt sich in einem Bereich, wo eine Referenzierung auf diese Daten wissenschaftlich keinen Sinn macht. Alle noch so großen Anstrengungen der Klimawissenschaftler mit ihren immer leistungsfähigeren Großcomputer werden die Simulationen zur Zukunft des Erdklimas nicht verbessern. Die Datenbasis ist schlecht, die Kenntnisse über das hoch komplexe Klima immer noch gering (Freeman Dyson). Deshalb bleibt die Klimawissenschaft, die einzig auf die Wirkung des anthropogenen CO2 abzielt, eine Pseudo-Wissenschaft.
Referenzen:
[1] Ivar Giaever, The Strange Case of Global Warming, 2012 in Lindau (DE)
https://www.mediatheque.lindau-nobel.org/videos/31259/the-strange-case-of-global-warming-2012
[2] Ivar Giaever, Global Warming Revisited, 2015 in Lindau (DE)
https://www.mediatheque.lindau-nobel.org/videos/31259/the-strange-case-of-global-warming-2012
[3] Energy & Environment · Vol. 21, No. 8, 2010; Pat Frank; Uncertainty in the global average surface
air temperature index: representative lower limit
[4] Energy & Environment · Energy & Environment · Vol. 22, No. 4, 2011; Pat Frank „Imposed and
Neglected Uncertainty in the Global Average Surface
[5] Patricia Adams in GWPF, 2020 https://www.thegwpf.org/content/uploads/2020/12/Green-reds.pdf?utm_source=CCNet+Newsletter&utm_campaign=ac7e32fda3-EMAIL_CAMPAIGN_2020_12_11_10_30_COPY_01&utm_medium=email&utm_term=0_fe4b2f45ef-ac7e32fda3-36485917&mc_cid=ac7e32fda3&mc_eid=a769d95777
[6] https://www.nasa.gov/feature/goddard/2016/carbon-dioxide-fertilization-greening-earth
[7] Metoffice: https://www.metoffice.gov.uk/hadobs/hadcrut3/diagnostics/comparison.html
Der Beitrag erschien zuerst bei Die Kalte Sonne hier
ich kann mir gut vorstellen, daß in Bodennähe 5m Höhendifferenz Auswirkung auf die Bewegung der Luftmasse hat und damit Temperaturmessungen unterschiedlich ausfallen können.
Gruß D. Sauerwald
generell gilt, dass die Gradienten der Temperatur in Erdbodennähe am steilsten sind und mit der Höhe immer flacher werden. Die Luft in Bodennähe wird nur unwesentlich durch Strahlung, sondern vor allem durch direkten Kontakt mit dem Erdboden aufgeheizt bzw bei Nacht umgekehrt abgekühlt. Konkret auf den Standort bezogen ist das quantitativ jedoch kaum einschätzbar.
Einige Hinweise: Nordseite einer Kirche (= grösseres Gebäude) bedeutet, dass der Boden unter dem Thermometer tagsüber nicht durch Solarstrahlung aufgeheizt wird. Die Messwerte dürften daher tagsüber stärker als bei anderen Standorten von der Luftströmung um die Kirche abhängen. Nachts erfolgt die Abstrahlung von Wärmeenergie genauso wie auch an anderen Standorten, beginnt aber auf einem deutlich niedrigeren T-Niveau. Dabei gilt es zu beachten, dass die Abstrahlung von der 4.Potenz der jeweiligen Temperatur abhängt. Auf jeden Fall wird der Boden auf der Nordseite niedrigere Temperaturniveaus erreichen. Die Temperatur der bodennahen Luftschichten und ihre Gradienten werden dadurch beeinflusst. Bei Vorhandensein von Vegetation ist mit allerdings höherem Anteil der Umsetzung von Solar-Strahlungsenergie in Verdunstung als in Aufheizung des Erdbodens zu rechnen, was zu niedrigeren T-Niveaus führt (und daher auch zu flacheren T-Gradienten).
Konkretere Aussagen würden die Existenz von Vergleichsmessungen voraussetzen.
Vielleicht macht es Sinn, wenn Sie für Ihre Abschätzungen mal auf die Daten des Hamburger Wettermasts schauen:
https://wettermast.uni-hamburg.de/frame.php?doc=Home.htm
Mfg
klar alle lesen mit! Gibt es eine Artikel von ihnen dazu?
Frohes Fest!
„misst“ war schon richtig. Ob nun viel oder weniger viel, darüber entscheidet der „Messknecht“. 🙂
Dipl.Ing. Klaus Feldmann
DAS sind sehr wichtige Feststellungen, die ich als Kenner von Glasthermometern und Hersteller von elektrischen Fühlern voll unterstreichen kann, die aber sogar von einschlägig Ausgebildeten der Klimahysteriker, wie hier immer wieder zu lesen, völlig ignoriert werden.
Auch Ihre letzten beiden Sätze kann ich als Praktiker in Herstellung und Kalibrierung von Temperaturfühlern nicht oft genug unterstreichen.
1. Terrestrische Messungen der Lufttemperatur ca. 2 m über der Oberfläche in meteorologischen Stationen erweitert um Ozeanwassertemperaturen.
2. Satellitenmessungen, welche eine offenbar temperaturabhängige elektromagnetische Strahlung des Luftsauerstoffs im 50-60 GHz Bereich messen und
3. die Methode der Okkultation, welche die dichteabhängige Laufzeit von Funksignalen quer durch die Erdatmosphäre als Basis für Feuchtigkeit und Temperatur nimmt.
Ad. 1)
siehe Basisbeitrag
Ad.2)
Gem. Literatur mißt so eine Messung Mittelwerte von Atmosphärenschichten. Also untere Troposphäre, mittlere Troposphäre, Tropopause, untere Stratosphäre usw. aber keinen Wert 2 m über dem Boden. Satelliten zur Temperaturmessung haben Bahnen unter 90° zum Äquator, laufen also auf einer Polarbahn, während die Erde sich unter ihnen durchdreht. Wenn so ein Satellit wie üblich 12 Umläufe pro Tag macht, verläuft der nächste Scan um 30° versetzt zum letzten. Weil sich die Erde kontinuierlich dreht, verlaufen die Projektionen der Bahn auf die Oberfläche aber nicht auf Meridianen sondern schräg dazu. Dadurch ergibt sich ein Muster, bei dem ein „Zweieck“ zunächst von Süd nach Nord abgetastet wird und ein dazu um 15° versetztes Zweieck 12 Stunden später von Nord nach Süd. Wenn man die Länge des Äquators grob mit 40.000 km rechnet, ergeben 30° am Äquator einen Abstand von 3330 km und 15° ergeben 1660 km. Es erhebt sich also die Frage, erfaßt das Meßprinzip die tatsächliche Fläche der 24 entstehenden schrägen Kugelzweiecke oder was wird eigentlich genau gemessen? Kontinuierlich, getaktet, direkt nach unten, auch mit Seitenscans links rechts, usw.? Und wie erfolgt aus diesen Werten die Fertigwertberechnung?
Ad.3)
Dazu werden gem. Literatur die Funksignale der GPS-Satelliten mitverwendet. Welche konkreten Aussagen auch in Bezug auf die beiden anderen Methoden dabei gewonnen werden können, habe ich bisher noch nicht herausgefunden.
auch wenn Sie kritisch zum AW-„Klimawandel“ eingestellt sind, Ihre „Kenntnis“ zur (Temperatur-)Messtechnik ist nicht ausreichend und die Schlussfolgerungen deshalb falsch.
Ein Messpunkt genügt nicht, da rund um den Globus verschiedene, teilweise zeitversetzte Trends vorliegen, die ein Messpunkt damit nicht erfassen kann.
Mehr Messstationen verringern den Fehler des Messergebnisses definitiv und zwar sowohl des Mittelwertes, vor allem jedoch der Varianz. Allerdings nur, wenn die Bedingungen für eine Stichprobe eingehalten sind, was aus verschiedenen Gründen nicht der Fall ist.