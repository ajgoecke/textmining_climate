Was ist üblicherweise mit Klima-Sensitivität gemeint? Um das zu erklären, muss ich einen kleinen Umweg machen. Erstens, einfallende Strahlung [Downwelling].
„Downwelling“ in der Klimawissenschaft bedeutet die auf die Oberfläche des Planeten treffende Strahlung. Einfallende Strahlung ist die Gesamtstrahlung in Richtung Erdoberfläche. Sie setzt sich zusammen aus Sonnenlicht (kurzwellig) plus thermischer Strahlung aus der Atmosphäre (langwellig). In der Klimawissenschaft nennt man diesen Parameter, also die gesamte einfallende Strahlung „Antrieb“ [Forcing F].
Das zentrale Paradigma der modernen Klimawissenschaft lautet, dass wenn sich die Stärke der einfallenden Strahlung (Forcing) ändert, sich auch die Temperatur an der Oberfläche notgedrungen ändert. Es wird behauptet, dass sich alles andere heraus mittelt, und falls das Forcing sich verstärkt, muss sich auch die Oberflächentemperatur ändern, um das globale Energie-Gleichgewicht zu erhalten. Sie muss sich ändern! Sie muss!
Kurz gesagt lautet das zentrale Paradigma der modernen Klimawissenschaft:
Langfristig ist die globale Temperaturänderung proportional zur Änderung der globalen Forcings.
Die vermeintlich konstanten Proportion zwischen beiden Größen, also Temperatur-Änderung dividiert durch Forcing-Änderung, nennt man „Klima-Sensitivität“.
Klima-Sensitivität wird oftmals ausgedrückt als die mutmaßliche Temperaturänderung bei einer Änderung der einfallenden Strahlung von 3,7 Watt pro Quadratmeter (W/m²). Die Berechnung dieser so genannten „Klima-Sensitivität“ ist eine zentrale Frage, die sich rund um das Paradigma erhebt, dass Temperaturänderung proportional zu Temperaturänderung ist.
Und das bringt mich zu der höchst langweiligen Graphik unten. Sie zeigt die Änderungen der Schätzung des Wertes der Klima-Sensitivität über die Zeit:
[Bildinschrift oben in der Graphik: UTTER STAGNATION – Dr. Shaviv schreibt: „Lassen Sie mich das in eine Perspektive bringen mit der langweiligsten Graphik, die ich je in meinem Leben geplottet habe. Die Graphik zeigt die wahrscheinliche Bandbreite der Klima-Sensitivität als Funktion der Zeit. Wie man sieht, hat sich die wahrscheinliche Bandbreite der Klima-Sensitivität seit dem Charney-Report aus dem Jahr 1979 nicht verändert, wenn man vom AR 4 mit einer etwas geringeren Bandbreite absieht. Mit anderen Worten: Nach vermutlich Milliarden Dollar, welche über mehr als drei Jahrzehnte in die Klimaforschung gepumpt worden waren, hat sich unsere Fähigkeit, die wichtigste Frage bzgl. Klima zu beantworten, nicht einmal ansatzweise verbessert!“]
Es ist der Erwähnung wert, dass seit dem Jahr 1979 gesamte neue Wissenschaftsbereiche wie die DNA-Analyse erstmals aufkamen und inzwischen erstaunliche Niveaus der Entwicklung erreicht haben … und das über die gleiche Zeit, in der die von Dr. Shaviv als die „bedeutendste Frage bzgl. Klima“ bezeichnet keinerlei Entwicklung aufweist. Überhaupt keine.
Seit 1979 ist die Rechenleistung der uns zur Verfügung stehenden Computer explodiert, sowohl individuell als auch bei großen Organisationen. Mein zuverlässiger PowerMac hat mehr Rechenleistung als den meisten Universitäten im Jahre 1979 zur Verfügung stand. Die Kosten sind ebenfalls gesunken, von 100.000 Dollar pro MIPS (Millionen Befehle pro Sekunde) auf weniger als 1 Dollar pro MIPS heute. Und die Geschwindigkeit der Rechenleistung hat alle Grenzen gesprengt, laufen doch über die Super-Computer Klimamodelle mit über einer Billion Rechen-Operationen in jeder Sekunde. Auch die Anzahl der sich mit der Erforschung des Wertes der Klima-Sensitivität befassenden Personen hat mit der Zeit zugenommen. Und Milliarden über Milliarden Dollar wurden aufgebracht, um die Frage zu beantworten.
Fazit: Seit dem Charney-Report im Jahre 1979 bzgl. Klima-Sensitivität gab es eine ebenso horrende wie unsinnige Zunahme bei:
● Computerleistung bzgl. der Frage
● Stunden intensiver Forschung bzgl. der Frage
● Diskussionen, Debatten und Interesse bzgl. der Frage
● Aufgebrachtes Geld bzgl. der Frage
Und trotz dieser ungeheuren Steigerung von Zeit, Arbeit, Diskussionen und Computerleistung hat sich die Untersuchung der Frage nach dem Wert der Klima-Sensitivität genau Null Millimeter bewegt. Keinerlei Fortschritt.
Wie können wir diese wissenschaftliche Eigenartigkeit einordnen? Was steckt dahinter, dass all die aufgebrachte wertvolle Zeit, Geld und Mühen genau nichts erreicht hat? Ich meine Null. Gar nichts. Keinerlei Bewegung. Die langweiligste Graphik.
Ich schlage mal als Grund vor, dass die Klimawissenschaft das Opfer von etwas ist, das ich das „Picasso-Problem“ nenne. Pablo Picasso sagte nämlich einmal etwas, das mir noch sehr lange im Kopf herumging:
„Welchen Nutzen haben Computer? Sie können doch nur Antworten geben“.
Ich selbst habe mein erstes Computerprogramm im Jahre 1963 geschrieben, also vor mehr als einem halben Jahrhundert. Damals war ich sechzehn Jahre alt. Ich ließ es auf einem Computer von der Größe eines kleinen Raumes laufen. Seitdem programmiere ich Computer. Ich habe Programme für alles geschrieben, von Stoffdesign für große Zelte [catenary tents ?] über die Berechnung der Gezeiten des nächsten Jahres aus den Gezeiten-Tabellen dieses Jahres bis hin zur Erstellung der Graphiken in diesem Beitrag. Und mit den Jahren habe ich auch ziemlich Kohle gemacht mit meiner Erfahrung bzgl. Computer.
Wenn ich also lese, dass Picasso Computer mit jener Feststellung abschreibt, lautete meine spontane Antwort „Was?! Computer sind großartig! Worauf will dieser verrückte Künstler hinaus? Ich habe viel Geld verdient mit meinen Computern. Wie können sie unnütz sein?“
Aber nach gründlicherem Nachdenken erkannte ich, dass Picasso recht hatte. Er meinte Folgendes:
„Selbst der allerbeste Computer kann keine richtige Antwort geben, wenn man ihm zuvor nicht die richtige Frage stellt“.
Das war für mich eine grundlegende Erkenntnis, eine, die mich über viele meiner wissenschaftlichen Unternehmungen geleitet hat – man konzentriere sich nicht zu sehr auf die Antworten. Man muss auch die Fragen in den Mittelpunkt rücken.
Betrachtet man nun also die Klimawissenschaft – welches ist die falsche Frage, welche die richtige? Auch hier möchte ich wieder ein wenig abschweifen.
Mein Interesse an der Klimawissenschaft wurde um die Jahrtausendwende geweckt, und zwar wegen der inflationären Zunahme von Untergangsprophezeiungen eines drohenden ThermageddonTM. Also begann ich, mich mit den Grundlagen zu befassen und zu lernen, wie der schlecht bezeichnete „Treibhauseffekt“ die Erde weit wärmer hält als es auf dem Mond ist, der etwa die gleiche Entfernung von der Sonne hat.
Allerdings habe ich im Zuge dieses Interesses gelesen, dass das Best Estimate der Erwärmung über das gesamte 20. Jahrhundert in der Größenordnung von 0,6°C aufgetreten war. Da kam mir spontan der Gedanke „Was – weniger als ein Grad?! All dieses Brimborium, und die Temperatur hat sich um weniger als 1 Grad geändert?“
Ich war überrascht wegen meiner Erfahrung mit der Reparatur von Maschinen mit einem Regler und aufgrund meiner Erfahrung mit Sonnenenergie. Ich betrachtete das Klima als eine gigantische, von der Sonne angetriebene Wärmemaschine, wobei die Sonnenenergie konvertiert wird zu der unablässigen Bewegung der Atmosphäre und der Ozeane gegen die bremsende Wirkung der Reibung und vielem mehr.
Wenn man die Effizienz oder andere Charakteristika einer Wärmemaschine analysiert, oder wenn man Dinge wie die Stefan-Boltzmann-Gleichung verwendet, um Temperatur in das Äquivalent thermischer Strahlung zu konvertieren, muss man die Kelvin-Temperaturskala (K) heranziehen. Diese Skala beginnt am absoluten Nullpunkt. Temperatur ist eine Funktion der Bewegung von Molekülen oder Atomen, und am absoluten Nullpunkt hören diese Bewegungen vollständig auf.
Man kann für derartige Berechnungen nicht die Celsius- oder die Fahrenheit-Skala heranziehen, weil beide willkürlich gewählte Nullpunkte haben. Nur mit der Kelvin-Skala funktionieren diese Berechnungen. Kelvin hat die gleichen Abstände der Einheiten wie Celsius und lediglich einen anderen Nullpunkt, nämlich minus 273,15°C.
Nun beträgt die mittlere Temperatur an der Erdoberfläche größenordnungsmäßig 14°C – oder 287 Kelvin. Und mit dieser mittleren globalen Temperatur von 287 Kelvin macht die globale Temperatur-Variation von 0,6 K über das 20. Jahrhundert gerade mal ein Fünftel von einem Prozent aus.
Dies war die Seltsamkeit, die mich bei meinen Untersuchungen bzgl. Klima begleitete … über einen Zeitraum von 100 Jahren hat die Temperatur nur mit etwa einem Fünftel eines Prozentes variiert. Das war für mich umwerfend. Ich hatte viel Erfahrung mit geregelten Systemen wegen meiner Arbeit mit elektrischen Generatoren. Diese müssen genau geregelt werden, damit ihre Geschwindigkeit konstant bleibt trotz der sich ändernden Last des Systems. Und bei dieser meiner Arbeit habe ich herausgefunden, dass es ziemlich schwierig ist, innerhalb eines Prozents regelnd in ein mechanisches System einzugreifen.
Und jetzt: Trotz Dürren und Überschwemmungen, trotz großer Vulkanausbrüche, trotz sich konstant ändernder globaler Wolkenbedeckung, trotz aller Arten von Änderungen des Antriebs, trotz hemisphärischer Temperaturänderungen um ~13°C zweimal pro Jahr in jedem Jahr, trotz des Ausgleichs des Globus‘ bzgl. eines Treibhauseffektes, der den Planeten um rund ~50°C wärmer hält als den Mond … trotz all dieser Änderungen und Variationen variierte die mittlere Temperatur der Erde nicht stärker als ein Viertel Prozent im Laufe eines ganzen Jahrhunderts.
Das ist eine erstaunlich enge Regelung. Hier folgt ein Beispiel aus der realen Welt, warum mich diese Stabilität überrascht hat.
In meinem Auto gibt es einen Geschwindigkeits-Regler. Den kann man auf eine bestimmte Geschwindigkeit einstellen, die dann stur gehalten wird, egal ob das Auto beladen ist oder nicht. Bei starkem Gefälle ändert sich die Geschwindigkeit geringfügig um plus/minus 1 km/h. Das heißt, diese vom Computer gesteuerte Regelung hält die Bandbreite der Geschwindigkeit innerhalb 2% … aber die Temperatur der Erde ist sogar noch viel besser reguliert. Sie schwankt innerhalb von plus/minus einem Zehntel Prozent.
Zu jener Zeit war diese thermische Stabilität für mich ein untrügliches Anzeichen für die Existenz irgendeines unbekannten natürlichen thermostatischen Prozesses, der auf sehr effiziente Art und Weise die Temperatur innerhalb dieser geringen Bandbreite hält. Also sah ich es als meine Aufgabe im Bereich Klimawissenschaft, das Phänomen zu finden, welches diese enge Bündelung der planetarischen Temperatur über Jahrhunderte hinweg bewirkt.
Was ich in eine eigentümliche Lage brachte. Alle etablierten Klimawissenschaftler versuchten und versuchen immer noch herauszufinden, warum sich die Temperatur so stark ändert. Sie verbrachten Zeit damit, auf Graphiken wie die Folgende zu starren, welche die Variationen der Temperatur auf der Erde zeigt:
Andererseits wollte ich als jemand mit Interesse für Wärmemaschinen und Regler versuchen herauszufinden, warum sich die Temperatur so wenig geändert hat. Dabei zog ich genau die gleichen Daten heran wie in Abbildung 2, habe sie aber anders dargestellt:
Und das bringt mich nach dem Beackern dieses entfernt liegenden Bereiches zurück zu der Frage nach der Klima-Sensitivität und der weitsichtigen Frage von Picasso: „Wozu sind Computer nütze? Sie können doch nur Antworten geben!“
Meine Aussage: Der Grund dafür, warum wir Jahrzehnte lang überhaupt keine Fortschritte gemacht haben, die Klima-Sensitivität zu berechnen oder zu messen ist, dass wir unsere unermessliche Computerleistung bemühen, um die Antwort auf die Frage zu finden, warum sich die globale Temperatur so stark ändert.
Für mich ist das die völlig falsche Frage. Stattdessen sollten wir viel eher die folgende Frage stellen:
Warum ändert sich die globale Temperatur nur so geringfügig?
Nach vielem Nachdenken und noch mehr Forschung erkenne ich, dass der Grund für die nur so geringe Änderung der globalen mittleren Temperatur NICHT die Proportionalität derselben zu irgendwelchen Antrieben ist, wie allgemein angenommen wird. Als Ergebnis ist die so genannte „Klima-Sensitivität“ keine Konstante, wie vermutet wird … und weil das so ist, ist der Versuch, den exakten Wert zu berechnen, vergebliche Mühe, weil es keinen festen Wert gibt. Darum auch können wir auch nicht den geringsten Fortschritt erzielen, sie zu messen … weil es eine Chimäre ist auf der Grundlage eines falschen Verständnisses dessen, was vor sich geht.
Stattdessen lautet meine Hypothese, dass die Temperatur durch eine Vielfalt von Phänomenen innerhalb dieser geringen Bandbreite gehalten wird. Sie kühlen die Erde, wenn es zu warm wird, und erwärmen sie, falls es zu kalt wird. Ich habe eine große Vielfalt von beobachteten Beweisen gefunden, dass dies tatsächlich der Fall ist. Siehe dazu auch die Links zu vielen Beiträgen, die ich zu diesem Thema bereits geschrieben habe*.
[*Siehe unten]
Aber das ist jetzt nur meine Antwort. Und ich bekenne sofort, dass diese durchaus falsch sein kann … aber zumindest ist es eine Antwort auf die richtige Frage. Das wirkliche Mysterium des Klimas ist dessen erstaunliche thermische Stabilität.
Und schließlich, wie kommt es, dass ein ganzer Wissenschaftsbereich sich abmüht zu versuchen, die falsche Frage zu beantworten? Ich sage, dass dies die Folge der Erschaffung des IPCC seitens der UN ist.
Im Jahre 1988 war der Bereich Klimawissenschaft ein ganz neuer Bereich. Trotzdem jedoch waren die UN bereits davon überzeugt, dass man genau wusste, um welches Problem es sich handelt. Typische bürokratische Arroganz. Als Folge davon heißt es in der Resolution 43/53 der Vollversammlung aus dem Jahre 1988 (die das IPCC hervorbrachte):
Die Vollversammlung ist besorgt hinsichtlich menschlicher Aktivitäten, welche das globale Klima verändern können, wodurch gegenwärtige und zukünftige Generationen von ernsten ökonomischen und sozialen Konsequenzen bedroht sind.
Man nimmt besorgt zur Kenntnis, dass Beweise zunehmend darauf hindeuten, dass die fortgesetzte Zunahme der Konzentration von „Treibhaus“-Gasen eine globale Erwärmung hervorrufen könnte mit einem eventuellen Anstieg des Meeresspiegels, deren Auswirkungen katastrophal für die Menschheit sein können, wenn nicht rechtzeitig in allen Bereichen Gegenmaßnahmen getroffen werden.
Und als Erwiderung sprang man sofort zu der Frage, ob sie wissenschaftlich korrekt war oder nicht; und man ging direkt dazu über, Maßnahmen gegen etwas zu ergreifen, von dem die Vollversammlung nicht die geringste Ahnung hatte. Die Resolution sagte, dass …
…die Vollversammlung rechtzeitig notwendige Maßnahmen ergreifen sollte, um das Problem Klimawandel innerhalb eines globalen Rahmens anzugehen.
Rufe nach Maßnahmen machen Bürokraten immer glücklich. Also wurde das IPCC, eine ausgesprochen politische „zwischenstaatliche“ Organisation, de facto zur Führungskraft einer gesamten Wissenschaftsdisziplin … was sich als ein gewaltiger Fehler herausstellte.
Bis zu jener Zeit und auch danach bis jetzt gab es in allen anderen Bereichen der Wissenschaft erstaunliche Fortschritte hinsichtlich des Verständnisses ohne irgendein globales „zwischenstaatliches“ Gremium zur Anleitung ihrer Bemühungen. Es gab begeisternde Erfolge mit der gewöhnlichen wissenschaftlichen Methodik, wobei viele Wissenschaftler involviert sind, die ziemlich unabhängig auf der ganzen Welt irgendwelchen wissenschaftlichen Fragen nachgehen, manchmal in Kooperation, manchmal im Wettbewerb, ohne dass sie irgendjemanden brauchen oder haben wollen, der „die Wissenschaft zusammenfasst“, wie es das IPCC zu tun behauptet.
Und angesichts des Fehlens jedweden Fortschrittes, wie in der „langweiligsten Graphik jemals“ oben in diesem Beitrag sage ich, dass die Welt niemals wieder eine Haufen von UN-Bürokraten [pluted bloatocrats] einsetzen sollte für irgendetwas, das mit Wissenschaft zu tun hat. Hätten wir ein „zwischenstaatliches Gremium zur DNA-Analyse“ [Intergovernmental Panel on DNA-Analysis] eingerichtet, als der Bereich noch neu war, dann kann man sicher sein, dass dieser Bereich unweigerlich blind und taub unsinnigen Behauptungen hinterhergelaufen wäre wie „97% aller DNA-Wissenschaftler stimmen darin überein…“
Auf dem exzellenten Blog von Judith Curry fragte mich einmal jemand, was genau ich gegen das IPCC hätte. Ich erwiderte:
Hier sind einige Hauptgründe. Ich habe weitere.
1) Es wird ein Grad wissenschaftlicher Übereinstimmung vorausgesetzt, der einfach nicht vorhanden ist. Die meisten Menschen in dem Bereich einschließlich der Skeptiker glauben, dass sich die Welt erwärmt und dass Menschen sehr gut dazu beitragen könnten. Aber hier endet die Übereinstimmung schon. Wie groß der menschliche Beitrag ist, wie er aussieht und wie lange – zu diesen und ähnlichen Fragen gibt es keinerlei Übereinstimmung.
2) Das Gremium ist korrupt, wie unter Anderem das Jesus Paper zeigt.
3) Es ignoriert generell alles, was von der festgelegten Weisheit der Klimawissenschaft abweichen könnte.
4) Es wird von der Politik gesteuert, nicht von Wissenschaft. Bestimmte Paragraphen und Schlussfolgerungen sind verändert oder entfernt worden aufgrund politischer Einsprüche.
5) Bei dem Versuch, Entwicklungsländer mit ins Boot zu holen, wurde eine Anzahl sehr schlechter Wissenschaftler eingestellt.
6) Jedwede Organisation, die einen Leiter wie Rajendra Pachauri bekommt, ist sehr, sehr schlecht.
7) Man hat wirkliche Unsicherheiten ignoriert und diese ersetzt durch vollkommen subjektive Schätzungen der Unsicherheit.
8) Man hat Dinge wie die Hockeyschläger-Studie und die zahlreichen „Stick-alikes” veröffentlicht, obwohl es sich dabei um eklatant schlechte Wissenschaft handelt [laughably bad science]
9) Man erstellt „Projektionen“, die wenig oder gar nichts zu tun haben mit der realen Welt, wie etwa den Representative Concentration Pathway 8.5 (RCP 8.5).
10) Skeptiker aller Arten werden generell ausgeschlossen, entweder direkt oder weil die Skeptiker Besseres zu tun haben als sich einer solchen Organisation anzudienen.
11) Jeder, der „Projektionen“ erstellt, die über das Jahr 2010 hinausgehen, bläst Schall und Rauch in die Luft [der Autor drückt es drastischer aus. Anm. d. Übers.]
12) Man macht sich viel, viel zu stark abhängig von ungeprüften, nicht verifizierten und nicht validierten Klimamodellen.
13) Das IPCC denkt generell ohne nachzudenken, dass Erwärmung schlecht, schlecht, schlecht ist … was das Gegenteil der wirklichen Auswirkungen der Erwärmung seit der Kleinen Eiszeit ist.
14) Dem IPCC wurde die falsche Aufgabe bei seiner Gründung übertragen. Anstatt es zu beauftragen herauszufinden, was nun wirklich das Klima kontrolliert, übertrug man ihm die Aufgabe herauszufinden, wie viel CO2 wir emittieren dürfen, bevor es gefährlich wird. Diese Aufgabenstellung setzte eine Vielfalt von Dingen voraus, die niemals begründet worden waren.
15) … ach was, das reicht jetzt! Wen es interessiert – ich kann noch viele weitere Gründe benennen.
Das also ist das Klima-Picasso-Problem. Der Bereich Klimawissenschaft versucht, mittels Computern Antworten auf die falsche Frage zu finden, und als Folge davon führt der Bereich ins Nirgendwo.
Link: https://wattsupwiththat.com/2018/11/17/the-picasso-problem/
Übersetzt von Chris Frey EIKE
[Im Original folgen jetzt die o. g. Angesprochenen Links zu den Beiträgen von Willis Eschenbach zu diesem Thema. Aus Zeitgründen und weil sie nicht direkt zum Thema dieses Beitrags passen, werden diese hier aber nicht mit übersetzt. Anm. d. Übers.]
Aber man/Frau kann auch Geld ausgeben ohne Resultaten näher zu kommen.
Holger Narrog
Wenn man die Vorgänge um das IPPC verstehen will, muß man m.M. bei Maurice Strong anfangen. Diesem Kanadier schwebte schon vor über 30 Jahren die Idee einer „Weltregierung“ vor. Das ist natürlich nur erreichbar, wenn die nationalen Unterschiede und Interessen schwinden und dafür Kompetenzen der Weltregierung übertragen werden. Und wie macht man das gegen nationale Interessen? Indem man eine „Klammer“ erfindet, welche die ganze Menschheit umfangen hält. Das könnte Fußball sein, ist aber nicht so wirksam wie eine weltumspannende Gefahr, der Alle ausgesetzt sind,- und schon sind wir beim AGW.
Ich glaube, man darf darauf setzen, daß das IPPC den Klimawandel nur als Vehikel benutzt, um die angestrebte „Große Transformation“ ohne national demokratische Kontrolle (oder Behinderung aus Sicht der UN?) durchzuziehen. Der heutige Leiter des PIK, Herr Prof. Edenhofer, hat dies ja in Cancun vor sieben Jahren eingeräumt.
Dummerweise haben sich die weltweiten Messungen als nicht zielführend erwiesen, da sie seit vielen Jahren stagnieren oder sogar zurückgehen. Somit wurde als heutiges Problem der „Klimawandel“ eingeführt. Die ganze Vorgehensweise erinnert stark an den „Mexikanischen Revolverschützen“, der auf ein Scheunentor schießt und anschließend um das Schußloch konzentrische Kreise einer Zielscheibe malt. Gleichzeitig bestätigt der Paradigmenwechsel auf „Klimawandel“ die Angstthese.
Wenn sich die Damen und Herren des „97%- Klubs“ mal die Mühe machen würden, in mehreren Schritten das Forcing zu verringern um dann die Vergangenheit nachzubilden, würden sie wahrscheinlich feststellen, dass mit jeder Reduzierung eine größerer Nähe zur Realität erzielt wird. Aber dann wäre halt die Fiebertheorie futsch und der Papst müsste mit Herrn Schellnhuber eine neue Enzyklika erarbeiten. Nicht auszudenken!
Richtig gruselig wird mir aber angesichts des Umstandes, daß die Weltverbesserer in den NGOs und vor allem auch die Politiker, die diesen Weg gehen, keine demokratische Legitimation für ihr Tun haben. Ich hab’s an anderer Stelle schon gesagt: Herr Putin näßt sich ein vor Lachen,- der Westen fährt sich selbst vor die Wand und er kann zuschauen!
Ein Update einer anderen Graphik zum ECS ist hier: http://notrickszone.com/wp-content/uploads/2017/10/Climate-Sensitivity-Value-Estimates-Update.jpg
http://www.drroyspencer.com/2018/02/diagnosing-climate-sensitivity-assuming-some-natural-warming/
Sobald der natürliche Anteil an der Erwärmung signifikant wird (davon ist auszugehen), sinken TCR und ECS deutlich, möglicherweise auf unter 1 K. Dies wäre der Todesstoß für IPCC, PIK und andere völlig überflüssige Institutionen, wogegen sie sich aus verständlichen Gründen wehren (müssen). Hat dann natürlich nichts mehr mit Wissenschaft zu tun.
Und hier wurde ich stutzig, vor 13 Jahren, als ich mich mit der Begründung einer Erderwärmung durch CO2 zu befassen begann. Das Problem ist ja dass das stärkere „Forcing“ als Ursache den wärmeren Erdboden hat, und der hat als Ursache das stärkere „Forcing“ – bei konstanter Insolation! Im Grunde also ein sich selbst erwärmendes System, ein Konstrukt welches gegen den ersten Hauptsatz der Thermodynamik verstößt. Für mich basieren alle Computermodelle auf genau diesem input-Fehler und es gilt „shit in = shit out“, egal wie gross die Rechenleistung auch immer sein mag….
hier liegt mitnichten eine Verletzung des 1. HS der Thermodynamik vor. Das System erwärmt sich nicht selbst, sondern die Wärmeabstrahlung in den Weltraum erfolgt verzögert. Die Wärmequelle ist nach wie vor die Sonne.
Wenn dies eine Verletzung des 1. HS wäre, bräuchten Sie im Winter auch keinen Mantel, der ja verhindern soll, dass Ihr Körper zu schnell auskühlt.
Ich hoffe nicht, dass meine Bemerkung schon wieder zu einer endlosen Diskussion führt. Langsam verzweifle ich an diesem Thema…
In beiden Fällen erfolgt die Wärmeabgabe an die Umgebung verlangsamt.
dQ/dt (mit Mantel bzw. CO2) < dQ/dt (ohne Mantel bzw. CO2)
Im Prinzip ist es mir wurscht, ob andere dies verstehen oder nicht.
Nicht wurscht ist mir allerdings, dass EIKE genau deswegen verlacht wird, weil es hier einige Leute gibt, die immer und immer wieder hinausposaunen, dass es überhaupt keinen Treibhauseffekt gebe. Wir zerfleischen uns hier selbst und die Alarmisten lachen sich ins Fäustchen. Das macht mich echt wütend.
Es müsste heißen:
|dQ/dt (mit Mantel bzw. CO2)| < |dQ/dt (ohne Mantel bzw. CO2)|
dQ/dt ist nachts < 0
Wie die Abläufe in der Atmosphäre tatsächlich sind kann man bei Thieme nachlesen:
http://real-planet.eu/atmoseff.htm
http://krahmer.freepage.de/klima/thieme/thieme.html
Wenn innerhalb der Atmosphäre Wärmeabstrahlung der Erde absorbiert wird, erwärmt sich die absorbierende Materie. Der für einen Ruhezustand der Luftschichten notwendige und gegebenenfalls zuvor bestandene vertikale Verlauf von Temperatur, Volumen und Druck wird gestört. Luft dehnt sich bei Erwärmung aus, wird bezogen auf die Volumeneinheit leichter als die umgebende, auch darüberliegende, kühlere Luft, und steigt deshalb auf. Gegebenenfalls absorbierte Wärme wird durch Luftmassenaustausch abgeführt. Ebenso wie jene Wärme, die der Luft durch Konvektion zugeführt wird. Es kommt somit zur Abfuhr der absorbierten Wärme durch Aufsteigen der erwärmten Luft. Dieser Vorgang ist übrigens auch unter dem Begriff Thermik geläufig. Beim Aufsteigen der Luft sinkt gleichzeitig durch Ausdehnung infolge Druckabnahme deren Temperatur, so daß sich keine Erwärmung einstellt.
glauben Sie wirklich, dass Sie atmosphärische Phänomene wirklich nur im Rahmen der Thermik betrachten können?
Angeregte Zustände spielen für Sie keine Rolle?
Absorptions- und Emissionsspektren der Atmosphäre sind Humbug?
http://www.science-skeptical.de/wp-content/uploads/2016/09/THE-Paul2b.jpg
http://www.science-skeptical.de/blog/grundlagen-des-treibhauseffektes-fuer-eikianer/0015523/
Denn wenn es angeregte Zustände und Zurückfallen der Elektronen in den Grundzustand unter isotroper Photonen-Abstrahlung gäbe, würde Ihre Argumentation ziemlich hinken.
Wenn Quantenmechanik bzw. Atomphysik für Sie keine Rolle spielt, wundert mich gar nichts mehr.
Oder: „Sog. Treibhausgase wie CO2 und Wasserdampf und auch Wolken führen durch ihre Absorptions- und Strahlungseigenschaften von Wärmestrahlung dazu, dass wir eine Differenz von ca. 33°C zwischen Erdoberfläche und 70 km Höhe messen.“ Der Mann hat wohl noch nie etwas von einer druckabhängigen Gaszustandsgleichung gehört und in 70 km Höhe hat es nicht -18 sondern ca. – 60°C, also eine Differenz von 75°C bezogen auf +15°C.
In dieser „Betrachtung“ reiht sich eine Absurdität an die nächste, es ist unglaublich, wie manche Leute sich freiwillig „outen“ und sich dabei auch noch erhaben obergescheit vorkommen …
Unter den Bedingungen der Troposphäre wird die Anregungsenergie per Stoßdeaktivierung an die umgebenden Moleküle abgegeben. Es findet keine Reemission statt, somit auch keine „Gegenstrahlung“. CO2 strahlt erst bei hinreichend geringer Dichte der Atmosphäre, d.h. in Höhenlagen um 220 k (das zeigen auch die Emissionsspektren!) an der Tropopause seine Anregungsenergie in das Weltall ab. Niemals kann Wärme gegen den Temperaturgradienten von 220 K nach 298 K übertragen werden (2. Hauptsatz)! Ihre „Wintermanteltheorie“ ist genau so falsch wie die Glashaustheorie!
abgesehen davon, dass Sie den 2. HS in seiner Konsequenz offenbar nicht verstehen, erklären Sie mir nach Ihrem Weltbild doch mal dieses Phänomen:
– Bei hoher Luftfeuchtigkeit kühlt sich die Erdoberfläche nachts nur rel. wenig ab.
– Bei trockener Luft bzw. klarem Himmel erfolgt nachts eine rel. starke Abkühlung.
Vergleichen Sie mal – bei ansonsten gleichen Bedingungen – den Wärmeverlust bzw. die Abkühlung an der Erdoberfläche.
Wieso ist es in den tagsüber heißen Wüsten nachts so verdammt kalt? Wenn es nicht am hier fehlenden Wasserdampf in der Atmosphäre liegt, wieso ist es dann nachts am Boden unter hoher Luftfeuchtigkeit wärmer als unter trockeneren Bedingungen? Wodurch wird denn die Wärmeabfuhr behindert?
Lieber Herr Kohl,
das müssen Sie nicht!
Versuchen Sie es einfach mal mit „begreifen“.
So z.B.:
„Denn wenn es angeregte Zustände und Zurückfallen der Elektronen in den Grundzustand unter isotroper Photonen-Abstrahlung gäbe, würde Ihre Argumentation ziemlich hinken.“
Anregung der Elektronenschale spielen bei thermischer Strahlung keine Rolle. Es geht ausschließlich um molekulare Bindungsschwingungen.
„In beiden Fällen erfolgt die Wärmeabgabe an die Umgebung verlangsamt.“
Da wird nix „verlangsamt“. OT wird die selbe Energiemenge abgestrahlt, die bodennah absorbiert wurde (sogar mehr, da auch konvektiv in die Atmosphäre eingetragene Energie abgestrahlt wird)
„“Nicht wurscht ist mir allerdings, dass EIKE genau deswegen verlacht wird, weil es hier einige Leute gibt, die immer und immer wieder hinausposaunen, dass es überhaupt keinen Treibhauseffekt gebe.“
Wer lacht???
Ich verstehe sowieso nicht wie man den ganzen Müll glauben kann wenn man weiß, daß es keine einzige Messung dazu gibt.
Mit Computer-„Modellen“ kann man alles beweisen…
1. Fehler: Das Minimum der Kleinen Eiszeit war nicht 1850, sondern viel früher, nämlich um 1600. Da lebte Galileo Galilei. Wie viel Kohle, Erdöl, Erdgas haben wir zu dieser Zeit gefördert und verbrannt, um mit dem CO2-Ausstoß den Temperaturanstieg zu bewirken? — 2. Fehler: Man errechnet durchschnittlich 3 Kelvin für die Klimasensitivität, davon geht 1/4 aufs CO2 und 3/4 auf den Wasserdampf. Da sich der Wasserdampf entgegen der Theorie nicht erhöht hat, bleiben 1/4 von 3 Kelvin, also 0,75 Kelvin pro CO2-Verdoppelung übrig. Da eine Verdoppelung des CO2 das „schlimmste“ Szenario darstellt, kann es nach der (sowieso falschen) Berechnung maximal zu einem Temperaturanstieg von 0,75 Kelvin kommen. Das politische 2 oder 1,5 Grad Ziel wird also auch ohne irgendwelche CO2-senkende Maßnahmen leicht erreicht.
http://www.sciencebits.com/AR5-FirstImpressions
Es ist wirklich unglaublich, welche Horrorszenarien uns als zu erwartende Zukunft prophezeit werden, während das IPCC über Jahrzehnte hinweg keinerlei Erkenntnisgewinn in Sachen CO2-Sensitivität schaffte. Ein Faktor 3 im erwarteten Bereich spricht Bände.
Wenn man darüber hinaus bedenkt, dass bei fast allen Abschätzungen der CO2-Sensitivität implizit davon ausgegangen wird, dass der gesamte Temperaturanstieg seit ~1850 anthropogen ist (z.B. Otto et al., Lewis & Curry), kommt man schnell zu einer Sensitivität um oder sogar unter 1K pro CO2-Verdopplung.
http://www.drroyspencer.com/2018/02/diagnosing-climate-sensitivity-assuming-some-natural-warming/