---
title: "Empirical Work - Climate Change Discourse Corpora"
output:
  html_notebook: default
  pdf_document: default
---

## Loading Libraries

```{r}
# load libraries

library(quanteda)
library(readtext)
library(wordcloud)
library(RColorBrewer)
library(wordcloud2)
library(tidyverse)
library(tm)
library("textcat")
library("quanteda.textplots")
library("quanteda.textstats")
library("gsubfn")
library("spacyr")

spacy_initialize(model = "de_core_news_sm")
```

## Start here to load existing corpora:

```{r}
# load corpus files 
full_corpus = readRDS("corpora/full_corpus.rds")
full_corpus_sents = readRDS("corpora/full_corpus_sents.rds")

pro_corpus = readRDS("corpora/pro_corpus.rds")
contra_corpus = readRDS("corpora/contra_corpus.rds")

pro2000 = readRDS("corpora/pro2000.rds")
pro900 = readRDS("corpora/pro900.rds")
contra2000 = readRDS("corpora/contra2000.rds")
contra900 = readRDS("corpora/contra900.rds")

fff_de_corpus = readRDS("corpora/fff_de_corpus.rds")
ikem_corpus = readRDS("corpora/ikem_corpus.rds")
klimarep_corpus = readRDS("corpora/klimarep_corpus.rds")
klimafakten_corpus = readRDS("corpora/klimafakten_corpus.rds")
zero_corpus = readRDS("corpora/zero_corpus.rds")
komma_corpus = readRDS("corpora/komma_corpus.rds")
eike_corpus = readRDS("corpora/eike_corpus.rds")
ffh_corpus = readRDS("corpora/ffh_corpus.rds")
```



# Exploring the corpus

## Corpus statistics

```{r}
# to get index number 
#id_pro = 1:ndoc(pro900)
```

# Plotting Number of Sentences

```{r}
contra2000_sum <- summary(contra2000)
pro900_sum <- summary(pro900, ndoc(pro900))
contra900_sum <- summary(contra900, ndoc(contra900))

#to get id as x axis
#id_pro[1:100]
#contra900_sum$id

ggplot(pro2000_sum, aes(id, Sentences, group=1)) +
  geom_line() + 
  geom_point() +
  theme(axis.text.x = element_text(angle=0, vjust=1, hjust=1)) + 
  ggtitle("Sentences Pro2000")

ggplot(contra2000_sum, aes(id, Sentences, group=1)) +
  geom_line() + 
  geom_point() +
  theme(axis.text.x = element_text(angle=0, vjust=1, hjust=1)) + 
  ggtitle("Sentences Contra2000")

```

```{r}
pro900_sum <- summary(pro900, n=100)
contra900_sum <- summary(contra900, n=100)


ggplot(pro900_sum, aes(pro900_sum$id, Tokens, group=1)) +
  geom_line() + 
  geom_point() +
  theme(axis.text.x = element_text(angle=0, vjust=1, hjust=1)) + 
  ggtitle("Tokens Pro900")

ggplot(contra900_sum, aes(contra900_sum$id, Tokens, group=1)) +
  geom_line() + 
  geom_point() +
  theme(axis.text.x = element_text(angle=0, vjust=1, hjust=1)) + 
  ggtitle("Tokens Contra900")

```

```{r}
ggplot(pro900_sum, aes(pro900_sum$id, Types, group=1)) + 
  geom_line() + 
  geom_point() +
  theme(axis.text.x = element_text(angle=0, vjust=1, hjust=1)) + 
  ggtitle("Types Pro900")

ggplot(contra900_sum, aes(contra900_sum$id, Types, group=1)) +
  geom_line() + 
  geom_point() +
  theme(axis.text.x = element_text(angle=0, vjust=1, hjust=1)) + 
  ggtitle("Types Contra900")


```

```{r}
ggplot(pro900_sum, aes(Tokens, Types, group=1, label= id)) +
  geom_smooth(method = "lm", formula ="y ~ x", se = FALSE) +
  geom_text(check_overlap = T) +
  ggtitle("Type-Token-Relation Pro900")

ggplot(contra900_sum, aes(Tokens, Types, group=1, label= id)) +
  geom_smooth(method = "lm", formula ="y ~ x", se = FALSE) +
  geom_text(check_overlap = T) +
  ggtitle("Type-Token-Relation Contra900")
```
<<<<<<< Updated upstream
# Word Frequencies


## 1. Step: Create DFM, Load Stoplists 
```{r}
# stoplists
de_stopwords <- stopwords::stopwords("de", source="snowball")
en_stopwords <- stopwords::stopwords("en", source="snowball" )
custom_stopwords <- read.table("de_complete.txt", header=F, sep="\n")



# add own stopwords 
full_stopwords <- c(de_stopwords, "dass", "=", "the", "seit", "ab", "beim", "\n", "mal", "c", "|", "m", "kommentare", "neueste", "gepostet", custom_stopwords, en_stopwords)
de_stopwords1 <- c(de_stopwords, "dass", "=", "the", "seit", "ab", "beim", "\n", "mal", "c", "\\|","|", "m", "kommentare", "neueste", "gepostet", "admin", "cookies", "inhalte", "inhalt", "newsletter", "posten", "zugriff", "passwort", "geschützt", "seite", "website", "webseite", "and", "0", "1", "2", "3","4","5","6","7","8","9", "mfg","w","t","wer")

# create dfm
dfm_p2000 <- dfm(pro2000, remove=full_stopwords, remove_punct=TRUE, remove_numbers=TRUE)
dfm_c2000 <- dfm(contra2000, remove=full_stopwords, remove_punct=TRUE, remove_numbers=TRUE)
```

## Corpus Cleaning: Lemmatization -> new lemmatized dfm
```{r}
# pro 
sp_pro2000 <- spacy_parse(pro2000, pos=FALSE, entity=FALSE, dependency=FALSE)
sp_pro2000$token <- sp_pro2000$lemma

sp_dfm_p2000 <- as.tokens(sp_pro2000)%>%
  dfm(remove=full_stopwords, remove_punct=TRUE, remove_numbers=TRUE, tolower=TRUE)

# contra
sp_contra2000 <- spacy_parse(contra2000, pos=FALSE, entity=FALSE, dependency=FALSE)
sp_contra2000$token <- sp_contra2000$lemma

sp_dfm_c2000 <- as.tokens(sp_contra2000)%>%
  dfm(remove=full_stopwords, remove_punct=TRUE, remove_numbers=TRUE, tolower=TRUE)
```

```{r}
dfm_p2000
dfm_c2000
```

## 2. Step: Check Topfeatures of DFM -> most frequently occuring terms
```{r}
topfeatures(sp_dfm_p2000, n=50)
topfeatures(sp_dfm_c2000, n=50)
```

```{r}
tf_p2000 <- topfeatures(sp_dfm_p2000, n=50)
tf_c2000 <- topfeatures(sp_dfm_c2000, n=50)
```
```{r}
textstat_frequency(sp_dfm_p2000, n=50)
```

## Plotting word frequencies

```{r}
klima_p2000 <- dfm_select(sp_dfm_p2000, pattern="klima*")
klima_c2000 <- dfm_select(sp_dfm_c2000, pattern="klima*")


```

```{r}
freq_p2000 <- textstat_frequency(sp_dfm_p2000, n=50)
freq_c2000 <- textstat_frequency(sp_dfm_c2000, n=50)

plot_p2000 <- with(freq_p2000, reorder(feature, -frequency))
plot_c2000 <- with(freq_c2000, reorder(feature, -frequency))

#create plot for eike klima words frequencies
plot1 <- ggplot(freq_p2000, aes(x=feature, y=frequency)) + 
  geom_point()+ggtitle("P2000 Frequencies")+
  theme(axis.text.x = element_text(angle=90,hjust=1))
#ggsave(plot=plot1, width = 10, height = 5, dpi=300, filename="klima_eike_plot.jpeg" )
plot1

#create plot for klimareporter klima words frequencies
plot2 <- ggplot(freq_c2000, aes(x=feature, y=frequency)) + 
  geom_point()+ ggtitle("C2000 Frequencies")+
  theme(axis.text.x = element_text(angle=90,hjust=1)) 
  
#ggsave(plot=plot2, width = 10, height = 5, dpi=300, filename="klima_klimarep_plot.jpeg" )
plot2
```
## Same Plot for "Klima" Words
```{r}
freq_klima_p2000 <- textstat_frequency(klima_p2000, n=50)
freq_klima_c2000 <- textstat_frequency(klima_c2000, n=50)

freq_klima_p2000$feature <- with(freq_klima_p2000, reorder(feature, -frequency))
freq_klima_c2000$feature <- with(freq_klima_c2000, reorder(feature, -frequency))

#create plot for eike klima words frequencies
plot1 <- ggplot(freq_klima_p2000, aes(x=feature, y=frequency)) + 
  geom_point()+ggtitle("P2000 Klima Frequencies")+
  theme(axis.text.x = element_text(angle=90,hjust=1))
#ggsave(plot=plot1, width = 10, height = 5, dpi=300, filename="klima_eike_plot.jpeg" )
plot1

#create plot for klimareporter klima words frequencies
plot2 <- ggplot(freq_klima_c2000, aes(x=feature, y=frequency)) + 
  geom_point()+ ggtitle("C2000 Klima Frequencies")+
  theme(axis.text.x = element_text(angle=90,hjust=1)) 
  
#ggsave(plot=plot2, width = 10, height = 5, dpi=300, filename="klima_klimarep_plot.jpeg" )
plot2
```
## Plot TF-IDF (not working yet!!!)
```{r}
# weighted words
p2000_weight <- dfm_weight(sp_dfm_p2000, scheme="prop")
c2000_weight <- dfm_weight(sp_dfm_c2000, scheme="prop")

relfreq_p2000 <- textstat_frequency(p2000_weight, n=50)
relfreq_c2000 <- textstat_frequency(c2000_weight, n=50)

#tfidf
p2000_tfidf <- dfm_tfidf(sp_dfm_p2000)
c2000_tfidf <- dfm_tfidf(sp_dfm_c2000)

#plot3 <- with(relfreq_p2000, reorder(feature, -freqency))
relfreq_p2000$feature <- with(relfreq_p2000, reorder(feature, -frequency))
plot3 <- ggplot(relfreq_p2000, aes(x=feature, y=frequency)) + 
  geom_point()+ggtitle("P2000 Frequencies")+
  theme(axis.text.x = element_text(angle=90,hjust=1))
#ggsave(plot=plot1, width = 10, height = 5, dpi=300, filename="klima_eike_plot.jpeg" )
plot3
```
```{r}
sp_contra2000
```


```{r}
pro_freq_tfidf <- p2000_tfidf %>%
  textstat_frequency(n=10, force=TRUE)

con_freq_tfidf <- c2000_tfidf %>%
  textstat_frequency(n=10, force=TRUE)

tplot_tfidf_p2000 <- ggplot(data=pro_freq_tfidf,
                      aes(x=factor(nrow(pro_freq_tfidf):1),
                          y=frequency)) +
  geom_point() +
  coord_flip() +
  scale_x_discrete(breaks=factor(nrow(pro_freq_tfidf):1),
                   labels=pro_freq_tfidf$feature) +
  labs(x=NULL, y="tf-idf")

tplot_tfidf_p2000

tplot_tfidf_c2000 <- ggplot(data=con_freq_tfidf,
                      aes(x=factor(nrow(con_freq_tfidf):1),
                          y=frequency)) +
  geom_point() +
  coord_flip() +
  scale_x_discrete(breaks=factor(nrow(con_freq_tfidf):1),
                   labels=con_freq_tfidf$feature) +
  labs(x=NULL, y="tf-idf")

tplot_tfidf_c2000
```

```{r}
topfeatures(p2000_tfidf, n=20)
topfeatures(c2000_tfidf, n=20)
#p2000_weight

#textstat_frequency(p2000_tfidf, n=10)
```

### Comparison of Groups/Origin
```{r}
dfm_weight_origin <- full_corpus %>%
  #corpus_subset(origin > 2000) %>%
  tokens(remove_punct = TRUE) %>%
  tokens_remove(de_stopwords1) %>%
  dfm() %>%
  dfm_weight(scheme = "prop")

# Calculate relative frequency by president
freq_weight <- textstat_frequency(dfm_weight_origin, n = 10, 
                                  groups = dfm_weight_origin$group) # change to "origin" here to see for all websites 

ggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +
     geom_point() +
     facet_wrap(~ group, scales = "free") +
     coord_flip() +
     scale_x_continuous(breaks = nrow(freq_weight):1,
                        labels = freq_weight$feature) +
     labs(x = NULL, y = "Relative frequency")
```
# Collocations
```{r}
# to remove special chars from corpus
pro2000 <- gsub("\\|", "", pro2000)
pro2000 <- gsub("=", "", pro2000)

# convert into collocation dataframe
p_coll <- textstat_collocations(pro2000, min_count=50)
arrange(p_coll, desc(count))
```
```{r}
# transform stoplists for dplyr::filter function (why?)
de_stps <- paste0(de_stopwords, collapse = "\\b|\\b")
en_stps <- paste0(en_stopwords, collapse = "\\b|\\b")

# remove entries involving stopwords: NOT WORKING YET!
p2000_coll_clean <- p_coll %>%
  dplyr::filter(!str_detect(collocation, de_stps),
                !str_detect(collocation, en_stps),
                !str_detect(collocation, "="))

#p2000_coll_clean
arrange(p2000_coll_clean, desc(count))
```

```{r}
# collocations mit vorherigem stopwords removal: not working
p2000_toks <- tokens(pro2000)
p2000_toks_sw <- tokens_select(p2000_toks, pattern=full_stopwords, selection="remove")

p2000_coll <- textstat_collocations(p2000_toks_sw, min_count=3)
arrange(p2000_coll, desc(count))
```
```{r}
summary(full_corpus, n=5)
```

## Comparison Plots
```{r}
#create input
corp <- dfm(full_corpus)

# lemmatisierung versuchen:



de_klima <- dfm_select(corp, pattern ="klima*")
de_relfreq <- dfm_weight(de_klima, scheme="prop")
de_freqs <- textstat_frequency(de_relfreq, groups=group)

#plotting 
freqs.act <- filter(de_freqs, group == "activists") %>% as.data.frame() %>% select(feature, frequency)
freqs.scept <- filter(de_freqs, group == "sceptics") %>% as.data.frame() %>% select(feature, frequency)
freqs <- left_join(freqs.act, freqs.scept, by = "feature") %>% head(30) %>% arrange(frequency.x) %>% mutate(feature = factor(feature, feature))
ggplot(freqs) +
  geom_segment(aes(x=feature, xend=feature, y=frequency.x, yend=frequency.y), color="grey") +
  geom_point(aes(x=feature, y=frequency.x), color = "red", size = 3 ) +
  geom_point(aes(x=feature, y=frequency.y), color = "lightblue", size = 3 ) +
  ggtitle("Word Frequencies") + 
  xlab("") + ylab("Wortfrequenz") + 
  coord_flip()

freqs.act 
freqs.scept
```
```{r}

de_klima_pro <- dfm_select(sp_pro2000_dfm, pattern ="klima*")
de_relfreq_pro <- dfm_weight(de_klima_pro, scheme="prop")
de_freqs_pro <- textstat_frequency(de_relfreq_pro)

de_klima_con <- dfm_select(sp_con2000_dfm, pattern ="klima*")
de_relfreq_con <- dfm_weight(de_klima_con, scheme="prop")
de_freqs_con <- textstat_frequency(de_relfreq_con)


#plotting 
freqs.act <- filter(de_freqs_pro) %>% as.data.frame() %>% select(feature, frequency)
freqs.scept <- filter(de_freqs_con) %>% as.data.frame() %>% select(feature, frequency)
freqs <- left_join(freqs.act, freqs.scept, by = "feature") %>% head(30) %>% arrange(frequency.x) %>% mutate(feature = factor(feature, feature))
p <- ggplot(freqs) +
    geom_segment(aes(x=feature, xend=feature, y=frequency.x, yend=frequency.y), color="grey") +
    geom_point(aes(x=feature, y=frequency.x, colour="Activists"), size = 3) +
    geom_point(aes(x=feature, y=frequency.y, colour="Sceptics"), size = 3 ) +
    ggtitle("Word Frequencies") + 
    xlab("") + ylab("Frequency") +
    coord_flip()

p+labs(colour="Group")

```


```{r}
# to get lemmatized dfm 
sp_pro2000_dfm <- as.tokens(sp_pro2000) %>%
  dfm()

sp_con2000_dfm <- as.tokens(sp_contra2000) %>%
  dfm()

```



```{r}
dfm_weight_corp <- full_corpus %>%
  tokens(remove_punct = TRUE) %>%
  tokens_remove(de_stopwords1) %>%
  dfm() %>%
  dfm_weight(scheme = "prop")

# Calculate relative frequency by president
freq_weight <- textstat_frequency(dfm_weight_corp, n = 10, 
                                  groups = dfm_weight_corp$origin)

ggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +
     geom_point() +
     facet_wrap(~ group, scales = "free") +
     coord_flip() +
     scale_x_continuous(breaks = nrow(freq_weight):1,
                        labels = freq_weight$feature) +
     labs(x = NULL, y = "Relative frequency")
```
```{r}
summary(full_corpus, n=10)
```

## Keyness
```{r}
# Create a dfm grouped by president
corp_dfm <- tokens(full_corpus, remove_punct = TRUE) %>%
  tokens_remove(de_stopwords1) %>%
  tokens_group(groups = group) %>%
  dfm()

# Calculate keyness and determine Trump as target group
result_keyness <- textstat_keyness(corp_dfm, target = "activists")

# Plot estimated word keyness
textplot_keyness(result_keyness, margin=0.2, n=10) 
```
## Comparison Cloud
```{r}
full_corpus %>%
    tokens(remove_punct = TRUE, remove_numbers=TRUE) %>%
    tokens_remove(de_stopwords1) %>%
    dfm() %>%
    dfm_group(groups = group) %>%
    dfm_trim(min_termfreq = 5, verbose = FALSE) %>%
    textplot_wordcloud(comparison = TRUE, max_words=100)
                       #,color=c("lightblue","blue"))
```
```{r}
textplot_xray(
    kwic(tokens(pro2000), pattern = "klima*"),
    kwic(tokens(contra2000), pattern = "klima*"))
```
# Topic Modeling
```{r}
library(tidytext)
library(topicmodels)
```

```{r}
dfm_full <- dfm(full_corpus, remove=de_stopwords1, remove_punct=TRUE, remove_numbers=TRUE)
dfm_full

tm_full<-convert(dfm_full, to="topicmodels")

topicModel <- LDA(tm_full, k=5, method="Gibbs", control=list(iter = 500, verbose = 25))
terms(topicModel, 10)
```


```{r}
ap_lda <- LDA(tm_full, k=5, control=list(seed=1234))

ap_topics <- tidy(ap_lda, matrix="beta")

ap_lda


ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```
- try lemmatized version for topic modeling 


Ideas:
DONE: Wordcloud Plot of Comparison Group:
      https://quanteda.io/articles/pkgdown/examples/plotting.html

Lexical Dispersion Plot (X-Ray)
-> could be done for keywords "klima*"

Next:
- Calculate "Corpus Similarity"
- Klimawörter Liste mit Group und Counts abspeichern -> welche Klimawörter gibt es wo und wie oft
- alles mit der "Sents" Version testen
- Analyse der Ergebnisse
- Literatur-Recherche zu den Textmining Themen

Topic Modeling
https://www.tidytextmining.com/topicmodeling.html





